{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is time taking unfortunately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.gold import minibatch\n",
    "from spacy.language import Language\n",
    "from spacy import util\n",
    "\n",
    "import jsonlines \n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import exrex\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "class EntityMatcher(object):\n",
    "#     name = 'entity_matcher'\n",
    "\n",
    "    def __init__(self, nlp, terms, label):\n",
    "        \n",
    "        patterns = []\n",
    "        \n",
    "        if label =='OG':\n",
    "            for term in tqdm(terms): \n",
    "                individual_term = term.replace('[^A-Za-z0-9]','')\n",
    "                patterns.append([{\"TEXT\": {\"REGEX\": individual_term}}])\n",
    "        else:\n",
    "            for term in tqdm(terms):\n",
    "                individual_terms =[]\n",
    "                for individual_term in term.split(' '):\n",
    "                    individual_terms.append({\"TEXT\": individual_term})\n",
    "                patterns.append(individual_terms)\n",
    "        \n",
    "        self.matcher = Matcher(nlp.vocab)\n",
    "        self.matcher.add(label, None, *patterns)        \n",
    "\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        matches = self.matcher(doc)\n",
    "        seen_tokens = set()\n",
    "        new_entities = []\n",
    "        entities = doc.ents\n",
    "        for match_id, start, end in matches:\n",
    "            # check for end - 1 here because boundaries are inclusive\n",
    "            if start not in seen_tokens and end - 1 not in seen_tokens:\n",
    "                new_entities.append(Span(doc, start, end, label=match_id))\n",
    "                entities = [\n",
    "                    e for e in entities if not (e.start < end and e.end > start)\n",
    "                ]\n",
    "                seen_tokens.update(range(start, end))\n",
    "\n",
    "        doc.ents = tuple(entities) + tuple(new_entities)\n",
    "        return doc\n",
    "    \n",
    "#         matches = set(\n",
    "#                     [(m_id, start, end) for m_id, start, end in matches if start != end]\n",
    "#                 )\n",
    "#         get_sort_key = lambda m: (m[2] - m[1], m[1])\n",
    "#         matches = sorted(matches, key=get_sort_key, reverse=True)\n",
    "#         entities = list(doc.ents)\n",
    "#         new_entities = []\n",
    "#         seen_tokens = set()\n",
    "#         for match_id, start, end in matches:\n",
    "#             if any(t.ent_type for t in doc[start:end]):\n",
    "#                 continue\n",
    "#             # check for end - 1 here because boundaries are inclusive\n",
    "#             if start not in seen_tokens and end - 1 not in seen_tokens:\n",
    "#                 new_entities.append(Span(doc, start, end, label=match_id))\n",
    "#                 entities = [\n",
    "#                     e for e in entities if not (e.start < end and e.end > start)\n",
    "#                 ]\n",
    "#                 seen_tokens.update(range(start, end))\n",
    "#         doc.ents = entities + new_entities\n",
    "#         return doc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Diseases = '/nfs/gns/literature/lit-textmining-pipelines/automata/DiseaseDictionary.mwt' #(# of terms - 59088)\n",
    "Genes_Proteins = '/nfs/gns/literature/lit-textmining-pipelines/automata/swissprot_Sept2014.2.3.mwt' #(# of terms - 347929)\n",
    "# Organism = '/nfs/gns/literature/lit-textmining-pipelines/automata/Organisms150507.2.mwt' #(# of terms - 1683021)\n",
    "Organism = '/nfs/gns/literature/machine-learning/Dictionaries/MWT/organisms_toy.mwt' #(# of terms - 1683021)\n",
    "\n",
    "gene_patterns = []\n",
    "disease_patterns = []\n",
    "organisms_patterns = []\n",
    "\n",
    "\n",
    "print('Loading Genes Dictionary')\n",
    "with open(Genes_Proteins, 'r') as f:\n",
    "    contents = f.read()\n",
    "    soup = BeautifulSoup(contents, 'lxml')\n",
    "    pattern_types = soup.find_all('t')\n",
    "    for pattern in tqdm(pattern_types):\n",
    "        gene_patterns.append(pattern.text)\n",
    "        \n",
    "        \n",
    "\n",
    "print('loading Disease Dictionary')\n",
    "with open(Diseases, 'r') as f:\n",
    "    contents = f.read()\n",
    "    soup = BeautifulSoup(contents, 'lxml')\n",
    "    pattern_types = soup.find_all('t')\n",
    "    for pattern in tqdm(pattern_types):\n",
    "        disease_patterns.append(pattern.text)\n",
    "\n",
    "\n",
    "print('Loading Organims Dictionary')\n",
    "with open(Organism, 'r') as f:\n",
    "    contents = f.read()\n",
    "    soup = BeautifulSoup(contents, 'lxml')\n",
    "    pattern_types = soup.find_all('r')\n",
    "    for pattern in tqdm(pattern_types):\n",
    "         organisms_patterns.append(pattern.text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from ast import literal_eval\n",
    "import csv\n",
    "\n",
    "\n",
    "gene_patterns_ = set(gene_patterns)\n",
    "disease_patterns_ = set(disease_patterns)\n",
    "organisms_patterns_ = set(organisms_patterns)\n",
    "\n",
    "train_set = '/nfs/gns/literature/machine-learning/evaluation/300articles/CSV formats/train.csv'\n",
    "dev_set = '/nfs/gns/literature/machine-learning/evaluation/300articles/CSV formats/dev.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_set, sep = '\\t', names = ['pmcid', 'sentence','ner'])\n",
    "dev_df = pd.read_csv(dev_set, sep = '\\t', names = ['pmcid', 'sentence','ner'])\n",
    "\n",
    "for index, row in tqdm(train_df.iterrows(), total=train_df.shape[0]):\n",
    "    ner_tags = row['ner']\n",
    "    if ner_tags:\n",
    "        try:\n",
    "            ner_tags = literal_eval(ner_tags)\n",
    "#             print(ner_tags)\n",
    "        except:\n",
    "            pass\n",
    "    try:        \n",
    "        for each_tag in ner_tags:\n",
    "            token_ = each_tag[2].lower()\n",
    "            ner_ = each_tag[3]\n",
    "\n",
    "            if ner_ == 'GP':\n",
    "                gene_patterns_.add(token_)\n",
    "            elif ner_ == 'OG':\n",
    "                organisms_patterns_.add(token_)\n",
    "            elif ner_ == 'DS':\n",
    "                disease_patterns_.add(token_)  \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "            \n",
    "for index, row in tqdm(dev_df.iterrows(), total=dev_df.shape[0]):\n",
    "    ner_tags = row['ner']\n",
    "    if ner_tags:\n",
    "        try:\n",
    "            ner_tags = literal_eval(ner_tags)\n",
    "#             print(ner_tags)\n",
    "        except:\n",
    "            pass\n",
    "    try:        \n",
    "        for each_tag in ner_tags:\n",
    "            token_ = each_tag[2].lower()\n",
    "            ner_ = each_tag[3]\n",
    "\n",
    "            if ner_ == 'GP':\n",
    "                gene_patterns_.add(token_)\n",
    "            elif ner_ == 'OG':\n",
    "                organisms_patterns_.add(token_)\n",
    "            elif ner_ == 'DS':\n",
    "                disease_patterns_.add(token_)  \n",
    "    except:\n",
    "        pass               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the best model\n",
    "\n",
    "best_model_path = '/nfs/gns/literature/Santosh_Tirunagari/GitHub/spacy_models/pubmed-pmc-lg/best'\n",
    "\n",
    "nlp = spacy.load(best_model_path)\n",
    "print(\"Loaded from\", best_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tstart = time.time()\n",
    "\n",
    "entity_matcher_GP = EntityMatcher(nlp, list(gene_patterns_), 'GP')\n",
    "\n",
    "tend = time.time()\n",
    "t_elapsed = round(tend - tstart)\n",
    "print(t_elapsed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tstart = time.time()\n",
    "\n",
    "entity_matcher_DS = EntityMatcher(nlp, list(disease_patterns_), 'DS')\n",
    "\n",
    "tend = time.time()\n",
    "t_elapsed = round(tend - tstart)\n",
    "print(t_elapsed) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tstart = time.time()\n",
    "\n",
    "entity_matcher_OG = EntityMatcher(nlp, list(organisms_patterns), 'OG')\n",
    "\n",
    "tend = time.time()\n",
    "t_elapsed = round(tend - tstart)\n",
    "print(t_elapsed) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100%|██████████| 347929/347929 [16:34<00:00, 349.81it/s] \n",
    "# 100%|██████████| 59088/59088 [02:47<00:00, 352.10it/s]\n",
    "\n",
    "entity_matcher_GP.name = 'GP_dict'\n",
    "entity_matcher_DS.name = 'DS_dict'\n",
    "entity_matcher_OG.name = 'OG_dict'\n",
    "\n",
    "# nlp.add_pipe(entity_matcher_GP, before='ner')\n",
    "# nlp.add_pipe(entity_matcher_DS, before='ner')\n",
    "# nlp.add_pipe(entity_matcher_OG, after='GP_dict')\n",
    "# nlp.add_pipe(entity_matcher_DS, after='OG_dict')\n",
    "\n",
    "\n",
    "nlp.add_pipe(entity_matcher_GP, before='ner')\n",
    "nlp.add_pipe(entity_matcher_DS, before='ner')\n",
    "# nlp.add_pipe(entity_matcher_OG, after='GP_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp.add_pipe(entity_matcher_OG, after='GP_dict')\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Interspecific tumor PT overgrowth phenocopies the s.Crocodylurus tarahumaras-frogs female gametophytic mutants feronia/sirène (fer/srn), lorelei (lre), nortia (nta), TuRan (tun), evAn (evn), and Zea mays embryo sac 4 (ZmES4) RNAi-lines1314151617181920, which are defective in the reception of intraspecific PTs. '\n",
    "# text = 'These results indicate the dentate gyrus is mostly comprised of mature neurons (NeuN), along with a smaller population of precursor cells (nestin) and newly differentiated neurons (DCX), which corresponds to prior findings examining the relative number of each cell population in the dentate gyrus, indicating the proportion of cells labeled by the sensor approximately reflects physiological proportions [21, 22]. '\n",
    "sentence =nlp(text)\n",
    "\n",
    "print(sentence)\n",
    "for ent in sentence.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"/nfs/gns/literature/Santosh_Tirunagari/GitHub/spacy_models/dictionary_en-pubmed-pmc-lg/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer, wordpunct_tokenize\n",
    "\n",
    "\n",
    "def convert2IOB(text_data, ner_tags):\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "\n",
    "    tokens = []\n",
    "    ners = []\n",
    "    spans = []\n",
    "\n",
    "    split_text = tokenizer.tokenize(text_data)\n",
    "    span_text = list(tokenizer.span_tokenize(text_data))\n",
    "    # for each word token append 'O'\n",
    "    arr = ['O'] * len(split_text)\n",
    "\n",
    "    if ner_tags:\n",
    "        try:\n",
    "            ner_tags = literal_eval(ner_tags)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    elif isinstance(ner_tags, float) or ner_tags is None:\n",
    "        return zip(split_text, arr)\n",
    "    else:\n",
    "        return zip(split_text, arr)\n",
    "\n",
    "    for each_tag in ner_tags:\n",
    "        span_list = (each_tag[0], each_tag[1])\n",
    "        token_list = wordpunct_tokenize(each_tag[2])\n",
    "        ner_list = wordpunct_tokenize(each_tag[3])\n",
    "\n",
    "        if (len(token_list) > len(ner_list)):\n",
    "            ner_list = len(token_list) * ner_list\n",
    "        for i in range(0, len(ner_list)):\n",
    "            # The logic here is look for the first B-tag and then append I-tag next\n",
    "            if (i == 0):\n",
    "                ner_list[i] = 'B-' + ner_list[i]\n",
    "            else:\n",
    "                ner_list[i] = 'I-' + ner_list[i]\n",
    "\n",
    "        tokens.append(token_list)\n",
    "        ners.append(ner_list)\n",
    "        spans.append(span_list)\n",
    "\n",
    "    split_token_span_list = list(zip(split_text, span_text))\n",
    "    span_ner_list = list(zip(spans, ners))\n",
    "\n",
    "    sub_spans = []  # get sub spans from the full spans of the ner\n",
    "\n",
    "    for each_span_ner_list in span_ner_list:\n",
    "        # in full range ner e.g., [144, 150, 'GM-CSF', 'GP']\n",
    "        count = 0\n",
    "        # count is to keep track of the B, I, sub tags in the ner list\n",
    "        for each_token in split_token_span_list:\n",
    "            sub_spans_ = find_sub_span(each_token[1], each_span_ner_list[0])\n",
    "            if sub_spans_:\n",
    "                sub_spans.append([sub_spans_, each_span_ner_list[1][count]])\n",
    "                count = count + 1\n",
    "\n",
    "    for i, each_span_token in enumerate(split_token_span_list):\n",
    "        for each_ner_span in sub_spans:\n",
    "            if each_span_token[1] == each_ner_span[0]:\n",
    "                arr[i] = ''.join(each_ner_span[1])\n",
    "\n",
    "    return zip(split_text, arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = '/nfs/gns/literature/Santosh_Tirunagari/GitHub/spacy_models/dictionary_en-pubmed-pmc-lg/'\n",
    "\n",
    "nlp = spacy.load(best_model_path)\n",
    "print(\"Loaded from\", best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from ast import literal_eval\n",
    "import csv\n",
    "\n",
    "\n",
    "def find_sub_span(sub_span_range, full_spans_range):\n",
    "    # if a sub span is present in full span return it\n",
    "    if sub_span_range[0] in range(full_spans_range[0], full_spans_range[1]):\n",
    "        return sub_span_range\n",
    "    \n",
    "    \n",
    "\n",
    "test_set = '/nfs/gns/literature/machine-learning/evaluation/300articles/CSV formats/test.csv'\n",
    "\n",
    "result_path = '/nfs/gns/literature/machine-learning/evaluation/300articles/ML-NER/Dictionary_fused_en_pubmed_pmc_lg/'\n",
    "\n",
    "\n",
    "df_45 = pd.read_csv(test_set, sep = '\\t', names = ['pmcid', 'sentence','ner'])\n",
    "\n",
    "with open(result_path + 'Dictionary_fused_en_pubmed_pmc_lg_iob.csv', 'w', newline='\\n') as f1:\n",
    "    ml_writer = csv.writer(f1, delimiter='\\t', lineterminator='\\n')\n",
    "    \n",
    "    for index, row in tqdm(df_45.iterrows(), total=df_45.shape[0]):\n",
    "#         print(row['ner'])\n",
    "        text = row['sentence'] # .encode('utf-8').decode('utf-8')\n",
    "#         print(text)\n",
    "        sentence = nlp(text)\n",
    "        ml_ner =[]\n",
    "        for ent in sentence.ents:\n",
    "            ml_ner.append([ent.start_char, ent.end_char, ent.text, ent.label_])\n",
    "\n",
    "        tagged_tokens = convert2IOB(text, ml_ner)\n",
    "\n",
    "        for each_word in tagged_tokens:\n",
    "            ml_writer.writerow(list(each_word))\n",
    "        ml_writer.writerow('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test best model performance on 2000 set\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "epmc_annotations_2000 = '/nfs/gns/literature/machine-learning/evaluation/2000articles/europePMC-NER/annotations_API/full_sentences/tagged_sentences/Europe_PMC_annotation.csv'\n",
    "\n",
    "result_path = '/nfs/gns/literature/machine-learning/evaluation/2000articles/ML-NER/Dictionary_fused_en_pubmed_pmc_lg/'\n",
    "\n",
    "\n",
    "df_2000 = pd.read_csv(epmc_annotations_2000, sep = '\\t', names = ['pmcid', 'section', 'sentence','ner'])\n",
    "\n",
    "\n",
    "with open(result_path + 'Dictionary_and_trainingset_en-pubmed-pmc-lg_2000_iob.csv', 'a', newline='\\n') as f1:\n",
    "    ml_writer = csv.writer(f1, delimiter='\\t', lineterminator='\\n')\n",
    "    \n",
    "    for index, row in tqdm(df_2000.iterrows(), total=df_2000.shape[0]):\n",
    "#         print(row['ner'])\n",
    "        text = row['sentence'].encode('utf-8').decode('utf-8')\n",
    "#         print(text)\n",
    "        sentence = nlp(text)\n",
    "        ml_ner =[]\n",
    "        for ent in sentence.ents:\n",
    "            ml_ner.append([ent.start_char, ent.end_char, ent.text, ent.label_])\n",
    "\n",
    "        tagged_tokens = convert2IOB(text, ml_ner)\n",
    "\n",
    "        for each_word in tagged_tokens:\n",
    "            ml_writer.writerow(list(each_word))\n",
    "        ml_writer.writerow('')\n",
    "        \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scispcacy",
   "language": "python",
   "name": "scispcacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
