{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "import argparse\n",
    "import tqdm\n",
    "import spacy\n",
    "from spacy.gold import minibatch\n",
    "from spacy.language import Language\n",
    "from spacy import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scispacy.data_util import read_full_med_mentions, read_ner_from_tsv\n",
    "from scispacy.per_class_scorer import PerClassScorer\n",
    "from scispacy.train_utils import evaluate_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy_transformers import TransformersLanguage, TransformersWordPiecer, TransformersTok2Vec\n",
    "\n",
    "name = \"scibert-scivocab-uncased\"\n",
    "path = \"/nfs/gns/literature/Santosh_Tirunagari/pretrained_word_embeddings/scibert_scivocab_uncased\"\n",
    "\n",
    "nlp = TransformersLanguage(trf_name=name, meta={\"lang\": \"en\"})\n",
    "nlp.add_pipe(nlp.create_pipe(\"sentencizer\"))\n",
    "nlp.add_pipe(TransformersWordPiecer.from_pretrained(nlp.vocab, path))\n",
    "nlp.add_pipe(TransformersTok2Vec.from_pretrained(nlp.vocab, path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ner(output_dir: str,\n",
    "              train_data_path: str,\n",
    "              dev_data_path: str,\n",
    "              test_data_path: str,\n",
    "              run_test: bool = None,\n",
    "#               model: str = None,\n",
    "              n_iter: int = 10,\n",
    "              meta_overrides: str = None):\n",
    "\n",
    "    util.fix_random_seed(util.env_opt(\"seed\", 0))\n",
    "    train_data = read_ner_from_tsv(train_data_path)\n",
    "    dev_data = read_ner_from_tsv(dev_data_path)\n",
    "    test_data = read_ner_from_tsv(test_data_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    if run_test:\n",
    "#         nlp = spacy.load(model)\n",
    "#         print(\"Loaded model '%s'\" % model)\n",
    "        evaluate_ner(nlp, dev_data, dump_path=os.path.join(output_dir, \"dev_metrics.json\"))\n",
    "        evaluate_ner(nlp, test_data, dump_path=os.path.join(output_dir, \"test_metrics.json\"))\n",
    "    else:\n",
    "        train(train_data, dev_data, test_data, output_dir, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data, dev_data, test_data, output_dir, n_iter):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    \n",
    "    original_tokenizer = nlp.tokenizer\n",
    "\n",
    "#     nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names and \"parser\" in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, after=\"parser\")\n",
    "    elif 'ner' not in nlp.pipe_names and \"tagger\" in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, after=\"tagger\")\n",
    "    elif 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in train_data:\n",
    "        for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "\n",
    "    dropout_rates = util.decaying(util.env_opt('dropout_from', 0.2),\n",
    "                                  util.env_opt('dropout_to', 0.2),\n",
    "                                  util.env_opt('dropout_decay', 0.005))\n",
    "    batch_sizes = util.compounding(util.env_opt('batch_from', 1),\n",
    "                                   util.env_opt('batch_to', 32),\n",
    "                                   util.env_opt('batch_compound', 1.001))\n",
    "\n",
    "    optimizer = nlp.begin_training()\n",
    "    best_epoch = 0\n",
    "    best_f1 = 0\n",
    "    for i in range(n_iter):\n",
    "        print(str(i)+'--'+str(n_iter))\n",
    "        random.shuffle(train_data)\n",
    "        count = 0\n",
    "        losses = {}\n",
    "        total = len(train_data)\n",
    "\n",
    "        with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "            with tqdm.tqdm(total=total, leave=True) as pbar:\n",
    "                for batch in minibatch(train_data, size=batch_sizes):\n",
    "                    docs, golds = zip(*batch)\n",
    "                    nlp.update(docs, golds, sgd=optimizer,\n",
    "                               losses=losses, drop=next(dropout_rates))\n",
    "                    pbar.update(len(batch))\n",
    "                    if count % 100 == 0 and count > 0:\n",
    "                        print('sum loss: %s' % losses['ner'])\n",
    "                    count += 1\n",
    "\n",
    "        # save model to output directory\n",
    "        output_dir_path = Path(output_dir + \"/\" + str(i))\n",
    "        if not output_dir_path.exists():\n",
    "            output_dir_path.mkdir()\n",
    "\n",
    "        with nlp.use_params(optimizer.averages):\n",
    "            nlp.tokenizer = original_tokenizer\n",
    "            nlp.to_disk(output_dir_path)\n",
    "            print(\"Saved model to\", output_dir_path)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir_path)\n",
    "        nlp2 = util.load_model_from_path(output_dir_path)\n",
    "\n",
    "        metrics = evaluate_ner(nlp2, dev_data)\n",
    "        if metrics[\"f1-measure-overall\"] > best_f1:\n",
    "            best_f1 = metrics[\"f1-measure-overall\"]\n",
    "            best_epoch = i\n",
    "    # save model to output directory\n",
    "    best_model_path = Path(output_dir + \"/\" + \"best\")\n",
    "    print(f\"Best Epoch: {best_epoch} of {n_iter}\")\n",
    "    if os.path.exists(best_model_path):\n",
    "        shutil.rmtree(best_model_path)\n",
    "    shutil.copytree(os.path.join(output_dir, str(best_epoch)),\n",
    "                    best_model_path)\n",
    "\n",
    "    # test the saved model\n",
    "    print(\"Loading from\", best_model_path)\n",
    "    nlp2 = util.load_model_from_path(best_model_path)\n",
    "\n",
    "    evaluate_ner(nlp2, dev_data, dump_path=os.path.join(output_dir, \"dev_metrics.json\"))\n",
    "    evaluate_ner(nlp2, test_data, dump_path=os.path.join(output_dir, \"test_metrics.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_dir = '/nfs/gns/literature/Santosh_Tirunagari/GitHub/spacy_models/scibert-scivocab-uncased/'\n",
    "train_data_path = '/nfs/gns/literature/machine-learning/Datasets/NER_Datasets/EBI_standard-IOB/train.csv'\n",
    "dev_data_path = '/nfs/gns/literature/machine-learning/Datasets/NER_Datasets/EBI_standard-IOB/dev.csv'\n",
    "test_data_path = '/nfs/gns/literature/machine-learning/Datasets/NER_Datasets/EBI_standard-IOB/test.csv'\n",
    "run_test = False\n",
    "# model_path = '/nfs/gns/literature/Santosh_Tirunagari/pretrained_word_embeddings/en-europepmc-lg' # None #'en_core_sci_md'\n",
    "iterations = 5\n",
    "meta_overrides = '/nfs/gns/literature/Santosh_Tirunagari/GitHub/scispacy/data/EPMC_ner.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/79401 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0--5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"[E001] No component 'trf_tok2vec' found in pipeline. Available names: ['ner']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-18fe05ef4f69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m               \u001b[0mrun_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#               model_path,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m               \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             )\n",
      "\u001b[0;32m<ipython-input-14-64b56c7bc398>\u001b[0m in \u001b[0;36mtrain_ner\u001b[0;34m(output_dir, train_data_path, dev_data_path, test_data_path, run_test, n_iter, meta_overrides)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mevaluate_ner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdump_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test_metrics.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-aafcdadb8592>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_data, dev_data, test_data, output_dir, n_iter)\u001b[0m\n\u001b[1;32m     50\u001b[0m                     \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgolds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                     nlp.update(docs, golds, sgd=optimizer,\n\u001b[0;32m---> 52\u001b[0;31m                                losses=losses, drop=next(dropout_rates))\n\u001b[0m\u001b[1;32m     53\u001b[0m                     \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy_transformers/language.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, docs, golds, drop, sgd, losses, component_cfg)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mwp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mtok2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPIPES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtok2vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mnew_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mnew_golds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mget_pipe\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpipe_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"[E001] No component 'trf_tok2vec' found in pipeline. Available names: ['ner']\""
     ]
    }
   ],
   "source": [
    "train_ner(model_output_dir,\n",
    "              train_data_path,\n",
    "              dev_data_path,\n",
    "              test_data_path,\n",
    "              run_test,\n",
    "#               model_path,\n",
    "              iterations,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test best model performance on test set\n",
    "best_model_path = '/nfs/gns/literature/Santosh_Tirunagari/GitHub/spacy_models/en-europepmc-lg/best'\n",
    "\n",
    "print(\"Loading from\", best_model_path)\n",
    "nlp2 = util.load_model_from_path(best_model_path)\n",
    "\n",
    "# nlp2.tokenizer = WhitespaceTokenizer(nlp2.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[113, 119, 'nortia', 'GP'], \n",
    "# [179, 184, 'ZmES4', 'GP'], \n",
    "# [146, 149, 'evn', 'GP'], [121, 124, 'nta', 'GP'], \n",
    "# [140, 144, 'evan', 'GP'], [127, 132, 'turan', 'GP'], \n",
    "# [72, 79, 'feronia', 'GP'], [88, 91, 'fer', 'GP'], \n",
    "# [107, 110, 'lre', 'GP'], [98, 105, 'lorelei', 'GP'], \n",
    "# [80, 86, 'sirène', 'GP'], [156, 177, 'Zea mays embryo sac 4', 'GP'], \n",
    "# [92, 95, 'srn', 'GP'], [134, 137, 'tun', 'GP']]\n",
    "\n",
    "text = 'Interspecific PT overgrowth phenocopies the female gametophytic mutants feronia/sirène (fer/srn), lorelei (lre), nortia (nta), turan (tun), evan (evn), and Zea mays embryo sac 4 (ZmES4) RNAi-lines1314151617181920, which are defective in the reception of intraspecific PTs. '\n",
    "sentence = nlp2(text)\n",
    "\n",
    "print(sentence)\n",
    "for ent in sentence.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer, wordpunct_tokenize\n",
    "\n",
    "\n",
    "def convert2IOB(text_data, ner_tags):\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "\n",
    "    tokens = []\n",
    "    ners = []\n",
    "    spans = []\n",
    "\n",
    "    split_text = tokenizer.tokenize(text_data)\n",
    "    span_text = list(tokenizer.span_tokenize(text_data))\n",
    "    # for each word token append 'O'\n",
    "    arr = ['O'] * len(split_text)\n",
    "\n",
    "    if ner_tags:\n",
    "        try:\n",
    "            ner_tags = literal_eval(ner_tags)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    elif isinstance(ner_tags, float) or ner_tags is None:\n",
    "        return zip(split_text, arr)\n",
    "    else:\n",
    "        return zip(split_text, arr)\n",
    "\n",
    "    for each_tag in ner_tags:\n",
    "        span_list = (each_tag[0], each_tag[1])\n",
    "        token_list = wordpunct_tokenize(each_tag[2])\n",
    "        ner_list = wordpunct_tokenize(each_tag[3])\n",
    "\n",
    "        if (len(token_list) > len(ner_list)):\n",
    "            ner_list = len(token_list) * ner_list\n",
    "        for i in range(0, len(ner_list)):\n",
    "            # The logic here is look for the first B-tag and then append I-tag next\n",
    "            if (i == 0):\n",
    "                ner_list[i] = 'B-' + ner_list[i]\n",
    "            else:\n",
    "                ner_list[i] = 'I-' + ner_list[i]\n",
    "\n",
    "        tokens.append(token_list)\n",
    "        ners.append(ner_list)\n",
    "        spans.append(span_list)\n",
    "\n",
    "    split_token_span_list = list(zip(split_text, span_text))\n",
    "    span_ner_list = list(zip(spans, ners))\n",
    "\n",
    "    sub_spans = []  # get sub spans from the full spans of the ner\n",
    "\n",
    "    for each_span_ner_list in span_ner_list:\n",
    "        # in full range ner e.g., [144, 150, 'GM-CSF', 'GP']\n",
    "        count = 0\n",
    "        # count is to keep track of the B, I, sub tags in the ner list\n",
    "        for each_token in split_token_span_list:\n",
    "            sub_spans_ = find_sub_span(each_token[1], each_span_ner_list[0])\n",
    "            if sub_spans_:\n",
    "                sub_spans.append([sub_spans_, each_span_ner_list[1][count]])\n",
    "                count = count + 1\n",
    "\n",
    "    for i, each_span_token in enumerate(split_token_span_list):\n",
    "        for each_ner_span in sub_spans:\n",
    "            if each_span_token[1] == each_ner_span[0]:\n",
    "                arr[i] = ''.join(each_ner_span[1])\n",
    "\n",
    "    return zip(split_text, arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from ast import literal_eval\n",
    "import csv\n",
    "\n",
    "\n",
    "def find_sub_span(sub_span_range, full_spans_range):\n",
    "    # if a sub span is present in full span return it\n",
    "    if sub_span_range[0] in range(full_spans_range[0], full_spans_range[1]):\n",
    "        return sub_span_range\n",
    "    \n",
    "    \n",
    "\n",
    "test_set = '/nfs/gns/literature/machine-learning/evaluation/300articles/CSV formats/test.csv'\n",
    "\n",
    "result_path = '/nfs/gns/literature/machine-learning/evaluation/300articles/ML-NER/en-europepmc-lg/'\n",
    "\n",
    "\n",
    "df_45 = pd.read_csv(test_set, sep = '\\t', names = ['pmcid', 'sentence','ner'])\n",
    "\n",
    "with open(result_path + 'en-europepmc-lg_iob.csv', 'a', newline='\\n') as f1:\n",
    "    ml_writer = csv.writer(f1, delimiter='\\t', lineterminator='\\n')\n",
    "    \n",
    "    for index, row in tqdm(df_45.iterrows(), total=df_45.shape[0]):\n",
    "#         print(row['ner'])\n",
    "        text = row['sentence'].encode('utf-8').decode('utf-8')\n",
    "#         print(text)\n",
    "        sentence = nlp2(text.strip())\n",
    "        ml_ner =[]\n",
    "        for ent in sentence.ents:\n",
    "            ml_ner.append([ent.start_char, ent.end_char, ent.text, ent.label_])\n",
    "\n",
    "        tagged_tokens = convert2IOB(text, ml_ner)\n",
    "\n",
    "        for each_word in tagged_tokens:\n",
    "            ml_writer.writerow(list(each_word))\n",
    "        ml_writer.writerow('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test best model performance on 2000 set\n",
    "\n",
    "\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "epmc_annotations_2000 = '/nfs/gns/literature/machine-learning/evaluation/2000articles/europePMC-NER/annotations_API/full_sentences/tagged_sentences/Europe_PMC_annotation.csv'\n",
    "\n",
    "result_path = '/nfs/gns/literature/machine-learning/evaluation/2000articles/ML-NER/en-pubmed-pmc-lg/'\n",
    "\n",
    "\n",
    "df_2000 = pd.read_csv(epmc_annotations_2000, sep = '\\t', names = ['pmcid', 'section', 'sentence','ner'])\n",
    "\n",
    "\n",
    "with open(result_path + 'en-pubmed-pmc-lg_2000_iob.csv', 'a', newline='\\n') as f1:\n",
    "    ml_writer = csv.writer(f1, delimiter='\\t', lineterminator='\\n')\n",
    "    \n",
    "    for index, row in tqdm(df_2000.iterrows(), total=df_2000.shape[0]):\n",
    "#         print(row['ner'])\n",
    "        text = row['sentence'].encode('utf-8').decode('utf-8')\n",
    "#         print(text)\n",
    "        sentence = nlp2(text.strip())\n",
    "        ml_ner =[]\n",
    "        for ent in sentence.ents:\n",
    "            ml_ner.append([ent.start_char, ent.end_char, ent.text, ent.label_])\n",
    "\n",
    "        tagged_tokens = convert2IOB(text, ml_ner)\n",
    "\n",
    "        for each_word in tagged_tokens:\n",
    "            ml_writer.writerow(list(each_word))\n",
    "        ml_writer.writerow('')\n",
    "        \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scispcacy",
   "language": "python",
   "name": "scispcacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
