{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "import argparse\n",
    "import tqdm\n",
    "import spacy\n",
    "from spacy.gold import minibatch\n",
    "from spacy.language import Language\n",
    "from spacy import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scispacy.data_util import read_full_med_mentions, read_ner_from_tsv\n",
    "from scispacy.per_class_scorer import PerClassScorer\n",
    "from scispacy.train_utils import evaluate_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ner(output_dir: str,\n",
    "              train_data_path: str,\n",
    "              dev_data_path: str,\n",
    "              test_data_path: str,\n",
    "              run_test: bool = None,\n",
    "              model: str = None,\n",
    "              n_iter: int = 10,\n",
    "              meta_overrides: str = None):\n",
    "\n",
    "    util.fix_random_seed(util.env_opt(\"seed\", 0))\n",
    "    train_data = read_ner_from_tsv(train_data_path)\n",
    "    dev_data = read_ner_from_tsv(dev_data_path)\n",
    "    test_data = read_ner_from_tsv(test_data_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    if run_test:\n",
    "        nlp = spacy.load(model)\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "        evaluate_ner(nlp, dev_data, dump_path=os.path.join(output_dir, \"dev_metrics.json\"))\n",
    "        evaluate_ner(nlp, test_data, dump_path=os.path.join(output_dir, \"test_metrics.json\"))\n",
    "    else:\n",
    "        train(model, train_data, dev_data, test_data, output_dir, n_iter, meta_overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, dev_data, test_data, output_dir, n_iter, meta_overrides):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    \n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    if meta_overrides is not None:\n",
    "        metadata = json.load(open(meta_overrides))\n",
    "        nlp.meta.update(metadata)\n",
    "\n",
    "    original_tokenizer = nlp.tokenizer\n",
    "\n",
    "#     nlp.tokenizer = nlp_en.tokenizer\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names and \"parser\" in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, after=\"parser\")\n",
    "    elif 'ner' not in nlp.pipe_names and \"tagger\" in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, after=\"tagger\")\n",
    "    elif 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in train_data:\n",
    "        for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "\n",
    "    dropout_rates = util.decaying(util.env_opt('dropout_from', 0.2),\n",
    "                                  util.env_opt('dropout_to', 0.2),\n",
    "                                  util.env_opt('dropout_decay', 0.005))\n",
    "    batch_sizes = util.compounding(util.env_opt('batch_from', 1),\n",
    "                                   util.env_opt('batch_to', 32),\n",
    "                                   util.env_opt('batch_compound', 1.001))\n",
    "\n",
    "    optimizer = nlp.begin_training()\n",
    "    best_epoch = 0\n",
    "    best_f1 = 0\n",
    "    for i in range(n_iter):\n",
    "        print(str(i)+'--'+str(n_iter))\n",
    "        random.shuffle(train_data)\n",
    "        count = 0\n",
    "        losses = {}\n",
    "        total = len(train_data)\n",
    "\n",
    "        with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "            with tqdm.tqdm(total=total, leave=True) as pbar:\n",
    "                for batch in minibatch(train_data, size=batch_sizes):\n",
    "                    docs, golds = zip(*batch)\n",
    "                    nlp.update(docs, golds, sgd=optimizer,\n",
    "                               losses=losses, drop=next(dropout_rates))\n",
    "                    pbar.update(len(batch))\n",
    "                    if count % 100 == 0 and count > 0:\n",
    "                        print('sum loss: %s' % losses['ner'])\n",
    "                    count += 1\n",
    "\n",
    "        # save model to output directory\n",
    "        output_dir_path = Path(output_dir + \"/\" + str(i))\n",
    "        if not output_dir_path.exists():\n",
    "            output_dir_path.mkdir()\n",
    "\n",
    "        with nlp.use_params(optimizer.averages):\n",
    "            nlp.tokenizer = original_tokenizer\n",
    "            nlp.to_disk(output_dir_path)\n",
    "            print(\"Saved model to\", output_dir_path)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir_path)\n",
    "        nlp2 = util.load_model_from_path(output_dir_path)\n",
    "#         nlp2.tokenizer = nlp_en.tokenizer\n",
    "\n",
    "        metrics = evaluate_ner(nlp2, dev_data)\n",
    "        if metrics[\"f1-measure-overall\"] > best_f1:\n",
    "            best_f1 = metrics[\"f1-measure-overall\"]\n",
    "            best_epoch = i\n",
    "    # save model to output directory\n",
    "    best_model_path = Path(output_dir + \"/\" + \"best\")\n",
    "    print(f\"Best Epoch: {best_epoch} of {n_iter}\")\n",
    "    if os.path.exists(best_model_path):\n",
    "        shutil.rmtree(best_model_path)\n",
    "    shutil.copytree(os.path.join(output_dir, str(best_epoch)),\n",
    "                    best_model_path)\n",
    "\n",
    "    # test the saved model\n",
    "    print(\"Loading from\", best_model_path)\n",
    "    nlp2 = util.load_model_from_path(best_model_path)\n",
    "#     nlp2.tokenizer = nlp_en.tokenizer\n",
    "\n",
    "    evaluate_ner(nlp2, dev_data, dump_path=os.path.join(output_dir, \"dev_metrics.json\"))\n",
    "    evaluate_ner(nlp2, test_data, dump_path=os.path.join(output_dir, \"test_metrics.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_dir = '/nfs/gns/literature/Santosh_Tirunagari/GitHub/spacy_models/public/public-BC2GM-en-pubmed-pmc-lg/'\n",
    "train_data_path = '/nfs/gns/literature/machine-learning/Datasets/NER_Datasets/BC2GM-IOB/train.tsv'\n",
    "dev_data_path = '/nfs/gns/literature/machine-learning/Datasets/NER_Datasets/BC2GM-IOB/devel.tsv'\n",
    "test_data_path = '/nfs/gns/literature/machine-learning/Datasets/NER_Datasets/BC2GM-IOB/test.tsv'\n",
    "run_test = False\n",
    "model_path = '/nfs/gns/literature/Santosh_Tirunagari/pretrained_word_embeddings/models/pubmed-pmc/' # None #'en_core_sci_md'\n",
    "iterations = 10\n",
    "meta_overrides = '/nfs/gns/literature/Santosh_Tirunagari/GitHub/scispacy/data/EPMC_ner.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model '/nfs/gns/literature/Santosh_Tirunagari/pretrained_word_embeddings/models/pubmed-pmc/'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/12574 [00:00<13:42, 15.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0--10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 104/12574 [00:05<09:28, 21.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum loss: 427.87183735749784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 204/12574 [00:09<10:14, 20.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum loss: 1024.7098525823785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 304/12574 [00:14<09:21, 21.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum loss: 1478.4263177531939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 403/12574 [00:19<09:41, 20.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum loss: 1742.4253785823503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 503/12574 [00:23<08:56, 22.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum loss: 1995.88675419521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 604/12574 [00:28<08:42, 22.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum loss: 2268.752457171292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 718/12574 [00:32<04:51, 40.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum loss: 2516.4872732253502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 910/12574 [00:36<04:17, 45.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum loss: 2942.7211757482855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1114/12574 [00:41<03:56, 48.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum loss: 3520.90184950706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1314/12574 [00:45<04:39, 40.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum loss: 4034.751620588914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1521/12574 [00:50<03:34, 51.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum loss: 4616.583646998284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1818/12574 [00:55<02:37, 68.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum loss: 5220.106285184183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 2115/12574 [00:59<02:47, 62.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum loss: 5891.228249731545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 2435/12574 [01:04<02:02, 82.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum loss: 6399.058188815519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 2831/12574 [01:08<02:02, 79.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum loss: 7055.080885267262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 3239/12574 [01:13<01:48, 85.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum loss: 7835.5801025713445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 3728/12574 [01:18<01:23, 105.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum loss: 8674.874266866766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 4245/12574 [01:23<01:12, 114.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum loss: 9362.216772793137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 4839/12574 [01:28<01:07, 114.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum loss: 10267.23287894361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 5489/12574 [01:33<00:53, 131.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum loss: 11198.310707841041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 6219/12574 [01:39<00:43, 145.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum loss: 12217.603227326732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 7015/12574 [01:44<00:38, 142.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum loss: 13278.029801352914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 7915/12574 [01:50<00:28, 162.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum loss: 14341.358171381928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 8917/12574 [01:56<00:21, 169.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum loss: 15760.599190648922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 10033/12574 [02:03<00:13, 186.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum loss: 17153.173019392892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 11269/12574 [02:10<00:07, 173.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum loss: 18690.89374062756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12574/12574 [02:17<00:00, 91.73it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to /nfs/gns/literature/Santosh_Tirunagari/GitHub/spacy_models/public/public-BC2GM-en-pubmed-pmc-lg/0\n",
      "Loading from /nfs/gns/literature/Santosh_Tirunagari/GitHub/spacy_models/public/public-BC2GM-en-pubmed-pmc-lg/0\n"
     ]
    }
   ],
   "source": [
    "train_ner(model_output_dir,\n",
    "              train_data_path,\n",
    "              dev_data_path,\n",
    "              test_data_path,\n",
    "              run_test,\n",
    "              model_path,\n",
    "              iterations,\n",
    "              meta_overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from /nfs/gns/literature/Santosh_Tirunagari/GitHub/spacy_models/pretrain_exp/best/\n"
     ]
    }
   ],
   "source": [
    "## Test best model performance on test set\n",
    "best_model_path = '/nfs/gns/literature/Santosh_Tirunagari/GitHub/spacy_models/pretrain_exp/best/'\n",
    "\n",
    "print(\"Loading from\", best_model_path)\n",
    "nlp2 = util.load_model_from_path(best_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['These', 'results', 'indicate', 'the', 'dentate', 'gyrus', 'is', 'mostly', 'comprised', 'of', 'mature', 'neurons', '(', 'NeuN', ')', ',', 'along', 'with', 'a', 'smaller', 'population', 'of', 'precursor', 'cells', '(', 'nestin', ')', 'and', 'newly', 'differentiated', 'neurons', '(', 'DCX', ')', ',', 'which', 'corresponds', 'to', 'prior', 'findings', 'examining', 'the', 'relative', 'number', 'of', 'each', 'cell', 'population', 'in', 'the', 'dentate', 'gyrus', ',', 'indicating', 'the', 'proportion', 'of', 'cells', 'labeled', 'by', 'the', 'sensor', 'approximately', 'reflects', 'physiological', 'proportions', '[', '21', ',', '22', ']', '.']\n"
     ]
    }
   ],
   "source": [
    "# from spacy.lang.en import English\n",
    "# nlp_en = English()\n",
    "text = 'These results indicate the dentate gyrus is mostly comprised of mature neurons (NeuN), along with a smaller population of precursor cells (nestin) and newly differentiated neurons (DCX), which corresponds to prior findings examining the relative number of each cell population in the dentate gyrus, indicating the proportion of cells labeled by the sensor approximately reflects physiological proportions [21, 22]. '\n",
    "\n",
    "doc = nlp2(text)\n",
    "print([token.text for token in doc])\n",
    "# ['i', \"'d\", 'like', 'to', 'see', 'a', 'movie', ',', 'from', 'jean', '-', 'juncker', 'or', 'someone', '?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interspecific PT overgrowth phenocopies the female gametophytic mutants feronia/sirène (fer/srn), lorelei (lre), nortia (nta), turan (tun), evan (evn), and Zea mays embryo sac 4 (ZmES4) RNAi-lines1314151617181920, which are defective in the reception of intraspecific PTs. \n",
      "feronia 72 79 OG\n",
      "srn 92 95 GP\n",
      "Zea mays 156 164 OG\n",
      "ZmES4 179 184 GP\n"
     ]
    }
   ],
   "source": [
    "# [[113, 119, 'nortia', 'GP'], \n",
    "# [179, 184, 'ZmES4', 'GP'], \n",
    "# [146, 149, 'evn', 'GP'], [121, 124, 'nta', 'GP'], \n",
    "# [140, 144, 'evan', 'GP'], [127, 132, 'turan', 'GP'], \n",
    "# [72, 79, 'feronia', 'GP'], [88, 91, 'fer', 'GP'], \n",
    "# [107, 110, 'lre', 'GP'], [98, 105, 'lorelei', 'GP'], \n",
    "# [80, 86, 'sirène', 'GP'], [156, 177, 'Zea mays embryo sac 4', 'GP'], \n",
    "# [92, 95, 'srn', 'GP'], [134, 137, 'tun', 'GP']]\n",
    "\n",
    "# NeuN 80 84 GP\n",
    "# nestin 139 145 GP\n",
    "# DCX 181 184 GP\n",
    "\n",
    "text = 'Interspecific PT overgrowth phenocopies the female gametophytic mutants feronia/sirène (fer/srn), lorelei (lre), nortia (nta), turan (tun), evan (evn), and Zea mays embryo sac 4 (ZmES4) RNAi-lines1314151617181920, which are defective in the reception of intraspecific PTs. '\n",
    "# text = 'These results indicate the dentate gyrus is mostly comprised of mature neurons (NeuN), along with a smaller population of precursor cells (nestin) and newly differentiated neurons (DCX), which corresponds to prior findings examining the relative number of each cell population in the dentate gyrus, indicating the proportion of cells labeled by the sensor approximately reflects physiological proportions [21, 22]. '\n",
    "sentence = nlp2(text)\n",
    "\n",
    "print(sentence)\n",
    "for ent in sentence.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer, wordpunct_tokenize\n",
    "\n",
    "\n",
    "def convert2IOB(text_data, ner_tags):\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "\n",
    "    tokens = []\n",
    "    ners = []\n",
    "    spans = []\n",
    "\n",
    "    split_text = tokenizer.tokenize(text_data)\n",
    "    span_text = list(tokenizer.span_tokenize(text_data))\n",
    "    # for each word token append 'O'\n",
    "    arr = ['O'] * len(split_text)\n",
    "\n",
    "    if ner_tags:\n",
    "        try:\n",
    "            ner_tags = literal_eval(ner_tags)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    elif isinstance(ner_tags, float) or ner_tags is None:\n",
    "        return zip(split_text, arr)\n",
    "    else:\n",
    "        return zip(split_text, arr)\n",
    "\n",
    "    for each_tag in ner_tags:\n",
    "        span_list = (each_tag[0], each_tag[1])\n",
    "        token_list = wordpunct_tokenize(each_tag[2])\n",
    "        ner_list = wordpunct_tokenize(each_tag[3])\n",
    "\n",
    "        if (len(token_list) > len(ner_list)):\n",
    "            ner_list = len(token_list) * ner_list\n",
    "        for i in range(0, len(ner_list)):\n",
    "            # The logic here is look for the first B-tag and then append I-tag next\n",
    "            if (i == 0):\n",
    "                ner_list[i] = 'B-' + ner_list[i]\n",
    "            else:\n",
    "                ner_list[i] = 'I-' + ner_list[i]\n",
    "\n",
    "        tokens.append(token_list)\n",
    "        ners.append(ner_list)\n",
    "        spans.append(span_list)\n",
    "\n",
    "    split_token_span_list = list(zip(split_text, span_text))\n",
    "    span_ner_list = list(zip(spans, ners))\n",
    "\n",
    "    sub_spans = []  # get sub spans from the full spans of the ner\n",
    "\n",
    "    for each_span_ner_list in span_ner_list:\n",
    "        # in full range ner e.g., [144, 150, 'GM-CSF', 'GP']\n",
    "        count = 0\n",
    "        # count is to keep track of the B, I, sub tags in the ner list\n",
    "        for each_token in split_token_span_list:\n",
    "            sub_spans_ = find_sub_span(each_token[1], each_span_ner_list[0])\n",
    "            if sub_spans_:\n",
    "                sub_spans.append([sub_spans_, each_span_ner_list[1][count]])\n",
    "                count = count + 1\n",
    "\n",
    "    for i, each_span_token in enumerate(split_token_span_list):\n",
    "        for each_ner_span in sub_spans:\n",
    "            if each_span_token[1] == each_ner_span[0]:\n",
    "                arr[i] = ''.join(each_ner_span[1])\n",
    "\n",
    "    return zip(split_text, arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17957/17957 [00:47<00:00, 381.09it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from ast import literal_eval\n",
    "import csv\n",
    "\n",
    "\n",
    "def find_sub_span(sub_span_range, full_spans_range):\n",
    "    # if a sub span is present in full span return it\n",
    "    if sub_span_range[0] in range(full_spans_range[0], full_spans_range[1]):\n",
    "        return sub_span_range\n",
    "    \n",
    "    \n",
    "\n",
    "test_set = '/nfs/gns/literature/machine-learning/evaluation/300articles/CSV formats/test.csv'\n",
    "\n",
    "result_path = '/nfs/gns/literature/machine-learning/evaluation/300articles/ML-NER/LAMO-en-pubmed-pmc-lg/'\n",
    "\n",
    "\n",
    "df_45 = pd.read_csv(test_set, sep = '\\t', names = ['pmcid', 'sentence','ner'])\n",
    "\n",
    "with open(result_path + 'en-pubmed-pmc_iob.csv', 'a', newline='\\n') as f1:\n",
    "    ml_writer = csv.writer(f1, delimiter='\\t', lineterminator='\\n')\n",
    "    \n",
    "    for index, row in tqdm(df_45.iterrows(), total=df_45.shape[0]):\n",
    "#         print(row['ner'])\n",
    "        text = row['sentence'] # .encode('utf-8').decode('utf-8')\n",
    "#         print(text)\n",
    "        sentence = nlp2(text)\n",
    "        ml_ner =[]\n",
    "        for ent in sentence.ents:\n",
    "            ml_ner.append([ent.start_char, ent.end_char, ent.text, ent.label_])\n",
    "\n",
    "        tagged_tokens = convert2IOB(text, ml_ner)\n",
    "\n",
    "        for each_word in tagged_tokens:\n",
    "            ml_writer.writerow(list(each_word))\n",
    "        ml_writer.writerow('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 801835/801835 [35:21<00:00, 377.88it/s]\n"
     ]
    }
   ],
   "source": [
    "## Test best model performance on 2000 set\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "epmc_annotations_2000 = '/nfs/gns/literature/machine-learning/evaluation/2000articles/europePMC-NER/annotations_API/full_sentences/tagged_sentences/Europe_PMC_annotation.csv'\n",
    "\n",
    "result_path = '/nfs/gns/literature/machine-learning/evaluation/2000articles/ML-NER/LMAO-en-pubmed-pmc-lg/'\n",
    "\n",
    "\n",
    "df_2000 = pd.read_csv(epmc_annotations_2000, sep = '\\t', names = ['pmcid', 'section', 'sentence','ner'])\n",
    "\n",
    "\n",
    "with open(result_path + 'LMAO-en-pubmed-pmc-lg_2000_iob.csv', 'a', newline='\\n') as f1:\n",
    "    ml_writer = csv.writer(f1, delimiter='\\t', lineterminator='\\n')\n",
    "    \n",
    "    for index, row in tqdm(df_2000.iterrows(), total=df_2000.shape[0]):\n",
    "#         print(row['ner'])\n",
    "        text = row['sentence'].encode('utf-8').decode('utf-8')\n",
    "#         print(text)\n",
    "        sentence = nlp2(text)\n",
    "        ml_ner =[]\n",
    "        for ent in sentence.ents:\n",
    "            ml_ner.append([ent.start_char, ent.end_char, ent.text, ent.label_])\n",
    "\n",
    "        tagged_tokens = convert2IOB(text, ml_ner)\n",
    "\n",
    "        for each_word in tagged_tokens:\n",
    "            ml_writer.writerow(list(each_word))\n",
    "        ml_writer.writerow('')\n",
    "        \n",
    "        \n",
    "# 100%|██████████| 801835/801835 [44:07<00:00, 302.91it/s] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:scispacy]",
   "language": "python",
   "name": "conda-env-scispacy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
