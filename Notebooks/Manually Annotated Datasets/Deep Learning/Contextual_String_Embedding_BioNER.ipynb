{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flair.datasets\n",
    "from pathlib import Path\n",
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "from flair.data_fetcher import NLPTaskDataFetcher, NLPTask\n",
    "from flair.embeddings import TokenEmbeddings, BertEmbeddings,  ELMoEmbeddings, FlairEmbeddings, WordEmbeddings, PooledFlairEmbeddings, StackedEmbeddings, CharacterEmbeddings, CharLMEmbeddings\n",
    "from flair.models import LanguageModel\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from typing import List\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disease\n",
    "EBI_data_folder = '/nfs/gns/literature/machine-learning/Datasets/NER_Datasets/EBI_standard-IOB'\n",
    "# define columns\n",
    "columns = {0: 'text', 1: 'ner'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EBI = ColumnCorpus(EBI_data_folder, columns, \n",
    "                              train_file='train.csv',  test_file='test.csv', dev_file='dev.csv', in_memory=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_type = 'ner'\n",
    "\n",
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = EBI.make_tag_dictionary(tag_type=tag_type)\n",
    "print(tag_dictionary.idx2item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_types: List[TokenEmbeddings] = [\n",
    "#                    WordEmbeddings('glove'),\n",
    "#                    FlairEmbeddings('news-forward'),\n",
    "#                    FlairEmbeddings('news-backward'),\n",
    "#                    CharacterEmbeddings(), \n",
    "#                    PooledFlairEmbeddings('pubmed-backward', pooling='min'),\n",
    "#                    PooledFlairEmbeddings('pubmed-forward', pooling='min')\n",
    "                    FlairEmbeddings('pubmed-forward'),\n",
    "                    FlairEmbeddings('pubmed-backward')\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. initialize sequence tagger\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        tag_type=tag_type,\n",
    "                                        use_crf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. initialize trainer\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "trainer: ModelTrainer = ModelTrainer(tagger, EBI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train('/nfs/gns/literature/Santosh_Tirunagari/GitHub/flair_models/ner/manual_annotated_dataset/only_flair_embeddings/',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              patience=3,\n",
    "              max_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence, Token\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "flair_model = SequenceTagger.load('/nfs/gns/literature/Santosh_Tirunagari/GitHub/flair_models/ner/manual_annotated_dataset/only_flair_embeddings/best-model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, jsonify, request\n",
    "\n",
    "def term_highlighter(text: str = None, terms: list = None) -> str:\n",
    "    if not text or not terms:\n",
    "        raise ValueError('Either the supplied text or list of terms and scores is empty or of type None')\n",
    "\n",
    "    used_term_strs = set()\n",
    "\n",
    "    for term in terms:\n",
    "\n",
    "        # because here  each term is something like 'ProjectSummary:AI' followed by a float score\n",
    "        # (this is just the format that solr returns the 'interesting terms' list).\n",
    "        term_str = term[3]\n",
    "\n",
    "        if type(term_str) != str:\n",
    "            continue\n",
    "\n",
    "        # prevent double highlighting\n",
    "        if term_str in used_term_strs:\n",
    "            continue\n",
    "\n",
    "        used_term_strs.add(term_str)\n",
    "        new_term = set()\n",
    "        for s in filter(lambda x: term_str in x, wordpunct_tokenize(text)):\n",
    "            new_term.add(s)\n",
    "        try:\n",
    "            new_term_str = list(new_term)[0]\n",
    "        except:\n",
    "            new_term_str = term_str\n",
    "\n",
    "        if term[2] == 'GP':\n",
    "            text = text.replace(new_term_str, '<span class=\\'GP\\'>' + new_term_str + '</span>')\n",
    "        elif term[2] == 'DS':\n",
    "            text = text.replace(new_term_str, '<span class=\\'DS\\'>' + new_term_str + '</span>')\n",
    "        elif term[2] == 'OG':\n",
    "            text = text.replace(new_term_str, '<span class=\\'OG\\'>' + new_term_str + '</span>')\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def pcse_ner_predictor(text_sentence):\n",
    "\n",
    "    data_dict ={}\n",
    "\n",
    "    if not text_sentence:\n",
    "        return jsonify({\n",
    "            'error': 'No parameters supplied',\n",
    "            \"status\": 400,\n",
    "            \"service\": 'pcse_ner_predictor'\n",
    "        })\n",
    "\n",
    "    sentence = Sentence(' '.join(wordpunct_tokenize(text_sentence)))\n",
    "    # print(sentence)\n",
    "    # print(text_sentence)\n",
    "    flair_model.predict(sentence)\n",
    "\n",
    "    try:\n",
    "        data_dict['tagged'] = sentence.to_dict(tag_type='ner')\n",
    "\n",
    "        text_input = data_dict['tagged']['text']\n",
    "\n",
    "        terms_entities = []\n",
    "        for each_entity in data_dict['tagged']['entities']:\n",
    "            terms_entities.append(\n",
    "                [each_entity['start_pos'], each_entity['end_pos'], each_entity['type'], each_entity['text']])\n",
    "\n",
    "        data_dict['highlighted_text'] = term_highlighter(text_input,terms_entities)\n",
    "\n",
    "        data_dict['status'] = 200\n",
    "    except:\n",
    "        data_dict['status'] = 400\n",
    "\n",
    "    if data_dict['status'] != 200:\n",
    "        data_dict['status'] = 400\n",
    "        return data_dict\n",
    "    else:\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "text_sentence = 'AS1411 Aptamer-functionalized liposome was successfully formulated and found to be nanosized. Flow cytometer and CLSM results demonstrated that Aptamer enhanced the targeting of carrier in the cancer cells via nucleolin-mediated transmembrane endocytosis pathway. The lipofectaminebased miR-29b showed a typical concentration-dependent cytotoxic effect in the cancer cells. LP-miR induced a significant reduction in the cell viability of A2780 cells compared to that of nontreated control, while LP-Mut (mutant loaded) did not have any effect on the cell viability indicating the importance of the specific gene sequencing. LP-miR induced a significant decrease in the green fluorescence which is indicative of the decrease in the cell viability. Simultaneously, higher PI positive cells were observed for LP-miR treated cancer cells in Live/Dead assay. Cells treated with LP-miR exhibited the brightest fluorescence indicating the presence of apoptotic cells. Significant increase in the Annexin-V+ cells and PI+ cells were observed for cell treated with LP-miR compared to that of non-treated control indicating the potential of miR-29b. This novel miR-29b-loaded Aptamer-directed liposome could potential serve as a new platform to improve the therapeutic outcome in ovarian cancers.'\n",
    "\n",
    "print(pcse_ner_predictor(text_sentence))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize, WordPunctTokenizer\n",
    "from typing import List\n",
    "def custom_tokenizer(text: str) -> List[Token]:\n",
    "    \"\"\"\n",
    "    Tokenizer based on space character only.\n",
    "    \"\"\"\n",
    "    tokens: List[Token] = []\n",
    "    \n",
    "    tokenizer = WordPunctTokenizer()\n",
    "\n",
    "    text = tokenizer.tokenize(text)\n",
    "\n",
    "    index = 0\n",
    "    for index, word in enumerate(text):\n",
    "            tokens.append(\n",
    "                Token(\n",
    "                    text=word, start_position=index, whitespace_after=False\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# custom_tokenizer(text_temp_)\n",
    "\n",
    "\n",
    "text_temp = 'The disparate diversity in immunoglobulin (Ig) repertoire has been a subject of fascination since the emergence of prototypic adaptive immune system in vertebrates. The carboxy terminus region of activation-induced cytidine deaminase (AID) has been well established in tetrapod lineage and is crucial for its function in class switch recombination (CSR) event of Ig diversification. The absence of CSR in the paraphyletic group of fish is probably due to changes in catalytic domain of AID and lack of cis-elements in IgH locus. Therefore, understanding the arrangement of Ig genes in IgH locus and functional facets of fish AID opens up new realms of unravelling the alternative mechanisms of isotype switching and antibody diversity. Further, the teleost AID has been recently reported to have potential of catalyzing CSR in mammalian B cells by complementing AID deficiency in them. In that context, the present review focuses on the recent advances regarding the generation of diversity in Ig repertoire in the absence of AID-regulated class switching in teleosts and the possible role of T cell-independent pathway involving B cell activating factor and a proliferation-inducing ligand in activation of CSR machinery.'\n",
    "\n",
    "sentence_1 = Sentence(text_temp, use_tokenizer=custom_tokenizer)\n",
    "\n",
    "flair_model.predict(sentence_1)\n",
    "\n",
    "manual_json = sentence_1.to_dict(tag_type='ner')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entities = []\n",
    "for ea_an in manual_json['entities']:\n",
    "    all_entities.append([ea_an['start_pos'],ea_an['end_pos'],ea_an['text'],ea_an['type']])\n",
    "    \n",
    "[text_temp]+all_entities   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize, WordPunctTokenizer\n",
    "from typing import List\n",
    "def custom_tokenizer(text: str) -> List[Token]:\n",
    "    \"\"\"\n",
    "    Tokenizer based on space character only.\n",
    "    \"\"\"\n",
    "    tokens: List[Token] = []\n",
    "    \n",
    "    tokenizer = WordPunctTokenizer()\n",
    "\n",
    "    text = tokenizer.tokenize(text)\n",
    "\n",
    "    index = 0\n",
    "    for index, word in enumerate(text):\n",
    "            tokens.append(\n",
    "                Token(\n",
    "                    text=word, start_position=index, whitespace_after=False\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# custom_tokenizer(text_temp_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = '/nfs/gns/literature/machine-learning/evaluation/2000articles/ML-NER/flair/'\n",
    "result_file_name = 'flair_2000.csv'\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize, WordPunctTokenizer\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_csv('/nfs/gns/literature/machine-learning/evaluation/2000articles/europePMC-NER/annotations_API/full_sentences/tagged_sentences/Europe_PMC_annotation.csv', sep='\\t', names = ['pmc_id', 'section','sentence', 'gt'])\n",
    "\n",
    "\n",
    "with open(result_path + result_file_name, 'a', newline='\\n') as f1:\n",
    "    public_writer = csv.writer(f1, delimiter='\\t', lineterminator='\\n')\n",
    "\n",
    "    for index_, each_annotation in tqdm(test_df.iterrows(), total=test_df.shape[0]):\n",
    "        text_temp = each_annotation['sentence'].encode('utf-8').decode('utf-8')\n",
    "        all_entities = []\n",
    "        sentence = Sentence(text_temp, use_tokenizer=custom_tokenizer)\n",
    "        flair_model.predict(sentence)\n",
    "        PCSE_json = sentence.to_dict(tag_type='ner')\n",
    "\n",
    "        for ea_an in PCSE_json['entities']:\n",
    "            all_entities.append([ea_an['start_pos'],ea_an['end_pos'],ea_an['text'],ea_an['type']])\n",
    "    \n",
    "        public_writer.writerow([text_temp]+[all_entities])\n",
    "#         all_entities = []\n",
    "        \n",
    "#         if (index_ == 10):\n",
    "#             break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in sentence:\n",
    "    # print what you need (text and NER value)\n",
    "    print(f\"{token.text}\\t{token.get_tag('ner').value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "data_path = '/nfs/gns/literature/machine-learning/Datasets/NER_Datasets/BC2GM-IOB/test.tsv'\n",
    "targets = ['GENE']\n",
    "                \n",
    "def load_IOBdataset(data_path,targets):\n",
    "    \"\"\"\n",
    "    load the IOB dataset, which is in csv format\n",
    "    :param data_path: path to the csv file of IOB dataset\n",
    "    :type data_path: str\n",
    "    :param targets: a list of interest types\n",
    "    :type targets: List[str]\n",
    "    :return: list of labels of every sentence in dataset\n",
    "    :rtype: List[List[str\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    X_sent = []\n",
    "    y_sent = []\n",
    "    with open(data_path, 'r') as f:\n",
    "        csv_reader = csv.reader(f, delimiter='\\t')\n",
    "        for line in csv_reader:\n",
    "            if line:\n",
    "                token, tag = line[0], line[-1]\n",
    "                X_sent.append(token)\n",
    "                if targets:\n",
    "                    if tag.split('-')[-1] in set(targets):\n",
    "                        y_sent.append(tag)\n",
    "                    else:\n",
    "                        y_sent.append('O')\n",
    "                else:\n",
    "                    y_sent.append(tag)\n",
    "            else:\n",
    "                # we reach the end of a sentence\n",
    "                if len(X_sent) > 0:\n",
    "                    X.append(' '.join(X_sent))\n",
    "                    y.append(y_sent)\n",
    "                X_sent = []\n",
    "                y_sent = []\n",
    "    return X, y\n",
    "\n",
    "BC2GM_test,y = load_IOBdataset(data_path,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = '/nfs/gns/literature/machine-learning/Santosh/PCSE_on_public_test_data/'\n",
    "result_file_name = 'PCSE_on_BC2GM_IOB.csv'\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize, WordPunctTokenizer\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "with open(result_path + result_file_name, 'a', newline='\\n') as f1:\n",
    "    public_writer = csv.writer(f1, delimiter='\\t', lineterminator='\\n')\n",
    "\n",
    "    for each_sentence in tqdm(BC2GM_test):\n",
    "        text_temp = each_sentence\n",
    "\n",
    "#         sentence = ' '.join(text_temp.split(' '))\n",
    "        sentence = Sentence(text_temp)\n",
    "        flair_model.predict(sentence)\n",
    "\n",
    "        for token in sentence:\n",
    "        # print what you need (text and NER value)\n",
    "            ner_value = token.get_tag('ner').value\n",
    "            if  ner_value == 'B-GP':\n",
    "                ner_value = 'B-GENE'\n",
    "                \n",
    "            if  ner_value == 'I-GP':\n",
    "                ner_value = 'I-GENE'   \n",
    "            \n",
    "            public_writer.writerow([token.text, ner_value])\n",
    "        public_writer.writerow('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/nfs/gns/literature/machine-learning/Datasets/NER_Datasets/NCBI-disease-IOB/test.tsv'\n",
    "targets = ['Disease']\n",
    "NCBI_test,y = load_IOBdataset(data_path,targets)\n",
    "\n",
    "result_path = '/nfs/gns/literature/machine-learning/Santosh/PCSE_on_public_test_data/'\n",
    "result_file_name = 'PCSE_on_NCBI_IOB.csv'\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize, WordPunctTokenizer\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "with open(result_path + result_file_name, 'a', newline='\\n') as f1:\n",
    "    public_writer = csv.writer(f1, delimiter='\\t', lineterminator='\\n')\n",
    "\n",
    "    for each_sentence in tqdm(NCBI_test):\n",
    "        text_temp = each_sentence\n",
    "\n",
    "#         sentence = ' '.join(text_temp.split(' '))\n",
    "        sentence = Sentence(text_temp)\n",
    "        flair_model.predict(sentence)\n",
    "\n",
    "        for token in sentence:\n",
    "        # print what you need (text and NER value)\n",
    "            ner_value = token.get_tag('ner').value\n",
    "            if  ner_value == 'B-Disease':\n",
    "                ner_value = 'B-DS'\n",
    "                \n",
    "            if  ner_value == 'I-Disease':\n",
    "                ner_value = 'I-DS'   \n",
    "            \n",
    "            public_writer.writerow([token.text, ner_value])\n",
    "        public_writer.writerow('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/nfs/gns/literature/machine-learning/Datasets/NER_Datasets/linnaeus-IOB/test.tsv'\n",
    "targets = ['Species']\n",
    "NCBI_test,y = load_IOBdataset(data_path,targets)\n",
    "\n",
    "result_path = '/nfs/gns/literature/machine-learning/Santosh/PCSE_on_public_test_data/'\n",
    "result_file_name = 'PCSE_on_linnaeus_IOB.csv'\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize, WordPunctTokenizer\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "with open(result_path + result_file_name, 'a', newline='\\n') as f1:\n",
    "    public_writer = csv.writer(f1, delimiter='\\t', lineterminator='\\n')\n",
    "\n",
    "    for each_sentence in tqdm(NCBI_test):\n",
    "        text_temp = each_sentence\n",
    "\n",
    "#         sentence = ' '.join(text_temp.split(' '))\n",
    "        sentence = Sentence(text_temp)\n",
    "        flair_model.predict(sentence)\n",
    "\n",
    "        for token in sentence:\n",
    "        # print what you need (text and NER value)\n",
    "            ner_value = token.get_tag('ner').value\n",
    "            if  ner_value == 'B-OG':\n",
    "                ner_value = 'B-Species'\n",
    "                \n",
    "            if  ner_value == 'I-OG':\n",
    "                ner_value = 'I-Species'   \n",
    "            \n",
    "            public_writer.writerow([token.text, ner_value])\n",
    "        public_writer.writerow('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
