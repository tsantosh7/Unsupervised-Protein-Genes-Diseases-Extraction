{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-25 09:21:08,479 loading file /nfs/gns/literature/Santosh_Tirunagari/GitHub/flair_models/ner/multi_bio_ner_model_gene/v01/best-model.pt\n",
      "2019-11-25 09:21:17,415 loading file /nfs/gns/literature/Santosh_Tirunagari/GitHub/flair_models/ner/multi_bio_ner_model_organisms/v01/best-model.pt\n"
     ]
    }
   ],
   "source": [
    "# from flair.models import TextClassifier\n",
    "# from flair.data import Sentence, Token\n",
    "# from flair.models import SequenceTagger\n",
    "\n",
    "flair_models = '/nfs/gns/literature/Santosh_Tirunagari/GitHub/flair_models/ner/'\n",
    "# load the model you trained\n",
    "gene_model = SequenceTagger.load(flair_models+'multi_bio_ner_model_gene/v01/'+'best-model.pt')\n",
    "disease_model = SequenceTagger.load(flair_models+'multi_bio_ner_model_disease/'+'best-model.pt')\n",
    "organisms_model = SequenceTagger.load(flair_models+'multi_bio_ner_model_organisms/v01/'+'best-model.pt')\n",
    "\n",
    "empc_model = SequenceTagger.load(flair_models+'manual_annotated_dataset/best-model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "data_path = '/nfs/gns/literature/machine-learning/Datasets/NER_Datasets/NCBI-disease-IOB/test.tsv'\n",
    "targets = ['Disease']\n",
    "                \n",
    "def load_IOBdataset(data_path,targets):\n",
    "    \"\"\"\n",
    "    load the IOB dataset, which is in csv format\n",
    "    :param data_path: path to the csv file of IOB dataset\n",
    "    :type data_path: str\n",
    "    :param targets: a list of interest types\n",
    "    :type targets: List[str]\n",
    "    :return: list of labels of every sentence in dataset\n",
    "    :rtype: List[List[str\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    X_sent = []\n",
    "    y_sent = []\n",
    "    with open(data_path, 'r') as f:\n",
    "        csv_reader = csv.reader(f, delimiter='\\t')\n",
    "        for line in csv_reader:\n",
    "            if line:\n",
    "                token, tag = line[0], line[-1]\n",
    "                X_sent.append(token)\n",
    "                if targets:\n",
    "                    if tag.split('-')[-1] in set(targets):\n",
    "                        y_sent.append(tag)\n",
    "                    else:\n",
    "                        y_sent.append('O')\n",
    "                else:\n",
    "                    y_sent.append(tag)\n",
    "            else:\n",
    "                # we reach the end of a sentence\n",
    "                if len(X_sent) > 0:\n",
    "                    X.append(' '.join(X_sent))\n",
    "                    y.append(y_sent)\n",
    "                X_sent = []\n",
    "                y_sent = []\n",
    "    return X, y\n",
    "\n",
    "NCBI_test,y = load_IOBdataset(data_path,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize, WordPunctTokenizer\n",
    "from typing import List\n",
    "def custom_tokenizer(text: str) -> List[Token]:\n",
    "    \"\"\"\n",
    "    Tokenizer based on space character only.\n",
    "    \"\"\"\n",
    "    tokens: List[Token] = []\n",
    "    \n",
    "    tokenizer = WordPunctTokenizer()\n",
    "\n",
    "    text = tokenizer.tokenize(text)\n",
    "\n",
    "    index = 0\n",
    "    for index, word in enumerate(text):\n",
    "            tokens.append(\n",
    "                Token(\n",
    "                    text=word, start_position=index, whitespace_after=False\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# custom_tokenizer(text_temp_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence: \"Although mutations of JMJD2B have been suggested to be responsible for neurodevelopmental disorders , the function of JMJD2B in the central nervous system ( CNS ) remains to be elucidated .\" - 31 Tokens]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from flair.data import segtok_tokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "text_temp = 'Although mutations of JMJD2B have been suggested to be responsible for neurodevelopmental disorders, the function of JMJD2B in the central nervous system (CNS) remains to be elucidated. '\n",
    "\n",
    "sentence_1 = Sentence(text_temp, use_tokenizer=custom_tokenizer)\n",
    "sentence_2 = Sentence(text_temp, use_tokenizer=custom_tokenizer)\n",
    "empc_model.predict(sentence_1)\n",
    "disease_model.predict(sentence_2)\n",
    "\n",
    "sentence_3 = Sentence(text_temp, use_tokenizer=custom_tokenizer)\n",
    "gene_model.predict(sentence_3)\n",
    "\n",
    "sentence_4 = Sentence(text_temp, use_tokenizer=custom_tokenizer)\n",
    "organisms_model.predict(sentence_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_json = sentence_1.to_dict(tag_type='ner')\n",
    "ncbi_json = sentence_2.to_dict(tag_type='ner')\n",
    "bc2gm_json = sentence_3.to_dict(tag_type='ner')\n",
    "linn_json = sentence_4.to_dict(tag_type='ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Although mutations of JMJD2B have been suggested to be responsible for neurodevelopmental disorders, the function of JMJD2B in the central nervous system (CNS) remains to be elucidated.',\n",
       " 'labels': [],\n",
       " 'entities': [{'text': 'JMJD2B',\n",
       "   'start_pos': 22,\n",
       "   'end_pos': 28,\n",
       "   'type': 'GP',\n",
       "   'confidence': 0.998449444770813},\n",
       "  {'text': 'neurodevelopmental disorders',\n",
       "   'start_pos': 71,\n",
       "   'end_pos': 99,\n",
       "   'type': 'DS',\n",
       "   'confidence': 0.8485407531261444},\n",
       "  {'text': 'JMJD2B',\n",
       "   'start_pos': 117,\n",
       "   'end_pos': 123,\n",
       "   'type': 'GP',\n",
       "   'confidence': 0.9984619617462158}]}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Provoked by public pressure and triggered by an increasing number of lethal lung diseases over the last few decades [1, 2], more and more studies in the field of inhalation toxicology now concentrate on the understanding of particle-lung interactions.',\n",
       " 'labels': [],\n",
       " 'entities': [{'text': 'lethal lung diseases',\n",
       "   'start_pos': 69,\n",
       "   'end_pos': 89,\n",
       "   'type': 'DS',\n",
       "   'confidence': 0.7132398088773092}]}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncbi_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_from_json(man,ncbi,bc2gm,linn):\n",
    "    all_entities = []\n",
    "    for ea_an in man['entities']:\n",
    "        all_entities.append([ea_an['start_pos'],ea_an['end_pos'],ea_an['text'],ea_an['type']])\n",
    "        \n",
    "    for ea_an in ncbi['entities']:\n",
    "        ea_an['type'] ='DS'\n",
    "        all_entities.append([ea_an['start_pos'],ea_an['end_pos'],ea_an['text'],ea_an['type']])\n",
    "\n",
    "    for ea_an in bc2gm['entities']:\n",
    "        ea_an['type'] ='GP'\n",
    "        all_entities.append([ea_an['start_pos'],ea_an['end_pos'],ea_an['text'],ea_an['type']])\n",
    "\n",
    "    for ea_an in linn['entities']:\n",
    "        ea_an['type'] ='OG'\n",
    "        all_entities.append([ea_an['start_pos'],ea_an['end_pos'],ea_an['text'],ea_an['type']])\n",
    "\n",
    "\n",
    "    \n",
    "    b_set = set(map(tuple,all_entities))  #need to convert the inner lists to tuples so they are hashable\n",
    "    unique_entities = map(list,b_set) #Now convert tuples back into lists (maybe unnecessary?)\n",
    "    return list(unique_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[69, 89, 'lethal lung diseases', 'DS']]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_entities_from_json(manual_json,ncbi_json,bc2gm_json,linn_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[13, 16, 'aru', 'GP'],\n",
       " [332, 343, 'A. thaliana', 'OG'],\n",
       " [120, 124, 'OST3', 'GP'],\n",
       " [142, 175, 'oligosaccharyltransferase complex', 'GP'],\n",
       " [120, 134, 'OST3/6 subunit', 'GP'],\n",
       " [142, 167, 'oligosaccharyltransferase', 'GP'],\n",
       " [104, 107, 'ARU', 'GP'],\n",
       " [13, 24, 'aru mutants', 'GP'],\n",
       " [332, 348, 'A. thaliana OST3', 'OG'],\n",
       " [83, 94, 'A. thaliana', 'OG']]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_entities_from_json(manual_json,ncbi_json,bc2gm_json,linn_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize, WordPunctTokenizer\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "data_path = '/nfs/gns/literature/machine-learning/Datasets/NER_Datasets/NCBI-disease-IOB/test.tsv'\n",
    "targets = ['Disease']\n",
    "NCBI_test,y = load_IOBdataset(data_path,targets)\n",
    "\n",
    "result_path = '/nfs/gns/literature/machine-learning/fused_models_on_test_data/'\n",
    "result_file_name = 'PCSE_fused_on_NCBI_IOB.csv'\n",
    "\n",
    "\n",
    "with open(result_path + result_file_name, 'a', newline='\\n') as f1:\n",
    "    public_writer = csv.writer(f1, delimiter='\\t', lineterminator='\\n')\n",
    "\n",
    "    for each_sentence in tqdm(NCBI_test):\n",
    "        text_temp = each_sentence\n",
    "\n",
    "#         sentence = ' '.join(text_temp.split(' '))\n",
    "        sentence_1 = Sentence(text_temp)\n",
    "        sentence_2 = Sentence(text_temp)\n",
    "        empc_model.predict(sentence_1)\n",
    "        disease_model.predict(sentence_2)\n",
    "\n",
    "        for token in sentence:\n",
    "        # print what you need (text and NER value)\n",
    "            ner_value = token.get_tag('ner').value\n",
    "            if  ner_value == 'B-DS':\n",
    "                ner_value = 'B-Disease'\n",
    "                \n",
    "            if  ner_value == 'I-DS':\n",
    "                ner_value = 'I-Disease'   \n",
    "            \n",
    "            public_writer.writerow([token.text, ner_value])\n",
    "        public_writer.writerow('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
