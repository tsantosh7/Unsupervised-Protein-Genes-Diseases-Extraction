{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import gzip\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "path = '/nfs/misc/literature/shyama/OTAR_Old_Pipeline/AbsJan2020/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = sorted(glob.glob(path+'Annot_med*.xml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10657"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/nfs/misc/literature/shyama/OTAR_Old_Pipeline/AbsJan2020/Annot_medline.0.xml'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getfileblocks(file_path):\n",
    "\n",
    "    subFileBlocks = []\n",
    "\n",
    "    with open(file_path, 'r') as fh:\n",
    "        for line in fh:\n",
    "            if line.startswith('<!DOCTYPE'):\n",
    "                subFileBlocks.append(line)\n",
    "            else:\n",
    "                subFileBlocks[-1] += line\n",
    "                \n",
    "    return subFileBlocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getfileblocks_abs(file_path):\n",
    "\n",
    "    subFileBlocks = []\n",
    "\n",
    "    with open(file_path, 'r') as fh:\n",
    "        first_line = fh.readline()\n",
    "        for line in fh:\n",
    "            if line.startswith('<PubmedArticle>'):\n",
    "                subFileBlocks.append(line)\n",
    "            else:\n",
    "                subFileBlocks[-1] += line\n",
    "                \n",
    "    return subFileBlocks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GP_soup(file_soup):\n",
    "    dict_sentences = defaultdict(list)\n",
    "    for each_ztag in file_soup.find_all('z:uniprot'):\n",
    "        try:\n",
    "            gene_sentence = each_ztag.findParents('plain')[0].text\n",
    "            dict_sentences[gene_sentence].append(each_ztag) \n",
    "        except:\n",
    "            pass\n",
    "    return dict_sentences\n",
    "\n",
    "def GP_soup_abs(file_soup):\n",
    "    dict_sentences = defaultdict(list)\n",
    "    for each_ztag in file_soup.find_all('uniprot'):\n",
    "        try:\n",
    "            gene_sentence = each_ztag.findParents('plain')[0].text\n",
    "            dict_sentences[gene_sentence].append(each_ztag) \n",
    "        except:\n",
    "            pass\n",
    "    return dict_sentences\n",
    "\n",
    "from fuzzywuzzy import fuzz \n",
    "def compare_with_ml_annotations(r, z_tags):\n",
    "    FP_removed_z_tags = []\n",
    "    for each_z_tag in z_tags:\n",
    "        for each_ml_annotation in r.json()['annotations']:\n",
    "            score = fuzz.token_set_ratio(each_ml_annotation[3],each_z_tag.text)\n",
    "            if  score ==100 and each_ml_annotation[2]=='GP':\n",
    "                FP_removed_z_tags.append(each_z_tag)\n",
    "    \n",
    "    return list(set(z_tags) - set(FP_removed_z_tags))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_unique_fp_tags(GP_dict_sentences):\n",
    "#     url = 'http://ai-capo-api-1:5200/spaCy_ner_predictor?text_sentence='\n",
    "    url = 'http://ai-capo-api-lb/spaCy_ner_predictor?text_sentence='\n",
    "    FP_dict_sentences =defaultdict(list)\n",
    "    set_FPs = set()\n",
    "    \n",
    "    for each_uniprot_sentence, z_tags in GP_dict_sentences.items():\n",
    "        r = requests.get(url+each_uniprot_sentence)\n",
    "        if r.status_code == 200 and r.json()['status']==200:\n",
    "            FPs = compare_with_ml_annotations(r, z_tags)\n",
    "            if FPs:\n",
    "                FP_dict_sentences[each_uniprot_sentence].append(FPs)\n",
    "        else:\n",
    "            print(each_uniprot_sentence)\n",
    "        \n",
    "    for FP_sent,FP_tag in FP_dict_sentences.items():\n",
    "        for each_ztag in FP_tag[0]:\n",
    "            set_FPs.add(each_ztag)\n",
    "\n",
    "    return list(set_FPs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_new = '/nfs/misc/literature/Santosh_Tirunagari/OTAR_dumps/FP_removed/'\n",
    "path_new = '/nfs/gns/literature/machine-learning/'\n",
    "# for each_file_content in subFileBlocks:\n",
    "for each_file_path in all_files:\n",
    "    ss = getfileblocks(each_file_path)\n",
    "    for each_article in tqdm(ss):\n",
    "        soup = BeautifulSoup(each_article, 'html.parser')\n",
    "        GP_dict_sentences = GP_soup(soup)\n",
    "        if GP_dict_sentences:\n",
    "            unique_fps = get_unique_fp_tags(GP_dict_sentences)\n",
    "\n",
    "            for each_fp in unique_fps:\n",
    "                if each_fp:\n",
    "                    for tag in soup.findAll('z:uniprot', attrs={'ids':each_fp['ids']}):\n",
    "                        tag.unwrap()\n",
    "\n",
    "            new_file_content = str(soup).replace('\\n<ebiroot','<ebiroot').replace('<sent','<SENT').replace('</sent>','</SENT>')\n",
    "\n",
    "            with open(path_new+'edited_'+each_file_path.split('/')[-1], 'at') as f:\n",
    "                f.write(new_file_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_new = '/nfs/misc/literature/Santosh_Tirunagari/OTAR_dumps/FP_removed/'\n",
    "path_new = '/nfs/gns/literature/machine-learning/'\n",
    "# for each_file_content in subFileBlocks:\n",
    "for each_file_path in all_files:\n",
    "    \n",
    "    with open(path_new+'edited_'+each_file_path.split('/')[-1], 'a') as fl:\n",
    "        fl.write('<MedlineCitationSet>\\n')\n",
    "    \n",
    "    ss = getfileblocks_abs(each_file_path)\n",
    "    for each_article in tqdm(ss):\n",
    "        soup = BeautifulSoup(each_article, 'xml')\n",
    "        GP_dict_sentences = GP_soup_abs(soup)\n",
    "        if GP_dict_sentences:\n",
    "            unique_fps = get_unique_fp_tags(GP_dict_sentences)\n",
    "\n",
    "            for each_fp in unique_fps:\n",
    "                if each_fp:\n",
    "                    for tag in soup.findAll('uniprot', attrs={'ids':each_fp['ids']}):\n",
    "                        tag.unwrap()\n",
    "\n",
    "            new_file_content = str(soup).replace('<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n','').replace('<uniprot','<z:uniprot').replace('</uniprot','</z:uniprot').replace('<efo','<z:efo').replace('</efo','</z:efo')\n",
    "\n",
    "            with open(path_new+'edited_'+each_file_path.split('/')[-1], 'a') as f:\n",
    "                f.write(new_file_content)\n",
    "\n",
    "    with open(path_new+'edited_'+each_file_path.split('/')[-1], 'a') as fe:\n",
    "        fe.write('\\n</MedlineCitationSet>\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# offset = 25\n",
    "\n",
    "# start_index = key_fp.find(value_fp[0][0].text)\n",
    "# end_index = start_index+len(value_fp[0][0].text)\n",
    "\n",
    "# key_fp[start_index:end_index]\n",
    "\n",
    "# exact = value_fp[0][0].text\n",
    "# prefix = key_fp[max(0,start_index-offset):start_index-1]\n",
    "# postfix = key_fp[end_index+1:end_index+offset]\n",
    "            \n",
    "# [prefix, exact, postfix]      \n",
    "\n",
    "# # make sure the index starts from 0, if the start_index-offset<0, set it 0\n",
    "# # the total offset incuding prefix and posfix is 50\n",
    "# \n",
    "\n",
    "# value_fp[0][0].attrs['ids'].split(',')[0]\n",
    "# normalisation = 'http://purl.uniprot.org/uniprot/'+value_fp[0][0].attrs['ids'].split(',')[0]\n",
    "# normalisation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scispcacy",
   "language": "python",
   "name": "scispcacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
