{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import gzip\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import random\n",
    "import sys\n",
    "import pathlib\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "# import multiprocessing\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# url = 'http://ai-capo-api-lb/spaCy_ner_predictor?text_sentence='  # Load Balancer\n",
    "\n",
    "# result_path = '/nfs/gns/literature/machine-learning/evaluation/FP_Analysis/SpaCy/model_call/'\n",
    "# pathlib.Path(result_path).mkdir(parents=True, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/gns/literature/Santosh_Tirunagari/miniconda3/condabin/conda\r\n"
     ]
    }
   ],
   "source": [
    "!which conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/nfs/gns/literature/machine-learning/Santosh/Gitlab/biobertepmc/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "from biobert.model.bert_crf_model import BertCRF\n",
    "from biobert.data_loader.epmc_loader import NERDatasetBatch\n",
    "from biobert.utils.utils import my_collate\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from spacy import util\n",
    "# best_model_path = '/nfs/gns/literature/Santosh_Tirunagari/GitHub/spacy_models/pretrain_exp/best/'\n",
    "\n",
    "# print(\"Loading from\", best_model_path)\n",
    "# nlp2 = util.load_model_from_path(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_file_path = '/nfs/misc/literature/rdf_annotation_data/daily_pipeline_api/15_08_1947/fulltext/job_14/annotation/patch-total-417.xml.gz'\n",
    "data_file_path = '/nfs/misc/literature/rdf_annotation_data/daily_pipeline_api/15_08_1947/abstract/job_21/annotation/patch-total-415.abstract.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getfileblocks(file_path):\n",
    "    subFileBlocks = []\n",
    "\n",
    "    with gzip.open(file_path, 'rt') as fh:\n",
    "        for line in fh:\n",
    "            if line.startswith('<!DOCTYPE \"JATS-archivearticle1.dtd\">'):  # <!DOCTYPE article\n",
    "                subFileBlocks.append(line)\n",
    "            else:\n",
    "                subFileBlocks[-1] += line\n",
    "\n",
    "    return subFileBlocks\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = getfileblocks(data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(files_list[101], 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'http://ai-capo-api-3:5001/predict/sent' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "class MLModel:\n",
    "    def __init__(self):\n",
    "        self.bertCrf_model = load_model()\n",
    "\n",
    "        # bertCrf_model.load_state_dict(torch.load('/homes/yangx/home/gitrepo/biobertepmc/model/bert_crf_model.states', map_location=device))\n",
    "        self.bertCrf_model.load_state_dict(torch.load(MODEL_PATH + 'bert_crf_model.states', map_location=device))\n",
    "        self.bertCrf_model.bert_model.bert_model.to(device)\n",
    "\n",
    "    def post(self, sentences):\n",
    "        BATCH_SIZE = 16\n",
    "        text = sentences\n",
    "        # print(text)\n",
    "        with torch.no_grad():\n",
    "            processor, tokens, spans = load_data_processor(text)\n",
    "            dataLoader = DataLoader(dataset=processor, batch_size=BATCH_SIZE, collate_fn=my_collate, num_workers=2)\n",
    "\n",
    "            idx2label = params['idx2label']\n",
    "            self.bertCrf_model.eval()\n",
    "            entities = []\n",
    "            for i_batch, sample_batched in enumerate(dataLoader):\n",
    "                inputs = sample_batched['input']\n",
    "\n",
    "                bert_inputs, bert_attention_mask, bert_token_mask, wordpiece_alignment, split_alignments, lengths, token_mask \\\n",
    "                    = processor.tokens_totensor(inputs)\n",
    "\n",
    "                _, preds = self.bertCrf_model.predict(input_ids=bert_inputs.to(device),\n",
    "                                                      bert_attention_mask=bert_attention_mask.to(device),\n",
    "                                                      bert_token_mask=bert_token_mask,\n",
    "                                                      alignment=wordpiece_alignment,\n",
    "                                                      splits=(split_alignments, lengths),\n",
    "                                                      token_mask=token_mask)\n",
    "                if idx2label:\n",
    "                    for i, (path, score) in enumerate(preds):\n",
    "                        labels = [idx2label[p] for p in path]\n",
    "                        offset_index = i_batch * BATCH_SIZE + i\n",
    "                        entities.append([[e.span[0], e.span[1], e.tag, e.text]\n",
    "                                         for e in extract_entity(labels, spans[offset_index], text[offset_index])])\n",
    "        return {'annotations': entities}\n",
    "\n",
    "\n",
    "def load_data_processor(inputs):\n",
    "    token_spans = []\n",
    "    tokens = []\n",
    "    for line in inputs:\n",
    "        token_spans.append(list(tokenizer.span_tokenize(line)))\n",
    "        tokens.append([line[start: end] for start, end in token_spans[-1]])\n",
    "\n",
    "    processor = NERDatasetBatch.from_params(params=params, inputs=tokens)\n",
    "    return processor, tokens, token_spans\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    allowed_transitions = None\n",
    "    model = BertCRF(num_tags=params['num_tags'],\n",
    "                    model_name=params['model_name'],\n",
    "                    stride=params['stride'],\n",
    "                    include_start_end_transitions=True,\n",
    "                    constraints=allowed_transitions)\n",
    "    return model\n",
    "\n",
    "\n",
    "def extract_entity(preds, spans, text, length=20):\n",
    "    \"\"\"\n",
    "    extract entity from label sequence\n",
    "    :param preds: a list of labels in a sentence\n",
    "    :type preds: List[str\n",
    "    :param spans:\n",
    "    :type spans:\n",
    "    :return: A list of entity object\n",
    "    :rtype: List[Entity]\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    tmp = []\n",
    "\n",
    "    for i, token in enumerate(preds):\n",
    "        if token == 'O':\n",
    "            pos, tag = 'O', 'O'\n",
    "            label = None\n",
    "        else:\n",
    "            pos, tag = token.split('-')\n",
    "            label = Entity_Label(index=i, pos=pos, tag=tag, span=spans[i])\n",
    "\n",
    "        if pos in {'B', 'O'} and tmp:\n",
    "            start_span = tmp[0].span[0]\n",
    "            end_span = tmp[-1].span[1]\n",
    "            entities.append(Entity(span=(start_span, end_span),\n",
    "                                   tag=tmp[0].tag,\n",
    "                                   text=text[start_span:end_span],\n",
    "                                   pre=text[max(0, start_span - length):start_span],\n",
    "                                   post=text[end_span: end_span + length]))\n",
    "            tmp[:] = []\n",
    "        if pos == 'B' or pos == 'I':\n",
    "            tmp.append(label)\n",
    "\n",
    "    if tmp:\n",
    "        start_span = tmp[0].span[0]\n",
    "        end_span = tmp[-1].span[-1]\n",
    "        entities.append(\n",
    "            Entity(span=(start_span, end_span),\n",
    "                   tag=tmp[0].tag,\n",
    "                   text=text[start_span:end_span],\n",
    "                   pre=text[max(0, start_span - length):start_span],\n",
    "                   post=text[end_span:end_span + length])\n",
    "        )\n",
    "    return entities\n",
    "\n",
    "\n",
    "# Create a function called \"chunks\" with two arguments, l and n:\n",
    "def chunks(l, n):\n",
    "    # For item i in a range that is a length of l,\n",
    "    for i in range(0, len(l), n):\n",
    "        # Create an index range for l of n items:\n",
    "        yield l[i:i + n]\n",
    "\n",
    "\n",
    "def clean_Nones(ner_tags_):\n",
    "    ner_tags = []\n",
    "    # had to do this as the position of entity tag and entity are exchanged in CD\n",
    "    for each_ner_tag in ner_tags_:\n",
    "        if 'CD' == each_ner_tag[2]:\n",
    "            ner_tags.append([each_ner_tag[0], each_ner_tag[1], each_ner_tag[3], each_ner_tag[2]])\n",
    "        else:\n",
    "            ner_tags.append(each_ner_tag)\n",
    "\n",
    "    ner_tags = sorted(ner_tags, key=lambda x: len(x[3]), reverse=True)\n",
    "    if len(ner_tags) == 1 and 'None' in ner_tags:\n",
    "        return ner_tags\n",
    "    elif len(ner_tags) > 1 and 'None' in ner_tags:\n",
    "        ner_tags.remove('None')\n",
    "        return ner_tags\n",
    "    else:\n",
    "        return ner_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = soup.find_all('sent')# all_sentences = soup.find_all('SENT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_sentences = []\n",
    "section_tags = defaultdict(set)\n",
    "gene_sentences = defaultdict(set)\n",
    "disease_setences = defaultdict(set)\n",
    "for each_sentence in all_sentences:\n",
    "    extracted_sentence = each_sentence.plain\n",
    "    if extracted_sentence:\n",
    "        try:\n",
    "#             section_tags[extracted_sentence.text].add(extracted_sentence.findParents('SecTag')[0]['type'])\n",
    "            section_tags[extracted_sentence.text].add(extracted_sentence.findParents('sectag')[0]['type'])\n",
    "        except IndexError:\n",
    "            section_tags[extracted_sentence.text].add('')\n",
    "        plain_sentences.append(extracted_sentence.text.strip())\n",
    "        #### Extract GP sentences and their tags######\n",
    "        extracted_sentence_GP_ztags = extracted_sentence.find_all('z:uniprot')\n",
    "        if extracted_sentence_GP_ztags is not None:\n",
    "            for each_GP_z_tag in extracted_sentence_GP_ztags:\n",
    "                gene_sentences[each_sentence.plain.text].add(each_GP_z_tag.text)\n",
    "        #### Extract Disease sentences and their tags######\n",
    "        extracted_sentence_DS_ztags = extracted_sentence.find_all('z:disease')\n",
    "        if extracted_sentence_DS_ztags is not None:\n",
    "            for each_DS_z_tag in extracted_sentence_DS_ztags:\n",
    "                disease_setences[each_sentence.plain.text].add(each_DS_z_tag.text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {'Differential contribution of the two waves of cardiac progenitors and their derivatives to aorta and pulmonary artery. ': {''},\n",
       "             'During mouse development, part of the cells derived from the second heart field (SHF) progenitors contributes to the elongation and enlargement of the outflow tract (OFT) that subsequently septates into the trunks of aorta (Ao) and pulmonary artery (PA). ': {'ABS'},\n",
       "             'Thus, the cardiac progenitor-originated cells are distributed to both Ao and PA. ': {'ABS'},\n",
       "             'Here, we investigated that how these cells are assigned to the two great arteries during OFT septation through lineage tracing technology. ': {'ABS'},\n",
       "             'By use of the inducible Mef2c-AHF-CreERT2; Rosa26-mTmG reporter system, two waves of SHF progenitors and their derivatives were identified, and they made differential contribution to the Ao and PA, respectively. ': {'ABS'},\n",
       "             'While the early wave of cells (at E7.5) was preferentially destined to the Ao, the second wave of cells (from E8.5 till E11.5) made its favorite path to the PA. ': {'ABS'},\n",
       "             'In addition, we unveiled PDK1 as a critical regulator of the second wave of cells as deletion of Pdk1 resulted in poorly developed PA leading to pulmonary stenosis. ': {'ABS'},\n",
       "             'Thus, this study provides insights into the understanding of the pre-determined cell fate of the cardiac progenitor-derived cells with preferential contribution to the Ao and PA, as well as of the pathogenesis of pulmonary stenosis. ': {'ABS'}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "def get_all_tags(all_sentences):\n",
    "        ML_annotations = ml_model.post(all_sentences)\n",
    "        return ML_annotations['annotations']#list(filter(None, ML_annotations['annotations']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '/nfs/gns/literature/machine-learning/Santosh/Gitlab/biobertepmc/reproduce_GP_DS_OG_CD/1604049631/'\n",
    "\n",
    "# path to the file that has model parameters\n",
    "params_path = MODEL_PATH + \"params.pickle\"\n",
    "with open(params_path, 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "params['max_ner_token_len'] = -1\n",
    "params['max_bert_token_len'] = -1\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ml_model = MLModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Entity = namedtuple('Entity', ['span', 'tag', 'text', 'pre', 'post'])\n",
    "Entity_Label = namedtuple('Label', ['index', 'pos', 'tag', 'span'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = get_all_tags(plain_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [[7, 12, 'OG', 'mouse']],\n",
       " [],\n",
       " [],\n",
       " [[24, 29, 'GP', 'Mef2c'],\n",
       "  [30, 33, 'GP', 'AHF'],\n",
       "  [34, 41, 'GP', 'CreERT2'],\n",
       "  [85, 88, 'GP', 'SHF']],\n",
       " [],\n",
       " [[25, 29, 'GP', 'PDK1'], [97, 101, 'GP', 'Pdk1']],\n",
       " [[213, 231, 'DS', 'pulmonary stenosis']]]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_relationv01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = en_relationv01.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = relations(plain_sentences[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roundoff(dict_y):\n",
    "    for k, v in dict_y.items():\n",
    "        v = round(v,2) \n",
    "        dict_y[k] = v \n",
    "    return dict_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Yes': 0.97,\n",
       " 'No': 0.0,\n",
       " 'Positive': 0.16,\n",
       " 'Negative': 0.07,\n",
       " 'Neutral': 0.51,\n",
       " 'Altered Expression': 0.03,\n",
       " 'Genetic Variation': 0.0,\n",
       " 'Any': 0.34,\n",
       " 'Regulatory modification': 0.02}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roundoff(doc.cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 25575.02it/s]\n"
     ]
    }
   ],
   "source": [
    "ml_tagged_sentences = defaultdict(list)\n",
    "count = 0\n",
    "for each_sentence in tqdm(plain_sentences):\n",
    "#     rs = requests.post(url, data={'text': each_sentence})\n",
    "#     ml_annotations = rs.json()['annotations']\n",
    "    if annotations[count]:\n",
    "        ml_tagged_sentences[each_sentence].append(annotations[count])\n",
    "    count = count+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'During mouse development, part of the cells derived from the second heart field (SHF) progenitors contributes to the elongation and enlargement of the outflow tract (OFT) that subsequently septates into the trunks of aorta (Ao) and pulmonary artery (PA).': [[[7,\n",
       "                12,\n",
       "                'OG',\n",
       "                'mouse']]],\n",
       "             'By use of the inducible Mef2c-AHF-CreERT2; Rosa26-mTmG reporter system, two waves of SHF progenitors and their derivatives were identified, and they made differential contribution to the Ao and PA, respectively.': [[[24,\n",
       "                29,\n",
       "                'GP',\n",
       "                'Mef2c'],\n",
       "               [30, 33, 'GP', 'AHF'],\n",
       "               [34, 41, 'GP', 'CreERT2'],\n",
       "               [85, 88, 'GP', 'SHF']]],\n",
       "             'In addition, we unveiled PDK1 as a critical regulator of the second wave of cells as deletion of Pdk1 resulted in poorly developed PA leading to pulmonary stenosis.': [[[25,\n",
       "                29,\n",
       "                'GP',\n",
       "                'PDK1'],\n",
       "               [97, 101, 'GP', 'Pdk1']]],\n",
       "             'Thus, this study provides insights into the understanding of the pre-determined cell fate of the cardiac progenitor-derived cells with preferential contribution to the Ao and PA, as well as of the pathogenesis of pulmonary stenosis.': [[[213,\n",
       "                231,\n",
       "                'DS',\n",
       "                'pulmonary stenosis']]]})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML not recognising covid terms in some cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_list = ['covid-19', 'covid19', 'sarscov2', 'sars-cov-2', '2019-ncov']\n",
    "def compare_ml_annotations_with_dictionary_tagged(ml_tags_, z_tags_):\n",
    "    agreed_z_tags = set()\n",
    "#     print(z_tags_, ml_tags_)\n",
    "    for each_z_tag in z_tags_:\n",
    "        for each_ml_annotation in ml_tags_:\n",
    "            if each_z_tag.lower() in missing_list:\n",
    "                agreed_z_tags.add(each_z_tag)\n",
    "            else:\n",
    "                score = fuzz.partial_ratio(each_ml_annotation, each_z_tag) #token_set_ratio\n",
    "                if score == 100:\n",
    "                    agreed_z_tags.add(each_z_tag)\n",
    "    return list(z_tags_- agreed_z_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {'By use of the inducible Mef2c-AHF-CreERT2; Rosa26-mTmG reporter system, two waves of SHF progenitors and their derivatives were identified, and they made differential contribution to the Ao and PA, respectively. ': {'AHF',\n",
       "              'Mef2c'},\n",
       "             'In addition, we unveiled PDK1 as a critical regulator of the second wave of cells as deletion of Pdk1 resulted in poorly developed PA leading to pulmonary stenosis. ': {'PDK1',\n",
       "              'Pdk1'}})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-1fefa57629ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp_z_tags\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgene_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mml_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mml_tagged_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meach_sentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0meach_ml_tag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mml_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0meach_ml_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m'GP'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mall_ml_gp_tags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meach_ml_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "all_ml_gp_tags = set()\n",
    "all_ml_ds_tags = set()\n",
    "all_gp_ztags = set()\n",
    "all_ds_ztags = set()\n",
    "\n",
    "for each_sentence, gp_z_tags in gene_sentences.items():\n",
    "    ml_tags = ml_tagged_sentences[each_sentence]\n",
    "    for each_ml_tag in ml_tags[0]:\n",
    "        if each_ml_tag[2] =='GP':\n",
    "            all_ml_gp_tags.add(each_ml_tag[3])\n",
    "    for each_gp_ztag in gp_z_tags:\n",
    "        all_gp_ztags.add(each_gp_ztag)\n",
    "    \n",
    "for each_sentence, ds_z_tags in disease_setences.items():\n",
    "    ml_tags = ml_tagged_sentences[each_sentence]\n",
    "    for each_ml_tag in ml_tags[0]:\n",
    "        if each_ml_tag[2] =='DS':\n",
    "            all_ml_ds_tags.add(each_ml_tag[3])\n",
    "    for each_ds_ztag in ds_z_tags:\n",
    "        all_ds_ztags.add(each_ds_ztag)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_tagged_sentences[each_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GP_false_postives = compare_ml_annotations_with_dictionary_tagged(all_ml_gp_tags, all_gp_ztags)\n",
    "GP_false_postives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_false_postives = compare_ml_annotations_with_dictionary_tagged(all_ml_ds_tags, all_ds_ztags)\n",
    "DS_false_postives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## End of Europe PMC with the ML FP filtering!! Save the XMLS for EuropePMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_tagged_sentences['Projected healthcare resource needs for an effective response to COVID-19 in low-income and middle-income countries are significant, but an early response to limit the spread of the SARS-COV-2 virus will reduce resources needed and costs.'] = [[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_missing_tags(each_sentence):\n",
    "    new_entities = []\n",
    "    for missing_string in missing_list:\n",
    "        for i in re.finditer(missing_string, each_sentence.lower()):\n",
    "            indexlocation= i.span()\n",
    "    #         print(indexlocation)\n",
    "            startindex= i.start()\n",
    "            endindex= i.end()\n",
    "            entity = each_sentence[indexlocation[0]:indexlocation[1]]\n",
    "            new_entities.append([startindex,endindex, 'DS', entity])\n",
    "    return new_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'During mouse development, part of the cells derived from the second heart field (SHF) progenitors contributes to the elongation and enlargement of the outflow tract (OFT) that subsequently septates into the trunks of aorta (Ao) and pulmonary artery (PA).': [[[7,\n",
       "                12,\n",
       "                'OG',\n",
       "                'mouse']]],\n",
       "             'By use of the inducible Mef2c-AHF-CreERT2; Rosa26-mTmG reporter system, two waves of SHF progenitors and their derivatives were identified, and they made differential contribution to the Ao and PA, respectively.': [[[24,\n",
       "                29,\n",
       "                'GP',\n",
       "                'Mef2c'],\n",
       "               [30, 33, 'GP', 'AHF'],\n",
       "               [34, 41, 'GP', 'CreERT2'],\n",
       "               [85, 88, 'GP', 'SHF']]],\n",
       "             'In addition, we unveiled PDK1 as a critical regulator of the second wave of cells as deletion of Pdk1 resulted in poorly developed PA leading to pulmonary stenosis.': [[[25,\n",
       "                29,\n",
       "                'GP',\n",
       "                'PDK1'],\n",
       "               [97, 101, 'GP', 'Pdk1']]],\n",
       "             'Thus, this study provides insights into the understanding of the pre-determined cell fate of the cardiac progenitor-derived cells with preferential contribution to the Ao and PA, as well as of the pathogenesis of pulmonary stenosis.': [[[213,\n",
       "                231,\n",
       "                'DS',\n",
       "                'pulmonary stenosis']]],\n",
       "             'By use of the inducible Mef2c-AHF-CreERT2; Rosa26-mTmG reporter system, two waves of SHF progenitors and their derivatives were identified, and they made differential contribution to the Ao and PA, respectively. ': []})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-b2e71f1011af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mall_tags\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mnew_missing_stuff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_new_missing_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mml_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mall_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_missing_stuff\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mml_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# get gp, ds, cd, and og sets\n",
    "gp_set = set()\n",
    "ds_set = set()\n",
    "cd_set = set()\n",
    "og_set = set()\n",
    "\n",
    "final_sentences = defaultdict(list)\n",
    "\n",
    "for sentence, ml_tags in ml_tagged_sentences.items():\n",
    "    all_tags =[]\n",
    "    new_missing_stuff = get_new_missing_tags(sentence)\n",
    "    if ml_tags[0]:\n",
    "        all_tags = new_missing_stuff+ml_tags[0]\n",
    "    else:\n",
    "        all_tags = new_missing_stuff\n",
    "    if all_tags: # only if the tags are present, so discard the sentence without any tags\n",
    "        final_sentences[sentence] = all_tags\n",
    "        for each_ml_tag in all_tags:\n",
    "            if each_ml_tag[2] =='GP':\n",
    "                gp_set.add(each_ml_tag[3])\n",
    "            elif each_ml_tag[2] =='DS':\n",
    "                ds_set.add(each_ml_tag[3])\n",
    "            if each_ml_tag[2] =='CD':\n",
    "                cd_set.add(each_ml_tag[3])\n",
    "            if each_ml_tag[2] =='OG':\n",
    "                og_set.add(each_ml_tag[3])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_sentences_offset_per_tag(tagset, sentences_tags, tag_type):\n",
    "    dicttag = defaultdict(list)\n",
    "    for each_tag in tagset:\n",
    "        for each_sentence, ml_tags in sentences_tags.items():\n",
    "            for each_ml_tag in ml_tags:\n",
    "                if each_tag in each_ml_tag[3] and each_ml_tag[2] ==tag_type:\n",
    "                    dicttag[each_tag].append([each_sentence, each_ml_tag[0], each_ml_tag[1]])\n",
    "    return dicttag\n",
    "\n",
    "\n",
    "def get_mapped_list_from_annotations(annotation_list):\n",
    "    mapped_list = list(itertools.combinations(annotation_list, 2))\n",
    "\n",
    "    unique_maplist = []\n",
    "    for each_list in mapped_list:\n",
    "        if each_list[0][2] !=each_list[1][2] and each_list[1][2]!='OG' and each_list[0][2]!='OG':\n",
    "            unique_maplist.append((each_list[0], each_list[1]))\n",
    "\n",
    "    return unique_maplist  \n",
    "\n",
    "def get_sentences_offset_per_cooccurance(sentences_tags):\n",
    "    \n",
    "    dict_gp_ds = defaultdict(list)\n",
    "    dict_gp_cd = defaultdict(list)\n",
    "    dict_cd_ds = defaultdict(list)\n",
    "\n",
    "    for sentence, tags in final_sentences.items():\n",
    "        if len(tags)>1: # only if more than 1 tag is available\n",
    "            check_tags =np.array(tags)\n",
    "            if 'GP' in  check_tags and 'DS' in check_tags:\n",
    "                dict_gp_ds[sentence] = get_mapped_list_from_annotations(tags)\n",
    "            if 'GP' in  check_tags and 'CD' in check_tags:\n",
    "                dict_gp_cd[sentence] = get_mapped_list_from_annotations(tags)\n",
    "            if 'CD' in  check_tags and 'DS' in check_tags:\n",
    "                dict_cd_ds[sentence]= get_mapped_list_from_annotations(tags)         \n",
    "                \n",
    "    return dict_gp_ds, dict_gp_cd, dict_cd_ds\n",
    "\n",
    "\n",
    "\n",
    "def get_article_offset_per_tag(soup, tagset):\n",
    "    dicttag = defaultdict(list)\n",
    "    \n",
    "    \n",
    "    for each_tag in tagset:\n",
    "        for i in re.finditer(each_tag, each_sentence.lower()):\n",
    "            indexlocation= i.span()\n",
    "    #         print(indexlocation)\n",
    "            startindex= i.start()\n",
    "            endindex= i.end()\n",
    "            entity = each_sentence[indexlocation[0]:indexlocation[1]]\n",
    "        dicttag[each_tag].append([each_sentence, each_ml_tag[0], each_ml_tag[1]])\n",
    "    return dicttag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# GP\": [\n",
    "# {\n",
    "#   label: NR2B,\n",
    "#   abstract_matches: [{start: XXXX, end: XXXX}, {start: XXXX, end: XXXX}],\n",
    "#   full_text_matches: [{start: XXXX, end: XXXX}]\n",
    "# },\n",
    "# {\n",
    "#  label: \"NR2B\",\n",
    "#  abstract_matches:[{start:XXX, end:XXXXX}],\n",
    "# },\n",
    "\n",
    "## hard to get above notations are we process at sentences level, so the offset changes per sentence.\n",
    "\n",
    "dict_gp = get_sentences_offset_per_tag(gp_set, final_sentences, tag_type = 'GP')\n",
    "dict_ds = get_sentences_offset_per_tag(ds_set, final_sentences, tag_type = 'DS')\n",
    "dict_cd = get_sentences_offset_per_tag(cd_set, final_sentences, tag_type = 'CD')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_ = ' '.join(plain_sentences[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "#   \"lit_id\": \"http://europepmc.org/abstract/MED/11250803\",\n",
    "#   \"mined_sentences\": [\n",
    "# \t{\n",
    "#   \t\"section\": \"abstract\",\n",
    "#   \t\"t_start\": 222,\n",
    "#   \t\"t_end\": 242,\n",
    "#   \t\"d_start\": 23,\n",
    "#   \t\"d_end\": 41,\n",
    "#   \t\"text\": \"Both further stimulate cardiac hypertrophy and, importantly, activate counterregulatory mechanisms including overexpression of atrial natriuretic peptide and b-type natriuretic peptide, and production of cytokines such as tumor necrosis factor-alpha.\"\n",
    "# \t}\n",
    "#   ]\n",
    "\n",
    "# }\n",
    "\n",
    "# old style\n",
    "\n",
    "\n",
    "dict_gp_ds, dict_gp_cd, dict_cd_ds = get_sentences_offset_per_cooccurance(final_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_gp_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cooccurance_evidence(dict_tags, tag_type_1, tag_type_2):\n",
    "    mined_sentences = []\n",
    "    for each_sent_map, mappedtags in dict_tags.items():\n",
    "        for eachtag in mappedtags:\n",
    "            mini_dict = {}\n",
    "            eachtag = list(eachtag)\n",
    "            mini_dict['section'] = section_tags[each_sent_map]\n",
    "\n",
    "            if tag_type_1 =='GP' or tag_type_2 =='GP':          \n",
    "                if tag_type_1 in eachtag[0]:\n",
    "                    mini_dict['t_start'] = eachtag[0][0]\n",
    "                    mini_dict['t_end']= eachtag[0][1]\n",
    "                    mini_dict['t']= eachtag[0][3]\n",
    "                else:\n",
    "                    mini_dict['t_start'] = eachtag[1][0]\n",
    "                    mini_dict['t_end']= eachtag[1][1]\n",
    "                    mini_dict['t']= eachtag[0][3]           \n",
    "\n",
    "            if tag_type_1 =='DS' or tag_type_2 =='DS':  \n",
    "                if tag_type_2 in eachtag[0]:\n",
    "                    mini_dict['d_start'] = eachtag[0][0]\n",
    "                    mini_dict['d_end']= eachtag[0][1]\n",
    "                    mini_dict['d']= eachtag[1][3]\n",
    "                else:\n",
    "                    mini_dict['d_start'] = eachtag[1][0]\n",
    "                    mini_dict['d_end']= eachtag[1][1]\n",
    "                    mini_dict['d']= eachtag[1][3]\n",
    "                            \n",
    "            if tag_type_1 =='CD' or tag_type_2 =='CD':  \n",
    "                if tag_type_1 in eachtag[0]:\n",
    "                    mini_dict['c_start'] = eachtag[0][0]\n",
    "                    mini_dict['c_end']= eachtag[0][1]\n",
    "                    mini_dict['c']= eachtag[1][3]\n",
    "                else:\n",
    "                    mini_dict['c_start'] = eachtag[1][0]\n",
    "                    mini_dict['c_end']= eachtag[1][1]\n",
    "                    mini_dict['c']= eachtag[1][3]\n",
    "\n",
    "            mini_dict['text'] = each_sent_map\n",
    "            mined_sentences.append(mini_dict)\n",
    "    return mined_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_generated = {}\n",
    "for each_sentence, ml_tags in final_sentences.items():\n",
    "    try:\n",
    "        json_generated['pmid'] = 'PMC'+soup.find(attrs={\"pub-id-type\" : \"pmcid\"}).text #\"article-id\"\n",
    "    except:\n",
    "        json_generated['pmid'] = 'PMC'\n",
    "\n",
    "    json_generated['GP_set'] = list(gp_set)\n",
    "    json_generated['DS_set'] = list(ds_set)\n",
    "    json_generated['CD_set'] = list(cd_set)\n",
    "    json_generated['OG_set'] = list(og_set)\n",
    "\n",
    "# json_generated\n",
    "json_generated['GP_DS_mined_sentences'] = get_cooccurance_evidence(dict_gp_ds, tag_type_1='GP', tag_type_2='DS')\n",
    "json_generated['GP_CD_mined_sentences'] = get_cooccurance_evidence(dict_gp_cd, tag_type_1='GP', tag_type_2='CD')\n",
    "json_generated['CD_DS_mined_sentences'] = get_cooccurance_evidence(dict_cd_ds, tag_type_1='CD', tag_type_2='DS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.ebi.ac.uk/europepmc/webservices/rest/search?query=EXT_ID%3A2109978%20AND%20SRC%3AMED&resultType=core&cursorMark=*&pageSize=25&format=json\n",
    "\n",
    "import requests\n",
    "query = '2109978'\n",
    "url = 'https://www.ebi.ac.uk/europepmc/webservices/rest/search?query=EXT_ID:'+query+' AND SRC:MED&resultType=core&cursorMark=*&pageSize=25&format=json'\n",
    "response = requests.get(url)\n",
    "rjson = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rjson['resultList']['result'][0]['abstractText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_==yy_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FP Analysis on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pmc_meta(query):\n",
    "        \n",
    "        url = 'https://www.ebi.ac.uk/europepmc/webservices/rest/search?'\n",
    "        params = {'query':query, 'resultType':'core','cursorMark':'*','pageSize':'25','format':'json'}\n",
    "        response = requests.get(url,params)\n",
    "        rjson = response.json()\n",
    "\n",
    "        for rslt in rjson['resultList']['result']:\n",
    "            pmeta = {}\n",
    "            pmeta['pmid'] = ''\n",
    "            pmeta['pmcid'] = ''\n",
    "            pmeta['mesh'] = []\n",
    "            pmeta['pmc_title'] = ''\n",
    "            pmeta['pmc_abstract']= ''\n",
    "            pmeta['text_urls'] = []\n",
    "            pmeta['oa'] =  ''\n",
    "            pmeta['authMan'] = ''\n",
    "            if 'pmid' in rslt.keys():\n",
    "                pmeta['pmid'] = rslt['pmid']\n",
    "                pmids.append(rslt['pmid'])                \n",
    "            if 'meshHeadingList' in rslt.keys():\n",
    "                for m in rslt['meshHeadingList']['meshHeading']:\n",
    "                    if 'meshQualifierList' in m.keys():\n",
    "                        for q in m['meshQualifierList']['meshQualifier']:\n",
    "                            pmeta['mesh'].append(m['descriptorName'])\n",
    "                            pmeta['mesh'].append(q['qualifierName'])\n",
    "                    else:\n",
    "                        pmeta['mesh'].append(m['descriptorName'])\n",
    "            if 'title' in rslt.keys(): pmeta['pmc_title'] = rslt['title']\n",
    "            if 'abstractText' in rslt.keys(): pmeta['pmc_abstract'] = rslt['abstractText']\n",
    "            if 'fullTextUrlList' in rslt.keys():            \n",
    "                for u in rslt['fullTextUrlList']['fullTextUrl']:\n",
    "                    pmeta['text_urls'].append(u['url'])\n",
    "            if 'isOpenAccess' in rslt.keys():pmeta['oa'] =  rslt['isOpenAccess']\n",
    "            if 'authMan' in rslt.keys():pmeta['authMan'] = rslt['authMan']\n",
    "            if 'pmcid' in rslt.keys() and rslt['pmcid']!='':\n",
    "                pmeta['pmcid'] = rslt['pmcid']\n",
    "                pmcids.append(rslt['pmcid'])\n",
    "                if rslt['isOpenAccess'] == 'Y' or rslt['authMan'] == 'Y':\n",
    "                #if rslt['isOpenAccess'] == 'Y':    \n",
    "                    oapids.append(rslt['pmcid'])                        \n",
    "            pmetas.append(pmeta)\n",
    "    except:\n",
    "        print('ERROR IN PMC ID:'+query)\n",
    "        pass\n",
    "    return [pmids,pmcids,oapids,pmetas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path_EPMC_text = '/mnt/droplet/nfs/gns/literature/machine-learning/evaluation/300articles/europePMC-NER/annotations_API/full_sentences/test_annotations/Europe_PMC_annotation.csv'\n",
    "colNames = ['pmc_id', 'section', 'sentence','ner'] \n",
    "    \n",
    "test_df = pd.read_csv(path_EPMC_text,sep ='\\t', names=colNames) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "def deleting_epmc_GPS(list_1,del_name):\n",
    "   \n",
    "    for sub_list in list_1:\n",
    "        if del_name in sub_list:\n",
    "            list_1.remove(sub_list)\n",
    "    return list_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def remove_FP(epmc_list, ml_json):\n",
    "    all_ml_gps = []\n",
    "    if ml_json['annotations']:\n",
    "        for each_ml_annotation in ml_json['annotations']:\n",
    "            if each_ml_annotation[2] == 'GP':\n",
    "                all_ml_gps.append(each_ml_annotation[3])\n",
    "    \n",
    "    non_FP_removed =[]\n",
    "              \n",
    "\n",
    "    for each_ner in epmc_list:\n",
    "        if each_ner[2] == 'Gene_Proteins':\n",
    "            for each_ml_gp in all_ml_gps:         \n",
    "                score = fuzz.token_set_ratio(each_ml_gp, each_ner[1])\n",
    "                if score == 100:\n",
    "                    non_FP_removed.append(each_ner)\n",
    "\n",
    "    non_gp_tags =  deleting_epmc_GPS(epmc_list,'Gene_Proteins')  \n",
    "\n",
    "    fp_removed_tags = non_gp_tags+non_FP_removed\n",
    "\n",
    "    return fp_removed_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spacy_annotations(text_sentence):\n",
    "    data_dict ={}\n",
    "    doc = nlp2(text_sentence)\n",
    "    terms_entities = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        terms_entities.append(\n",
    "            [ent.start_char, ent.end_char, ent.label_, ent.text])\n",
    "    \n",
    "    data_dict['annotations'] = terms_entities\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "with open(result_path + 'spacy_fp_removal_80.tsv', 'w', newline='\\n') as f1:\n",
    "    public_writer = csv.writer(f1, delimiter='\\t', lineterminator='\\n')\n",
    "    \n",
    "    for index,row in tqdm(test_df.iterrows(),total = len(test_df)):\n",
    "        try:\n",
    "            ml_annotations = get_spacy_annotations(row['sentence'])\n",
    "            fp_removed = remove_FP(literal_eval(row['ner']), ml_annotations)\n",
    "        except ValueError:\n",
    "            fp_removed =''\n",
    "            \n",
    "        public_writer.writerow([row['pmc_id'], row['section'],row['sentence'], fp_removed])   \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to IOB format\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "\n",
    "sys.path.append('/mnt/droplet/nfs/gns/literature/Santosh_Tirunagari/test Gitlab/epmc-ml-misc-library/')\n",
    "\n",
    "import capo_tools_lib\n",
    "import evaluation_epmc_lib\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iob_result_path = result_path+'iob/'\n",
    "pathlib.Path(iob_result_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "file_path = result_path + 'spacy_fp_removal_80.tsv'\n",
    "capo_tools_lib.annotations_api_tagged_sentences_to_IOB(file_path,\n",
    "                                                       iob_result_path,'spacy_fp_removal_iob.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metrics.ner as ner_metrics\n",
    "\n",
    "\n",
    "# precision\t0.7\t0.7\t0.72\t0.73\n",
    "# recall\t0.53\t0.53\t0.54\t0.55\n",
    "# f1 score\t0.6\t0.6\t0.62\t0.6\n",
    "\n",
    "#\n",
    "# print(ner_metrics.semeval_scores_report(gold=epmc_labels, response=ml_labels, digits=2))\n",
    "\n",
    "root_path = '/mnt/droplet/nfs/gns/literature/machine-learning/'\n",
    "epmc_path = root_path+'Datasets/NER_Datasets/EBI_standard-IOB/test.csv'\n",
    "all_tags = ['GP', 'DS', 'OG']\n",
    "\n",
    "print('################ Annotation Pipeline Results ########################')\n",
    "CAPO_path = iob_result_path+'spacy_fp_removal_iob.tsv'\n",
    "for each_tag in all_tags:\n",
    "    print('############ '+each_tag+' ####################')\n",
    "    print('\\n')\n",
    "    print(ner_metrics.semeval_report(gold_path=epmc_path, response_path=CAPO_path, targets=[each_tag]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
