{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import gzip\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import random\n",
    "import sys\n",
    "import pathlib\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "# import multiprocessing\n",
    "from fuzzywuzzy import fuzz\n",
    "from statistics import mean\n",
    "import numpy as np\n",
    "import itertools\n",
    "import re\n",
    "import io\n",
    "# set the system path\n",
    "sys.path.insert(1, '/nfs/gns/literature/machine-learning/Santosh/Gitlab/biobertepmc/')\n",
    "\n",
    "# BioBERT NER models\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "from biobert.model.bert_crf_model import BertCRF\n",
    "from biobert.data_loader.epmc_loader import NERDatasetBatch\n",
    "from biobert.utils.utils import my_collate\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "# Relations and associations model\n",
    "import en_ner_europepmc_md\n",
    "import en_relationv01\n",
    "\n",
    "import unicodedata\n",
    "import datetime\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Entity = namedtuple('Entity', ['span', 'tag', 'text', 'pre', 'post'])\n",
    "Entity_Label = namedtuple('Label', ['index', 'pos', 'tag', 'span'])\n",
    "missing_list = ['covid-19', 'coronavirus disease 2019', '2019-ncov', 'covid 19']    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all functions here\n",
    "\n",
    "batch_size = 8\n",
    "class MLModel:\n",
    "    def __init__(self):\n",
    "        self.bertCrf_model = load_model()\n",
    "\n",
    "        # bertCrf_model.load_state_dict(torch.load('/homes/yangx/home/gitrepo/biobertepmc/model/bert_crf_model.states', map_location=device))\n",
    "        self.bertCrf_model.load_state_dict(torch.load(MODEL_PATH + 'bert_crf_model.states', map_location=device))\n",
    "        self.bertCrf_model.bert_model.bert_model.to(device)\n",
    "\n",
    "    def post(self, sentences):\n",
    "        BATCH_SIZE = 16\n",
    "        text = sentences\n",
    "        # print(text)\n",
    "        with torch.no_grad():\n",
    "            processor, tokens, spans = load_data_processor(text)\n",
    "            dataLoader = DataLoader(dataset=processor, batch_size=BATCH_SIZE, collate_fn=my_collate, num_workers=2)\n",
    "\n",
    "            idx2label = params['idx2label']\n",
    "            self.bertCrf_model.eval()\n",
    "            entities = []\n",
    "            for i_batch, sample_batched in enumerate(dataLoader):\n",
    "                inputs = sample_batched['input']\n",
    "\n",
    "                bert_inputs, bert_attention_mask, bert_token_mask, wordpiece_alignment, split_alignments, lengths, token_mask \\\n",
    "                    = processor.tokens_totensor(inputs)\n",
    "\n",
    "                _, preds = self.bertCrf_model.predict(input_ids=bert_inputs.to(device),\n",
    "                                                      bert_attention_mask=bert_attention_mask.to(device),\n",
    "                                                      bert_token_mask=bert_token_mask,\n",
    "                                                      alignment=wordpiece_alignment,\n",
    "                                                      splits=(split_alignments, lengths),\n",
    "                                                      token_mask=token_mask)\n",
    "                if idx2label:\n",
    "                    for i, (path, score) in enumerate(preds):\n",
    "                        labels = [idx2label[p] for p in path]\n",
    "                        offset_index = i_batch * BATCH_SIZE + i\n",
    "                        entities.append([[e.span[0], e.span[1], e.tag, e.text]\n",
    "                                         for e in extract_entity(labels, spans[offset_index], text[offset_index])])\n",
    "        return {'annotations': entities}\n",
    "\n",
    "\n",
    "def load_data_processor(inputs):\n",
    "    token_spans = []\n",
    "    tokens = []\n",
    "    for line in inputs:\n",
    "        token_spans.append(list(tokenizer.span_tokenize(line)))\n",
    "        tokens.append([line[start: end] for start, end in token_spans[-1]])\n",
    "\n",
    "    processor = NERDatasetBatch.from_params(params=params, inputs=tokens)\n",
    "    return processor, tokens, token_spans\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    allowed_transitions = None\n",
    "    model = BertCRF(num_tags=params['num_tags'],\n",
    "                    model_name=params['model_name'],\n",
    "                    stride=params['stride'],\n",
    "                    include_start_end_transitions=True,\n",
    "                    constraints=allowed_transitions)\n",
    "    return model\n",
    "\n",
    "\n",
    "def extract_entity(preds, spans, text, length=20):\n",
    "    \"\"\"\n",
    "    extract entity from label sequence\n",
    "    :param preds: a list of labels in a sentence\n",
    "    :type preds: List[str\n",
    "    :param spans:\n",
    "    :type spans:\n",
    "    :return: A list of entity object\n",
    "    :rtype: List[Entity]\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    tmp = []\n",
    "\n",
    "    for i, token in enumerate(preds):\n",
    "        if token == 'O':\n",
    "            pos, tag = 'O', 'O'\n",
    "            label = None\n",
    "        else:\n",
    "            pos, tag = token.split('-')\n",
    "            label = Entity_Label(index=i, pos=pos, tag=tag, span=spans[i])\n",
    "\n",
    "        if pos in {'B', 'O'} and tmp:\n",
    "            start_span = tmp[0].span[0]\n",
    "            end_span = tmp[-1].span[1]\n",
    "            entities.append(Entity(span=(start_span, end_span),\n",
    "                                   tag=tmp[0].tag,\n",
    "                                   text=text[start_span:end_span],\n",
    "                                   pre=text[max(0, start_span - length):start_span],\n",
    "                                   post=text[end_span: end_span + length]))\n",
    "            tmp[:] = []\n",
    "        if pos == 'B' or pos == 'I':\n",
    "            tmp.append(label)\n",
    "\n",
    "    if tmp:\n",
    "        start_span = tmp[0].span[0]\n",
    "        end_span = tmp[-1].span[-1]\n",
    "        entities.append(\n",
    "            Entity(span=(start_span, end_span),\n",
    "                   tag=tmp[0].tag,\n",
    "                   text=text[start_span:end_span],\n",
    "                   pre=text[max(0, start_span - length):start_span],\n",
    "                   post=text[end_span:end_span + length])\n",
    "        )\n",
    "    return entities\n",
    "\n",
    "\n",
    "# Create a function called \"chunks\" with two arguments, l and n:\n",
    "def chunks(l, n):\n",
    "    # For item i in a range that is a length of l,\n",
    "    for i in range(0, len(l), n):\n",
    "        # Create an index range for l of n items:\n",
    "        yield l[i:i + n]\n",
    "\n",
    "\n",
    "def clean_Nones(ner_tags_):\n",
    "    ner_tags = []\n",
    "    # had to do this as the position of entity tag and entity are exchanged in CD\n",
    "    for each_ner_tag in ner_tags_:\n",
    "        if 'CD' == each_ner_tag[2]:\n",
    "            ner_tags.append([each_ner_tag[0], each_ner_tag[1], each_ner_tag[3], each_ner_tag[2]])\n",
    "        else:\n",
    "            ner_tags.append(each_ner_tag)\n",
    "\n",
    "    ner_tags = sorted(ner_tags, key=lambda x: len(x[3]), reverse=True)\n",
    "    if len(ner_tags) == 1 and 'None' in ner_tags:\n",
    "        return ner_tags\n",
    "    elif len(ner_tags) > 1 and 'None' in ner_tags:\n",
    "        ner_tags.remove('None')\n",
    "        return ner_tags\n",
    "    else:\n",
    "        return ner_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will compare ml tags and ztags. The agreed tags are then returned back\n",
    "def compare_ml_annotations_with_dictionary_tagged(ml_tags_, z_tags_, missing_list_):\n",
    "    agreed_z_tags = set()\n",
    "#     print(z_tags_, ml_tags_)\n",
    "    for each_z_tag in z_tags_:\n",
    "        for each_ml_annotation in ml_tags_:\n",
    "            if each_z_tag.lower() in missing_list_:\n",
    "                agreed_z_tags.add(each_z_tag)\n",
    "            else:\n",
    "                score = fuzz.partial_ratio(each_ml_annotation, each_z_tag) #token_set_ratio\n",
    "                if score > 80:\n",
    "                    agreed_z_tags.add(each_z_tag)\n",
    "    return agreed_z_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read xmls files\n",
    "def getfileblocks(file_path):\n",
    "    \n",
    "    subFileBlocks = []\n",
    "\n",
    "    with io.open(file_path, 'r', encoding='utf8') as fh:\n",
    "        abs_text_content = fh.read()\n",
    "        \n",
    "    subFileBlocks = abs_text_content.split('<PubmedArticle xmlns:z=\"ebistuff\">\\n')\n",
    "\n",
    "    return subFileBlocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will generate the tag spans given the missing spans of entities\n",
    "def get_new_missing_tags(each_sentence, missing_list_, tag_type):\n",
    "    new_entities = []\n",
    "    for missing_string in missing_list_:\n",
    "        for i in re.finditer(missing_string, each_sentence):\n",
    "            indexlocation= i.span()\n",
    "    #         print(indexlocation)\n",
    "            startindex= i.start()\n",
    "            endindex= i.end()\n",
    "            entity = each_sentence[indexlocation[0]:indexlocation[1]]\n",
    "            new_entities.append([startindex,endindex, tag_type, entity])\n",
    "    return new_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will get matches\n",
    "def get_sentences_matches_tags(sentences_tags,abs_full):\n",
    "    matches = defaultdict(list)\n",
    "    for each_sentence, ml_tags in sentences_tags.items():\n",
    "        for each_ml_tag in ml_tags:\n",
    "            if each_ml_tag[2]!= 'OG':\n",
    "                mini_dict = {}\n",
    "                mini_dict['label'] = each_ml_tag[3]\n",
    "                mini_dict['type'] = each_ml_tag[2]\n",
    "                mini_dict['startInSentence'] = each_ml_tag[0]\n",
    "                mini_dict['endInSentence'] = each_ml_tag[1]\n",
    "                if each_sentence in abs_full:\n",
    "                    start_index = abs_full.find(each_sentence)\n",
    "                    mini_dict['sectionStart'] = start_index\n",
    "                    mini_dict['sectionEnd'] = start_index + len(each_sentence)\n",
    "                matches[each_sentence].append(mini_dict)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "# map annotations in sets of pairs\n",
    "def get_mapped_list_from_annotations(annotation_list):\n",
    "    mapped_list = list(itertools.combinations(annotation_list, 2))\n",
    "\n",
    "    unique_maplist = []\n",
    "    for each_list in mapped_list:\n",
    "        if each_list[0][2] !=each_list[1][2] and each_list[1][2]!='OG' and each_list[0][2]!='OG':\n",
    "            unique_maplist.append((each_list[0], each_list[1]))\n",
    "\n",
    "    return unique_maplist  \n",
    "\n",
    "# get only those sentences with relevant pairs\n",
    "def get_sentences_offset_per_cooccurance(sentences_tags):\n",
    "    \n",
    "    dict_gp_ds = defaultdict(list)\n",
    "    dict_gp_cd = defaultdict(list)\n",
    "    dict_ds_cd = defaultdict(list)\n",
    "\n",
    "    for sentence, tags in sentences_tags.items():\n",
    "        if len(tags)>1: # only if more than 1 tag is available\n",
    "            check_tags =np.array(tags)\n",
    "            if 'GP' in  check_tags and 'DS' in check_tags:\n",
    "                dict_gp_ds[sentence] = get_mapped_list_from_annotations(tags)\n",
    "            if 'GP' in  check_tags and 'CD' in check_tags:\n",
    "                dict_gp_cd[sentence] = get_mapped_list_from_annotations(tags)\n",
    "            if 'DS' in  check_tags and 'CD' in check_tags:\n",
    "                dict_ds_cd[sentence]= get_mapped_list_from_annotations(tags)         \n",
    "                \n",
    "    return dict_gp_ds, dict_gp_cd, dict_ds_cd\n",
    "\n",
    "\n",
    "# if not in the right position if the pair and swap them such that always GP is followed by either CD or DS and DS is followed by CD\n",
    "def swap_positions(cooccurance_list, pos1, pos2): \n",
    "    cooccurance_list[pos1], cooccurance_list[pos2] = cooccurance_list[pos2], cooccurance_list[pos1]\n",
    "    return cooccurance_list    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for getting relationship text\n",
    "def get_relations(gp_ds_text_sentence):\n",
    "    docs = relation_model2(gp_ds_text_sentence)\n",
    "    rel_list =[]\n",
    "    for ent in docs.ents:\n",
    "        if ent.label_!='GP' and ent.label_!='DS':\n",
    "            rel_dict = {}\n",
    "            rel_dict['startr'] = ent.start_char\n",
    "            rel_dict['endr'] = ent.end_char\n",
    "            rel_dict['labelr'] = ent.text\n",
    "            rel_dict['typer'] = ent.label_\n",
    "            rel_list.append(rel_dict)\n",
    "    return rel_list\n",
    "\n",
    "# roundoff the association model scores\n",
    "def roundoff(dict_y):\n",
    "    for k, v in dict_y.items():\n",
    "        v = round(v,2) \n",
    "        dict_y[k] = v \n",
    "    return dict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the occurances\n",
    "def get_cooccurance_evidence(average_evidence_scores, dict_tags, tag_type_1, tag_type_2):\n",
    "    co_occurance_sentences = defaultdict(list)\n",
    "    #     mined_sentences = []\n",
    "    for each_sent_map, mappedtags in dict_tags.items():\n",
    "        # always see that GP-DS, GP-CD and CD-DS is followed\n",
    "\n",
    "        if tag_type_1 not in mappedtags[0][0][2]:\n",
    "            mappedtags[0] = swap_positions(list(mappedtags[0]), 0, 1)\n",
    "        else:\n",
    "            mappedtags[0] = list(mappedtags[0])\n",
    "\n",
    "        for eachtag in mappedtags:\n",
    "            if tag_type_1 == eachtag[0][2] and tag_type_2 == eachtag[1][2]:\n",
    "                mini_dict = {}\n",
    "                mini_dict['start1'] = eachtag[0][0]\n",
    "                mini_dict['end1'] = eachtag[0][1]\n",
    "                mini_dict['label1'] = eachtag[0][3]\n",
    "                mini_dict['start2'] = eachtag[1][0]\n",
    "                mini_dict['end2'] = eachtag[1][1]\n",
    "                mini_dict['label2'] = eachtag[1][3]\n",
    "                mini_dict['type'] = tag_type_1 + '-' + tag_type_2\n",
    "\n",
    "                if average_evidence_scores[each_sent_map]:\n",
    "                    mini_dict['evidence_score'] = average_evidence_scores[each_sent_map]\n",
    "                else:\n",
    "                    mini_dict['evidence_score'] = 1\n",
    "\n",
    "                if tag_type_1 == 'GP' and tag_type_2 == 'DS':\n",
    "                    # get associations scores\n",
    "                    mini_dict['association'] = roundoff(relation_model1(each_sent_map).cats)\n",
    "                    # get relations\n",
    "                    rels = get_relations(each_sent_map)\n",
    "                    if rels:\n",
    "                        mini_dict['relation'] = rels\n",
    "                co_occurance_sentences[each_sent_map].append(mini_dict)\n",
    "    return co_occurance_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dictionary for matches and co-occurances, section and other scores\n",
    "def generate_interested_sentences_in_json_format(final_sentences, section_tags, match_gp_ds_cd, co_occurance_gp_ds,co_occurance_gp_cd,co_occurance_ds_cd):\n",
    "    interested_sentences=[]\n",
    "    for each_sentence, tags in final_sentences.items():\n",
    "        minidict = {}\n",
    "\n",
    "        minidict['text'] = each_sentence\n",
    "\n",
    "        if section_tags[each_sentence]:\n",
    "            minidict['section'] = list(section_tags[each_sentence])[0]\n",
    "        else:\n",
    "            minidict['section'] = 'Other'\n",
    "\n",
    "        all_matches = match_gp_ds_cd[each_sentence]\n",
    "\n",
    "        if all_matches:\n",
    "            minidict['matches'] = all_matches\n",
    "\n",
    "        all_co_occurances = co_occurance_gp_ds[each_sentence] + co_occurance_gp_cd[each_sentence]+co_occurance_ds_cd[each_sentence]\n",
    "\n",
    "        if all_co_occurances:\n",
    "            minidict['co-occurrence'] = all_co_occurances\n",
    "        if all_co_occurances or all_matches:\n",
    "            interested_sentences.append(minidict)\n",
    "    \n",
    "    return interested_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ml_tags(all_sentences, missing_list_):\n",
    "    ML_annotations = ml_model.post(all_sentences)\n",
    "    # Biobert is missing COVIS-19, need to retrain the model later. For now I tag it as DS\n",
    "    final_annotations =[]\n",
    "    for each_annotation in ML_annotations['annotations']:\n",
    "        if each_annotation: # Biobert is tagging COVIS-19 as GP need to retrain the model later. For now I tag it as DS\n",
    "            if each_annotation[0][2]=='GP' and each_annotation[0][3].lower() in missing_list_: \n",
    "                each_annotation[0][2]='DS'\n",
    "                final_annotations.append(each_annotation)\n",
    "            elif each_annotation[0][2]=='CD' and each_annotation[0][3].lower()=='and':\n",
    "                final_annotations.append(each_annotation)\n",
    "            else:\n",
    "                final_annotations.append(each_annotation)\n",
    "        else:\n",
    "            final_annotations.append(each_annotation)\n",
    "    \n",
    "    return final_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentence_level_details(soup):\n",
    "    \n",
    "    plain_sentences_ = []\n",
    "    section_tags_ = defaultdict(set)\n",
    "    evidence_scores_ = defaultdict(list)\n",
    "    average_evidence_scores__ = defaultdict(list)\n",
    "    uniprot_set_ = set()\n",
    "    efo_set_ = set()\n",
    "    line_count = 0    \n",
    "  \n",
    "    # get all the sentences\n",
    "    all_sentences = soup.find_all('sent')# all_sentences = soup.find_all('SENT')\n",
    "    # get uniprot tags \n",
    "    try:\n",
    "        uniprot_ztags = soup.find_all('z:uniprot')\n",
    "        for each_tag in uniprot_ztags:\n",
    "            uniprot_set_.add(each_tag.text)\n",
    "    except:\n",
    "        print('no uniprot_ztags found ')\n",
    "    # get efo tags \n",
    "    try:   \n",
    "        efo_ztags = soup.find_all('z:efo')\n",
    "        for each_tag in efo_ztags:\n",
    "            efo_set_.add(each_tag.text)\n",
    "    except:\n",
    "        print('no efo_ztags found ')\n",
    "    \n",
    "\n",
    "    # get abstract details if found\n",
    "    try:\n",
    "        abs_full = soup.find('abstract').text\n",
    "        abs_sentences = soup.find('abstract').find_all('plain')\n",
    "        total_abstract_length = len(abs_sentences)\n",
    "    except:\n",
    "        abs_full =''\n",
    "        abs_sentences =''\n",
    "\n",
    "    # get section tags, evidence_scores_ and plain sentences\n",
    "    for each_sentence in all_sentences:\n",
    "        extracted_sentence = each_sentence.plain\n",
    "\n",
    "        if extracted_sentence:\n",
    "            clean_text = unicodedata.normalize(\"NFKD\",extracted_sentence.text).strip()\n",
    "\n",
    "            try:\n",
    "                title_tag = extracted_sentence.findParent('articletitle')\n",
    "            except:\n",
    "                title_tag =''\n",
    "\n",
    "            try:\n",
    "                if title_tag:\n",
    "                    section_tags_[clean_text].add('title')\n",
    "                    evidence_scores_[clean_text].append(10)\n",
    "                else:\n",
    "                    try:\n",
    "                        if extracted_sentence in abs_sentences:\n",
    "                            section_tagged = 'Abstract'\n",
    "                        else:\n",
    "                            section_tagged = extracted_sentence.findParent('sec').title.text.strip()\n",
    "\n",
    "                    except:\n",
    "                        section_tagged =''\n",
    "\n",
    "                    if section_tagged:\n",
    "                        section_tags_[clean_text].add(section_tagged)\n",
    "                        # evidence scores\n",
    "                        if 'abstract' in section_tagged.lower():\n",
    "    #                         print(line_count)\n",
    "                            line_count = line_count+1\n",
    "                            if line_count ==1 or line_count==2:\n",
    "                                evidence_scores_[clean_text].append(2)\n",
    "                            elif line_count==total_abstract_length:\n",
    "                                evidence_scores_[clean_text].append(5)\n",
    "                            else:\n",
    "                                evidence_scores_[clean_text].append(3)\n",
    "                        else:\n",
    "                            evi_scor = assign_scores_to_sections(fulltext_scores,section_tagged)\n",
    "                            evidence_scores_[clean_text].append(evi_scor)\n",
    "                    else:\n",
    "                        evidence_scores_[clean_text].append(1)               \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            plain_sentences_.append(clean_text)\n",
    "#     calculate average evidence scores        \n",
    "    for each_sentence,scores in evidence_scores_.items():\n",
    "        average_score = mean(scores)\n",
    "        average_evidence_scores__[each_sentence] = average_score\n",
    "    \n",
    "    return section_tags_, average_evidence_scores__, plain_sentences_, uniprot_set_, efo_set_,abs_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '/nfs/gns/literature/machine-learning/Santosh/Gitlab/biobertepmc/reproduce_GP_DS_OG_CD/1604049631/'\n",
    "\n",
    "# path to the file that has model parameters\n",
    "params_path = MODEL_PATH + \"params.pickle\"\n",
    "with open(params_path, 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "params['max_ner_token_len'] = -1\n",
    "params['max_bert_token_len'] = -1\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ml_model = MLModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load association and relation models\n",
    "relation_model1 = en_relationv01.load()\n",
    "relation_model2 = en_ner_europepmc_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_publication_date(soup):\n",
    "    \n",
    "    date_year_1 = soup.find('pubmedpubdate')\n",
    "    date_year_2 = soup.find('datecompleted')\n",
    "    date_year_3 = soup.find('pubdate')\n",
    "\n",
    "    \n",
    "    if date_year_1:\n",
    "        try:\n",
    "            year = date_year_1.year.text\n",
    "        except:\n",
    "            year = ''\n",
    "        try:    \n",
    "            month = date_year_1.month.text\n",
    "        except:\n",
    "            month = ''\n",
    "        try:\n",
    "            day = date_year_1.day.text\n",
    "        except:\n",
    "            day = ''\n",
    "    elif date_year_2:\n",
    "        try:\n",
    "            year = date_year_2.year.text\n",
    "        except:\n",
    "            year = ''\n",
    "        try:    \n",
    "            month = date_year_2.month.text\n",
    "        except:\n",
    "            month = ''\n",
    "        try:\n",
    "            day = date_year_2.day.text\n",
    "        except:\n",
    "            day = ''\n",
    "    elif date_year_3:\n",
    "        try:\n",
    "            year = date_year_3.year.text\n",
    "        except:\n",
    "            year = ''\n",
    "        try:    \n",
    "            month = date_year_3.month.text\n",
    "        except:\n",
    "            month = ''\n",
    "        try:\n",
    "            day = date_year_3.day.text\n",
    "        except:\n",
    "            day = ''\n",
    "    else:\n",
    "        year = ''\n",
    "        month = ''\n",
    "        day = ''\n",
    "            \n",
    "   \n",
    "    pub_date = year+'-'+month+'-'+day \n",
    "    \n",
    "    return pub_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_json(soup,section_tags,og_set,tagged_sentences, match_gp_ds_cd, co_occurance_gp_ds, co_occurance_gp_cd, co_occurance_ds_cd):\n",
    "    json_generated = {}\n",
    "\n",
    "    try:\n",
    "        json_generated['pmid'] = soup.find('pmid').text\n",
    "    except:\n",
    "            json_generated['pmid'] = ''\n",
    "\n",
    " \n",
    "    json_generated['pubDate'] = get_publication_date(soup)\n",
    "\n",
    "    json_generated['organisms'] = list(og_set)\n",
    "\n",
    "    interested_sentences = generate_interested_sentences_in_json_format(tagged_sentences, section_tags, match_gp_ds_cd, co_occurance_gp_ds, co_occurance_gp_cd, co_occurance_ds_cd)\n",
    "    json_generated['sentences'] = interested_sentences\n",
    "    \n",
    "    return json_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_ml_tagged_sentences(sentences,ml_annots, missing_list_):\n",
    "    \n",
    "    gp_set = set()\n",
    "    ds_set = set()\n",
    "    cd_set = set()\n",
    "    og_set = set()\n",
    "    ml_tagged_sentences ={}\n",
    "    count=0\n",
    "    \n",
    "    for each_sentence in sentences:\n",
    "        new_entities = get_new_missing_tags(each_sentence, missing_list_, tag_type='DS')\n",
    "        all_tags = new_entities+ml_annots[count] \n",
    "\n",
    "        if all_tags:\n",
    "            ml_tagged_sentences[each_sentence] = all_tags\n",
    "            for each_ml_tag in all_tags:\n",
    "                if each_ml_tag[2] =='GP':\n",
    "                    gp_set.add(each_ml_tag[3])\n",
    "                elif each_ml_tag[2] =='DS':\n",
    "                    ds_set.add(each_ml_tag[3])\n",
    "                if each_ml_tag[2] =='CD':\n",
    "                    cd_set.add(each_ml_tag[3])\n",
    "                if each_ml_tag[2] =='OG':\n",
    "                    og_set.add(each_ml_tag[3])\n",
    "        count = count+1\n",
    "    return ml_tagged_sentences, gp_set, cd_set, og_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_ztag_sentences(sentences,uniprot_fp_removed_set,z_efo_set,cd_set):\n",
    "    \n",
    "    new_cd_set = set()\n",
    "    ztag_sentences = {}\n",
    "\n",
    "    for each_cd_tag in cd_set:\n",
    "        if 'and' != each_cd_tag:\n",
    "            new_cd_set.add(each_cd_tag.replace(')','').replace('(','').strip())\n",
    "        \n",
    "\n",
    "    for each_sentence in sentences:\n",
    "        uniport_entities = get_new_missing_tags(each_sentence, uniprot_fp_removed_set, tag_type='GP')\n",
    "        efo_entities = get_new_missing_tags(each_sentence, z_efo_set, tag_type='DS')\n",
    "        try:\n",
    "            cd_entities = get_new_missing_tags(each_sentence, new_cd_set, tag_type='CD')\n",
    "        except:\n",
    "            cd_entities =[]\n",
    "        \n",
    "        all_tags = uniport_entities+efo_entities+cd_entities\n",
    "        \n",
    "        if all_tags:\n",
    "            ztag_sentences[each_sentence]= all_tags\n",
    "    \n",
    "    return ztag_sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_path = '/nfs/production/literature/shyama/Abs20.09/'\n",
    "data_file_path = data_folder_path+ 'Annot_medline.20016000.xml' #'Annot_PMC1851099_PMC1994013_split_19.xml', Annot_PMC6432232_PMC6447240_split_36.xml'#'Annot_PMC2111990_PMC2131188_split_99.xml'#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/nfs/production/literature/shyama/Abs20.09/Annot_medline.20016000.xml'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = getfileblocks(data_file_path)\n",
    "\n",
    "# ml_result_path = '/nfs/production/literature/Santosh_Tirunagari/NMP_test/'\n",
    "# ztag_result_path ='/nfs/production/literature/Santosh_Tirunagari/NDP_test/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2860"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "each_file =files_list[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_soup = BeautifulSoup(each_file, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2009-12-18'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_publication_date(xml_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "section_tag_sents, average_evidence_scores_sents, plain_sentences, uniprot_set, efo_set, absfull = extract_sentence_level_details(\n",
    "    xml_soup)\n",
    "\n",
    "ml_annotations = get_ml_tags(plain_sentences,missing_list)\n",
    "mltag_sentences, ml_gp_set, ml_cd_set, ml_og_set = get_only_ml_tagged_sentences(plain_sentences, ml_annotations,\n",
    "                                                                                missing_list)\n",
    "\n",
    "uniprot_nofp_set = compare_ml_annotations_with_dictionary_tagged(ml_gp_set, uniprot_set, missing_list)\n",
    "ztag_sentences = get_only_ztag_sentences(plain_sentences, uniprot_nofp_set, efo_set, ml_cd_set)\n",
    "\n",
    "ml_gp_ds, ml_gp_cd, ml_ds_cd = get_sentences_offset_per_cooccurance(mltag_sentences)\n",
    "\n",
    "ztag_gp_ds, ztag_gp_cd, ztag_ds_cd = get_sentences_offset_per_cooccurance(ztag_sentences)\n",
    "\n",
    "ml_co_occurance_gp_ds = get_cooccurance_evidence(average_evidence_scores_sents, ml_gp_ds, tag_type_1='GP', tag_type_2='DS')\n",
    "ml_co_occurance_gp_cd = get_cooccurance_evidence(average_evidence_scores_sents, ml_gp_cd, tag_type_1='GP', tag_type_2='CD')\n",
    "ml_co_occurance_ds_cd = get_cooccurance_evidence(average_evidence_scores_sents, ml_ds_cd, tag_type_1='DS', tag_type_2='CD')\n",
    "\n",
    "ztag_co_occurance_gp_ds = get_cooccurance_evidence(average_evidence_scores_sents, ztag_gp_ds, tag_type_1='GP', tag_type_2='DS')\n",
    "ztag_co_occurance_gp_cd = get_cooccurance_evidence(average_evidence_scores_sents, ztag_gp_cd, tag_type_1='GP', tag_type_2='CD')\n",
    "ztag_co_occurance_ds_cd = get_cooccurance_evidence(average_evidence_scores_sents, ztag_ds_cd, tag_type_1='DS', tag_type_2='CD')\n",
    "\n",
    "ml_match_gp_ds_cd = get_sentences_matches_tags(mltag_sentences, absfull)\n",
    "ztag_match_gp_ds_cd = get_sentences_matches_tags(ztag_sentences, absfull)\n",
    "\n",
    "ml_json = generate_final_json(xml_soup, section_tag_sents, ml_og_set, mltag_sentences, ml_match_gp_ds_cd,\n",
    "                              ml_co_occurance_gp_ds, ml_co_occurance_gp_cd, ml_co_occurance_ds_cd)\n",
    "ztag_json = generate_final_json(xml_soup, section_tag_sents, ml_og_set, ztag_sentences, ztag_match_gp_ds_cd,\n",
    "                                ztag_co_occurance_gp_ds, ztag_co_occurance_gp_cd, ztag_co_occurance_ds_cd)\n",
    "\n",
    "# # save ml json\n",
    "# with open(ml_result_path + 'NMP_' + data_file_path.split('/')[-1][:-3] + 'jsonl', 'at',\n",
    "#           encoding='utf8') as json_file:\n",
    "#     json.dump(ml_json, json_file, ensure_ascii=False)\n",
    "#     json_file.write('\\n')\n",
    "\n",
    "# # save ml json\n",
    "# with open(ztag_result_path + 'NDP_' + data_file_path.split('/')[-1][:-3] + 'jsonl', 'at',\n",
    "#           encoding='utf8') as json_file:\n",
    "#     json.dump(ztag_json, json_file, ensure_ascii=False)\n",
    "#     json_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><body><medlinecitation owner=\"NLM\" status=\"MEDLINE\">\n",
       "<document>\n",
       "<pmid version=\"1\">15528</pmid>\n",
       "<datecompleted>\n",
       "<year>1977</year>\n",
       "<month>05</month>\n",
       "<day>20</day>\n",
       "</datecompleted>\n",
       "<daterevised>\n",
       "<year>2019</year>\n",
       "<month>09</month>\n",
       "<day>02</day>\n",
       "</daterevised>\n",
       "<article pubmodel=\"Print\">\n",
       "<journal>\n",
       "<issn issntype=\"Print\">0302-8933</issn>\n",
       "<journalissue citedmedium=\"Print\">\n",
       "<volume>112</volume>\n",
       "<issue>2</issue>\n",
       "<pubdate>\n",
       "<year>1977</year>\n",
       "<month>Mar</month>\n",
       "<day>01</day>\n",
       "</pubdate>\n",
       "</journalissue>\n",
       "<title>Archives of microbiology</title>\n",
       "<isoabbreviation>Arch. Microbiol.</isoabbreviation>\n",
       "</journal>\n",
       "<articletitle><text><sent pm=\".\" sid=\"2263\"><plain>Acidostability of speroplasts prepared from Thiobacillus thiooxidans. </plain></sent>\n",
       "</text></articletitle>\n",
       "<pagination>\n",
       "<medlinepgn>163-8</medlinepgn>\n",
       "</pagination>\n",
       "<abstract>\n",
       "<abstracttext><text><sent pm=\".\" sid=\"2264\"><plain>Thiobacillus thiooxidans was acidostable even in the absence of its respiratory substrate, elementary sulfur. </plain></sent>\n",
       "<sent pm=\".\" sid=\"2265\"><plain>This suggests that the acidostability of the bacterium was enery-independent. </plain></sent>\n",
       "<sent pm=\".\" sid=\"2266\"><plain>The organism was subjected to osmotic <z:efo cat=\"phenotype\" ids=\"HP_0031273\">shock</z:efo> with 0.75 M sucrose at 0 degrees C and then treated with snail intestinal juice in the presence of 0.3 M sucrose. </plain></sent>\n",
       "<sent pm=\".\" sid=\"2267\"><plain>The decrease in the optical density of the sample thus prepared on dilution with deionized water and electron microscopic observation of the sample showed that spheroplasts were formed from the bacterium by this procedure. </plain></sent>\n",
       "<sent pm=\".\" sid=\"2268\"><plain>Spheroplasts were able to respire sulfur and their respiratory activity was acidostable. </plain></sent>\n",
       "<sent pm=\".\" sid=\"2269\"><plain>Spheroplasts, when treated with Nagase, proteolytic enzyme, lost their acidostability, and some protein components disappeared from the membrane fraction. </plain></sent>\n",
       "<sent pm=\".\" sid=\"2270\"><plain>This suggests that the acidostability of the bacterium may be related to protein conponents of the membrane. </plain></sent>\n",
       "</text></abstracttext>\n",
       "</abstract>\n",
       "<authorlist completeyn=\"Y\">\n",
       "<author validyn=\"Y\">\n",
       "<lastname>Noguchi</lastname>\n",
       "<forename>A</forename>\n",
       "<initials>A</initials>\n",
       "</author>\n",
       "<author validyn=\"Y\">\n",
       "<lastname>Takama</lastname>\n",
       "<forename>M</forename>\n",
       "<initials>M</initials>\n",
       "</author>\n",
       "<author validyn=\"Y\">\n",
       "<lastname>Sekiguchi</lastname>\n",
       "<forename>T</forename>\n",
       "<initials>T</initials>\n",
       "</author>\n",
       "<author validyn=\"Y\">\n",
       "<lastname>Koyama</lastname>\n",
       "<forename>N</forename>\n",
       "<initials>N</initials>\n",
       "</author>\n",
       "<author validyn=\"Y\">\n",
       "<lastname>Nosoh</lastname>\n",
       "<forename>Y</forename>\n",
       "<initials>Y</initials>\n",
       "</author>\n",
       "</authorlist>\n",
       "<language>eng</language>\n",
       "<publicationtypelist>\n",
       "<publicationtype ui=\"D016428\">Journal Article</publicationtype>\n",
       "</publicationtypelist>\n",
       "</article>\n",
       "<medlinejournalinfo>\n",
       "<country>Germany</country>\n",
       "<medlineta>Arch Microbiol</medlineta>\n",
       "<nlmuniqueid>0410427</nlmuniqueid>\n",
       "<issnlinking>0302-8933</issnlinking>\n",
       "</medlinejournalinfo>\n",
       "<chemicallist>\n",
       "<chemical>\n",
       "<registrynumber>0</registrynumber>\n",
       "<nameofsubstance ui=\"D001426\">Bacterial Proteins</nameofsubstance>\n",
       "</chemical>\n",
       "<chemical>\n",
       "<registrynumber>70FD1KFU70</registrynumber>\n",
       "<nameofsubstance ui=\"D013455\">Sulfur</nameofsubstance>\n",
       "</chemical>\n",
       "</chemicallist>\n",
       "<citationsubset>IM</citationsubset>\n",
       "<meshheadinglist>\n",
       "<meshheading>\n",
       "<descriptorname majortopicyn=\"Y\" ui=\"D013856\">Acidithiobacillus thiooxidans</descriptorname>\n",
       "<qualifiername majortopicyn=\"N\" ui=\"Q000378\">metabolism</qualifiername>\n",
       "<qualifiername majortopicyn=\"N\" ui=\"Q000648\">ultrastructure</qualifiername>\n",
       "</meshheading>\n",
       "<meshheading>\n",
       "<descriptorname majortopicyn=\"N\" ui=\"D001426\">Bacterial Proteins</descriptorname>\n",
       "<qualifiername majortopicyn=\"N\" ui=\"Q000502\">physiology</qualifiername>\n",
       "</meshheading>\n",
       "<meshheading>\n",
       "<descriptorname majortopicyn=\"N\" ui=\"D001433\">Bacteriolysis</descriptorname>\n",
       "</meshheading>\n",
       "<meshheading>\n",
       "<descriptorname majortopicyn=\"N\" ui=\"D006863\">Hydrogen-Ion Concentration</descriptorname>\n",
       "</meshheading>\n",
       "<meshheading>\n",
       "<descriptorname majortopicyn=\"N\" ui=\"D008854\">Microscopy, Electron</descriptorname>\n",
       "</meshheading>\n",
       "<meshheading>\n",
       "<descriptorname majortopicyn=\"N\" ui=\"D010101\">Oxygen Consumption</descriptorname>\n",
       "</meshheading>\n",
       "<meshheading>\n",
       "<descriptorname majortopicyn=\"Y\" ui=\"D013104\">Spheroplasts</descriptorname>\n",
       "<qualifiername majortopicyn=\"N\" ui=\"Q000378\">metabolism</qualifiername>\n",
       "<qualifiername majortopicyn=\"N\" ui=\"Q000648\">ultrastructure</qualifiername>\n",
       "</meshheading>\n",
       "<meshheading>\n",
       "<descriptorname majortopicyn=\"N\" ui=\"D013455\">Sulfur</descriptorname>\n",
       "<qualifiername majortopicyn=\"N\" ui=\"Q000378\">metabolism</qualifiername>\n",
       "</meshheading>\n",
       "<meshheading>\n",
       "<descriptorname majortopicyn=\"Y\" ui=\"D013855\">Thiobacillus</descriptorname>\n",
       "<qualifiername majortopicyn=\"N\" ui=\"Q000648\">ultrastructure</qualifiername>\n",
       "</meshheading>\n",
       "</meshheadinglist>\n",
       "</document>\n",
       "</medlinecitation>\n",
       "<pubmeddata>\n",
       "<history>\n",
       "<pubmedpubdate pubstatus=\"pubmed\">\n",
       "<year>1977</year>\n",
       "<month>3</month>\n",
       "<day>1</day>\n",
       "</pubmedpubdate>\n",
       "<pubmedpubdate pubstatus=\"medline\">\n",
       "<year>1977</year>\n",
       "<month>3</month>\n",
       "<day>1</day>\n",
       "<hour>0</hour>\n",
       "<minute>1</minute>\n",
       "</pubmedpubdate>\n",
       "<pubmedpubdate pubstatus=\"entrez\">\n",
       "<year>1977</year>\n",
       "<month>3</month>\n",
       "<day>1</day>\n",
       "<hour>0</hour>\n",
       "<minute>0</minute>\n",
       "</pubmedpubdate>\n",
       "</history>\n",
       "<publicationstatus>ppublish</publicationstatus>\n",
       "<articleidlist>\n",
       "<articleid idtype=\"pubmed\">15528</articleid>\n",
       "<articleid idtype=\"doi\">10.1007/bf00429330</articleid>\n",
       "</articleidlist>\n",
       "<referencelist>\n",
       "<reference>\n",
       "<citation>J Bacteriol. 1967 Jan;93(1):427-37</citation>\n",
       "<articleidlist>\n",
       "<articleid idtype=\"pubmed\">4960155</articleid>\n",
       "</articleidlist>\n",
       "</reference>\n",
       "<reference>\n",
       "<citation>J Biophys Biochem Cytol. 1961 Feb;9:409-14</citation>\n",
       "<articleidlist>\n",
       "<articleid idtype=\"pubmed\">13764136</articleid>\n",
       "</articleidlist>\n",
       "</reference>\n",
       "<reference>\n",
       "<citation>J Biol Chem. 1972 Jun 25;247(12):3962-72</citation>\n",
       "<articleidlist>\n",
       "<articleid idtype=\"pubmed\">4555955</articleid>\n",
       "</articleidlist>\n",
       "</reference>\n",
       "<reference>\n",
       "<citation>Biochemistry. 1971 Jun 22;10(13):2606-17</citation>\n",
       "<articleidlist>\n",
       "<articleid idtype=\"pubmed\">4326772</articleid>\n",
       "</articleidlist>\n",
       "</reference>\n",
       "<reference>\n",
       "<citation>Biochim Biophys Acta. 1973 Aug 31;314(2):257-60</citation>\n",
       "<articleidlist>\n",
       "<articleid idtype=\"pubmed\">4747069</articleid>\n",
       "</articleidlist>\n",
       "</reference>\n",
       "<reference>\n",
       "<citation>J Bacteriol. 1968 Jun;95(6):2182-5</citation>\n",
       "<articleidlist>\n",
       "<articleid idtype=\"pubmed\">5669895</articleid>\n",
       "</articleidlist>\n",
       "</reference>\n",
       "<reference>\n",
       "<citation>Biochim Biophys Acta. 1972 Mar 17;255(3):720-33</citation>\n",
       "<articleidlist>\n",
       "<articleid idtype=\"pubmed\">5020221</articleid>\n",
       "</articleidlist>\n",
       "</reference>\n",
       "<reference>\n",
       "<citation>J Cell Biol. 1963 Apr;17:19-58</citation>\n",
       "<articleidlist>\n",
       "<articleid idtype=\"pubmed\">13975866</articleid>\n",
       "</articleidlist>\n",
       "</reference>\n",
       "<reference>\n",
       "<citation>Z Naturforsch B. 1958 Sep;13B(9):597-605</citation>\n",
       "<articleidlist>\n",
       "<articleid idtype=\"pubmed\">13604673</articleid>\n",
       "</articleidlist>\n",
       "</reference>\n",
       "<reference>\n",
       "<citation>J Bacteriol. 1966 Aug;92(2):487-95</citation>\n",
       "<articleidlist>\n",
       "<articleid idtype=\"pubmed\">16562139</articleid>\n",
       "</articleidlist>\n",
       "</reference>\n",
       "<reference>\n",
       "<citation>Arch Microbiol. 1976 Aug;109(1-2):105-8</citation>\n",
       "<articleidlist>\n",
       "<articleid idtype=\"pubmed\">9043</articleid>\n",
       "</articleidlist>\n",
       "</reference>\n",
       "<reference>\n",
       "<citation>J Biol Chem. 1975 Sep 10;250(17):6963-8</citation>\n",
       "<articleidlist>\n",
       "<articleid idtype=\"pubmed\">1158890</articleid>\n",
       "</articleidlist>\n",
       "</reference>\n",
       "<reference>\n",
       "<citation>Arch Mikrobiol. 1972;84(1):54-68</citation>\n",
       "<articleidlist>\n",
       "<articleid idtype=\"pubmed\">4559703</articleid>\n",
       "</articleidlist>\n",
       "</reference>\n",
       "<reference>\n",
       "<citation>Bacteriol Rev. 1957 Sep;21(3):195-213</citation>\n",
       "<articleidlist>\n",
       "<articleid idtype=\"pubmed\">13471458</articleid>\n",
       "</articleidlist>\n",
       "</reference>\n",
       "<reference>\n",
       "<citation>J Bacteriol. 1971 Dec;108(3):992-5</citation>\n",
       "<articleidlist>\n",
       "<articleid idtype=\"pubmed\">4945206</articleid>\n",
       "</articleidlist>\n",
       "</reference>\n",
       "<reference>\n",
       "<citation>Biochim Biophys Acta. 1965 Jul 8;104(2):359-71</citation>\n",
       "<articleidlist>\n",
       "<articleid idtype=\"pubmed\">5855047</articleid>\n",
       "</articleidlist>\n",
       "</reference>\n",
       "<reference>\n",
       "<citation>Biochem J. 1966 Jun;99(3):682-7</citation>\n",
       "<articleidlist>\n",
       "<articleid idtype=\"pubmed\">5964965</articleid>\n",
       "</articleidlist>\n",
       "</reference>\n",
       "<reference>\n",
       "<citation>J Gen Microbiol. 1975 Feb;86(2):259-66</citation>\n",
       "<articleidlist>\n",
       "<articleid idtype=\"pubmed\">234509</articleid>\n",
       "</articleidlist>\n",
       "</reference>\n",
       "<reference>\n",
       "<citation>J Cell Biol. 1965 May;25:407-8</citation>\n",
       "<articleidlist>\n",
       "<articleid idtype=\"pubmed\">14287192</articleid>\n",
       "</articleidlist>\n",
       "</reference>\n",
       "<reference>\n",
       "<citation>Biochim Biophys Acta. 1969;193(2):268-76</citation>\n",
       "<articleidlist>\n",
       "<articleid idtype=\"pubmed\">4242764</articleid>\n",
       "</articleidlist>\n",
       "</reference>\n",
       "<reference>\n",
       "<citation>J Biol Chem. 1957 May;226(1):497-509</citation>\n",
       "<articleidlist>\n",
       "<articleid idtype=\"pubmed\">13428781</articleid>\n",
       "</articleidlist>\n",
       "</reference>\n",
       "<reference>\n",
       "<citation>J Biol Chem. 1951 Nov;193(1):265-75</citation>\n",
       "<articleidlist>\n",
       "<articleid idtype=\"pubmed\">14907713</articleid>\n",
       "</articleidlist>\n",
       "</reference>\n",
       "<reference>\n",
       "<citation>J Bacteriol. 1967 Dec;94(6):2069-70</citation>\n",
       "<articleidlist>\n",
       "<articleid idtype=\"pubmed\">6074409</articleid>\n",
       "</articleidlist>\n",
       "</reference>\n",
       "</referencelist>\n",
       "</pubmeddata>\n",
       "</body></html>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {'Acidostability of speroplasts prepared from Thiobacillus thiooxidans.': {'title'},\n",
       "             'Thiobacillus thiooxidans was acidostable even in the absence of its respiratory substrate, elementary sulfur.': {'Abstract'},\n",
       "             'This suggests that the acidostability of the bacterium was enery-independent.': {'Abstract'},\n",
       "             'The organism was subjected to osmotic shock with 0.75 M sucrose at 0 degrees C and then treated with snail intestinal juice in the presence of 0.3 M sucrose.': {'Abstract'},\n",
       "             'The decrease in the optical density of the sample thus prepared on dilution with deionized water and electron microscopic observation of the sample showed that spheroplasts were formed from the bacterium by this procedure.': {'Abstract'},\n",
       "             'Spheroplasts were able to respire sulfur and their respiratory activity was acidostable.': {'Abstract'},\n",
       "             'Spheroplasts, when treated with Nagase, proteolytic enzyme, lost their acidostability, and some protein components disappeared from the membrane fraction.': {'Abstract'},\n",
       "             'This suggests that the acidostability of the bacterium may be related to protein conponents of the membrane.': {'Abstract'}})"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_tag_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pmid': '15528',\n",
       " 'pubDate': '1977-05-20',\n",
       " 'organisms': ['snail', 'bacterium', 'Thiobacillus thiooxidans'],\n",
       " 'sentences': [{'text': 'The organism was subjected to osmotic shock with 0.75 M sucrose at 0 degrees C and then treated with snail intestinal juice in the presence of 0.3 M sucrose.',\n",
       "   'section': 'Abstract',\n",
       "   'matches': [{'label': 'sucrose',\n",
       "     'type': 'CD',\n",
       "     'startInSentence': 149,\n",
       "     'endInSentence': 156,\n",
       "     'sectionStart': 191,\n",
       "     'sectionEnd': 348}]}]}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pubmedpubdate pubstatus=\"pubmed\">\n",
       "<year>1994</year>\n",
       "<month>6</month>\n",
       "<day>15</day>\n",
       "<hour>0</hour>\n",
       "<minute>0</minute>\n",
       "</pubmedpubdate>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_year = xml_soup.find('pubmedpubdate')\n",
    "date_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    try:\n",
    "        try:\n",
    "            date_year = soup.find('pubdate')\n",
    "            try:\n",
    "                year = date_year.year.text\n",
    "            except:\n",
    "                year = ''\n",
    "            try:    \n",
    "                month = date_year.month.text\n",
    "            except:\n",
    "                month = ''\n",
    "            try:\n",
    "                day = date_year.day.text\n",
    "            except:\n",
    "                day = ''\n",
    "        \n",
    "        except:\n",
    "            try:\n",
    "                year = soup.find('year').text\n",
    "            except:\n",
    "                year = ''\n",
    "            try:    \n",
    "                month = soup.find('month').text\n",
    "            except:\n",
    "                month = ''\n",
    "            try:\n",
    "                day = soup.find('day').text\n",
    "            except:\n",
    "                day = ''\n",
    "            \n",
    "    except:\n",
    "         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='This script will process patch files to extract GP DS CDs in job folders on OTAR FullTextLoadings')\n",
    "parser.add_argument(\"-f\", \"--file\", nargs=1, required=True, help=\"OTAR New Pipeline GP DS CD extractor to Jsonl format\", metavar=\"PATH\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "process_each_file_in_job(args.file[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
