{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import gzip\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import random\n",
    "import sys\n",
    "import pathlib\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "# import multiprocessing\n",
    "from fuzzywuzzy import fuzz\n",
    "from statistics import mean\n",
    "import numpy as np\n",
    "import itertools\n",
    "import re\n",
    "import io\n",
    "# set the system path\n",
    "sys.path.insert(1, '/nfs/gns/literature/machine-learning/Santosh/Gitlab/biobertepmc/')\n",
    "\n",
    "# BioBERT NER models\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "from biobert.model.bert_crf_model import BertCRF\n",
    "from biobert.data_loader.epmc_loader import NERDatasetBatch\n",
    "from biobert.utils.utils import my_collate\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "# Relations and associations model\n",
    "import en_ner_europepmc_md\n",
    "import en_relationv01\n",
    "\n",
    "import unicodedata\n",
    "# import datetime\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Entity = namedtuple('Entity', ['span', 'tag', 'text', 'pre', 'post'])\n",
    "Entity_Label = namedtuple('Label', ['index', 'pos', 'tag', 'span'])\n",
    "missing_list = ['covid-19', 'coronavirus disease 2019', '2019-ncov', 'covid 19']    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all functions here\n",
    "\n",
    "batch_size = 8\n",
    "class MLModel:\n",
    "    def __init__(self):\n",
    "        self.bertCrf_model = load_model()\n",
    "\n",
    "        # bertCrf_model.load_state_dict(torch.load('/homes/yangx/home/gitrepo/biobertepmc/model/bert_crf_model.states', map_location=device))\n",
    "        self.bertCrf_model.load_state_dict(torch.load(MODEL_PATH + 'bert_crf_model.states', map_location=device))\n",
    "        self.bertCrf_model.bert_model.bert_model.to(device)\n",
    "\n",
    "    def post(self, sentences):\n",
    "        BATCH_SIZE = 16\n",
    "        text = sentences\n",
    "        # print(text)\n",
    "        with torch.no_grad():\n",
    "            processor, tokens, spans = load_data_processor(text)\n",
    "            dataLoader = DataLoader(dataset=processor, batch_size=BATCH_SIZE, collate_fn=my_collate, num_workers=2)\n",
    "\n",
    "            idx2label = params['idx2label']\n",
    "            self.bertCrf_model.eval()\n",
    "            entities = []\n",
    "            for i_batch, sample_batched in enumerate(dataLoader):\n",
    "                inputs = sample_batched['input']\n",
    "\n",
    "                bert_inputs, bert_attention_mask, bert_token_mask, wordpiece_alignment, split_alignments, lengths, token_mask \\\n",
    "                    = processor.tokens_totensor(inputs)\n",
    "\n",
    "                _, preds = self.bertCrf_model.predict(input_ids=bert_inputs.to(device),\n",
    "                                                      bert_attention_mask=bert_attention_mask.to(device),\n",
    "                                                      bert_token_mask=bert_token_mask,\n",
    "                                                      alignment=wordpiece_alignment,\n",
    "                                                      splits=(split_alignments, lengths),\n",
    "                                                      token_mask=token_mask)\n",
    "                if idx2label:\n",
    "                    for i, (path, score) in enumerate(preds):\n",
    "                        labels = [idx2label[p] for p in path]\n",
    "                        offset_index = i_batch * BATCH_SIZE + i\n",
    "                        entities.append([[e.span[0], e.span[1], e.tag, e.text]\n",
    "                                         for e in extract_entity(labels, spans[offset_index], text[offset_index])])\n",
    "        return {'annotations': entities}\n",
    "\n",
    "\n",
    "def load_data_processor(inputs):\n",
    "    token_spans = []\n",
    "    tokens = []\n",
    "    for line in inputs:\n",
    "        token_spans.append(list(tokenizer.span_tokenize(line)))\n",
    "        tokens.append([line[start: end] for start, end in token_spans[-1]])\n",
    "\n",
    "    processor = NERDatasetBatch.from_params(params=params, inputs=tokens)\n",
    "    return processor, tokens, token_spans\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    allowed_transitions = None\n",
    "    model = BertCRF(num_tags=params['num_tags'],\n",
    "                    model_name=params['model_name'],\n",
    "                    stride=params['stride'],\n",
    "                    include_start_end_transitions=True,\n",
    "                    constraints=allowed_transitions)\n",
    "    return model\n",
    "\n",
    "\n",
    "def extract_entity(preds, spans, text, length=20):\n",
    "    \"\"\"\n",
    "    extract entity from label sequence\n",
    "    :param preds: a list of labels in a sentence\n",
    "    :type preds: List[str\n",
    "    :param spans:\n",
    "    :type spans:\n",
    "    :return: A list of entity object\n",
    "    :rtype: List[Entity]\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    tmp = []\n",
    "\n",
    "    for i, token in enumerate(preds):\n",
    "        if token == 'O':\n",
    "            pos, tag = 'O', 'O'\n",
    "            label = None\n",
    "        else:\n",
    "            pos, tag = token.split('-')\n",
    "            label = Entity_Label(index=i, pos=pos, tag=tag, span=spans[i])\n",
    "\n",
    "        if pos in {'B', 'O'} and tmp:\n",
    "            start_span = tmp[0].span[0]\n",
    "            end_span = tmp[-1].span[1]\n",
    "            entities.append(Entity(span=(start_span, end_span),\n",
    "                                   tag=tmp[0].tag,\n",
    "                                   text=text[start_span:end_span],\n",
    "                                   pre=text[max(0, start_span - length):start_span],\n",
    "                                   post=text[end_span: end_span + length]))\n",
    "            tmp[:] = []\n",
    "        if pos == 'B' or pos == 'I':\n",
    "            tmp.append(label)\n",
    "\n",
    "    if tmp:\n",
    "        start_span = tmp[0].span[0]\n",
    "        end_span = tmp[-1].span[-1]\n",
    "        entities.append(\n",
    "            Entity(span=(start_span, end_span),\n",
    "                   tag=tmp[0].tag,\n",
    "                   text=text[start_span:end_span],\n",
    "                   pre=text[max(0, start_span - length):start_span],\n",
    "                   post=text[end_span:end_span + length])\n",
    "        )\n",
    "    return entities\n",
    "\n",
    "\n",
    "# Create a function called \"chunks\" with two arguments, l and n:\n",
    "def chunks(l, n):\n",
    "    # For item i in a range that is a length of l,\n",
    "    for i in range(0, len(l), n):\n",
    "        # Create an index range for l of n items:\n",
    "        yield l[i:i + n]\n",
    "\n",
    "\n",
    "def clean_Nones(ner_tags_):\n",
    "    ner_tags = []\n",
    "    # had to do this as the position of entity tag and entity are exchanged in CD\n",
    "    for each_ner_tag in ner_tags_:\n",
    "        if 'CD' == each_ner_tag[2]:\n",
    "            ner_tags.append([each_ner_tag[0], each_ner_tag[1], each_ner_tag[3], each_ner_tag[2]])\n",
    "        else:\n",
    "            ner_tags.append(each_ner_tag)\n",
    "\n",
    "    ner_tags = sorted(ner_tags, key=lambda x: len(x[3]), reverse=True)\n",
    "    if len(ner_tags) == 1 and 'None' in ner_tags:\n",
    "        return ner_tags\n",
    "    elif len(ner_tags) > 1 and 'None' in ner_tags:\n",
    "        ner_tags.remove('None')\n",
    "        return ner_tags\n",
    "    else:\n",
    "        return ner_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will compare ml tags and ztags. The agreed tags are then returned back\n",
    "def compare_ml_annotations_with_dictionary_tagged(ml_tags_, z_tags_, missing_list_):\n",
    "    agreed_z_tags = set()\n",
    "#     print(z_tags_, ml_tags_)\n",
    "    for each_z_tag in z_tags_:\n",
    "        for each_ml_annotation in ml_tags_:\n",
    "            if each_z_tag.lower() in missing_list_:\n",
    "                agreed_z_tags.add(each_z_tag)\n",
    "            else:\n",
    "                score = fuzz.partial_ratio(each_ml_annotation, each_z_tag) #token_set_ratio\n",
    "                if score > 80:\n",
    "                    agreed_z_tags.add(each_z_tag)\n",
    "    return agreed_z_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulltext_scores={\n",
    "    \n",
    "'title':10,\n",
    "'intro':1,\n",
    "'result':5,\n",
    "'discus':2,\n",
    "'conclus':2,\n",
    "'case':1,\n",
    "'fig':5,\n",
    "'table':5,\n",
    "'appendix':1,\n",
    "'other':1\n",
    "}\n",
    "# fulltext_scores\n",
    "\n",
    "\n",
    "def assign_scores_to_sections(fulltext_scores_, section_tagged_):\n",
    "    scores = []\n",
    "    for key,val in fulltext_scores_.items():\n",
    "        if key in section_tagged_.lower():\n",
    "            return val\n",
    "\n",
    "    return fulltext_scores_['other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read xmls files\n",
    "def getfileblocks(file_path):\n",
    "    subFileBlocks = []\n",
    "\n",
    "    with io.open(file_path, 'r', encoding='utf8') as fh:\n",
    "        for line in fh:\n",
    "            if line.startswith('<!DOCTYPE'):\n",
    "                subFileBlocks.append(line)\n",
    "            else:\n",
    "                subFileBlocks[-1] += line\n",
    "\n",
    "    return subFileBlocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will generate the tag spans given the missing spans of entities\n",
    "def get_new_missing_tags(each_sentence, missing_list_, tag_type):\n",
    "    new_entities = []\n",
    "    for missing_string in missing_list_:\n",
    "        for i in re.finditer(missing_string, each_sentence):\n",
    "            indexlocation= i.span()\n",
    "    #         print(indexlocation)\n",
    "            startindex= i.start()\n",
    "            endindex= i.end()\n",
    "            entity = each_sentence[indexlocation[0]:indexlocation[1]]\n",
    "            new_entities.append([startindex,endindex, tag_type, entity])\n",
    "    return new_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will get matches\n",
    "def get_sentences_matches_tags(sentences_tags,abs_full):\n",
    "    matches = defaultdict(list)\n",
    "    for each_sentence, ml_tags in sentences_tags.items():\n",
    "        for each_ml_tag in ml_tags:\n",
    "            if each_ml_tag[2]!= 'OG':\n",
    "                mini_dict = {}\n",
    "                mini_dict['label'] = each_ml_tag[3]\n",
    "                mini_dict['type'] = each_ml_tag[2]\n",
    "                mini_dict['startInSentence'] = each_ml_tag[0]\n",
    "                mini_dict['endInSentence'] = each_ml_tag[1]\n",
    "                if each_sentence in abs_full:\n",
    "                    start_index = abs_full.find(each_sentence)\n",
    "                    mini_dict['sectionStart'] = start_index\n",
    "                    mini_dict['sectionEnd'] = start_index + len(each_sentence)\n",
    "                matches[each_sentence].append(mini_dict)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "# map annotations in sets of pairs\n",
    "def get_mapped_list_from_annotations(annotation_list):\n",
    "    mapped_list = list(itertools.combinations(annotation_list, 2))\n",
    "\n",
    "    unique_maplist = []\n",
    "    for each_list in mapped_list:\n",
    "        if each_list[0][2] !=each_list[1][2] and each_list[1][2]!='OG' and each_list[0][2]!='OG':\n",
    "            unique_maplist.append((each_list[0], each_list[1]))\n",
    "\n",
    "    return unique_maplist  \n",
    "\n",
    "# get only those sentences with relevant pairs\n",
    "def get_sentences_offset_per_cooccurance(sentences_tags):\n",
    "    \n",
    "    dict_gp_ds = defaultdict(list)\n",
    "    dict_gp_cd = defaultdict(list)\n",
    "    dict_ds_cd = defaultdict(list)\n",
    "\n",
    "    for sentence, tags in sentences_tags.items():\n",
    "        if len(tags)>1: # only if more than 1 tag is available\n",
    "            check_tags =np.array(tags)\n",
    "            if 'GP' in  check_tags and 'DS' in check_tags:\n",
    "                dict_gp_ds[sentence] = get_mapped_list_from_annotations(tags)\n",
    "            if 'GP' in  check_tags and 'CD' in check_tags:\n",
    "                dict_gp_cd[sentence] = get_mapped_list_from_annotations(tags)\n",
    "            if 'DS' in  check_tags and 'CD' in check_tags:\n",
    "                dict_ds_cd[sentence]= get_mapped_list_from_annotations(tags)         \n",
    "                \n",
    "    return dict_gp_ds, dict_gp_cd, dict_ds_cd\n",
    "\n",
    "\n",
    "# if not in the right position if the pair and swap them such that always GP is followed by either CD or DS and DS is followed by CD\n",
    "def swap_positions(cooccurance_list, pos1, pos2): \n",
    "    cooccurance_list[pos1], cooccurance_list[pos2] = cooccurance_list[pos2], cooccurance_list[pos1]\n",
    "    return cooccurance_list    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for getting relationship text\n",
    "def get_relations(gp_ds_text_sentence):\n",
    "    docs = relation_model2(gp_ds_text_sentence)\n",
    "    rel_list =[]\n",
    "    for ent in docs.ents:\n",
    "        if ent.label_!='GP' and ent.label_!='DS':\n",
    "            rel_dict = {}\n",
    "            rel_dict['startr'] = ent.start_char\n",
    "            rel_dict['endr'] = ent.end_char\n",
    "            rel_dict['labelr'] = ent.text\n",
    "            rel_dict['typer'] = ent.label_\n",
    "            rel_list.append(rel_dict)\n",
    "    return rel_list\n",
    "\n",
    "# roundoff the association model scores\n",
    "def roundoff(dict_y):\n",
    "    for k, v in dict_y.items():\n",
    "        v = round(v,2) \n",
    "        dict_y[k] = v \n",
    "    return dict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the occurances\n",
    "def get_cooccurance_evidence(average_evidence_scores, dict_tags, tag_type_1, tag_type_2):\n",
    "    co_occurance_sentences = defaultdict(list)\n",
    "    #     mined_sentences = []\n",
    "    for each_sent_map, mappedtags in dict_tags.items():\n",
    "        # always see that GP-DS, GP-CD and CD-DS is followed\n",
    "\n",
    "        if tag_type_1 not in mappedtags[0][0][2]:\n",
    "            mappedtags[0] = swap_positions(list(mappedtags[0]), 0, 1)\n",
    "        else:\n",
    "            mappedtags[0] = list(mappedtags[0])\n",
    "\n",
    "        for eachtag in mappedtags:\n",
    "            if tag_type_1 == eachtag[0][2] and tag_type_2 == eachtag[1][2]:\n",
    "                mini_dict = {}\n",
    "                mini_dict['start1'] = eachtag[0][0]\n",
    "                mini_dict['end1'] = eachtag[0][1]\n",
    "                mini_dict['label1'] = eachtag[0][3]\n",
    "                mini_dict['start2'] = eachtag[1][0]\n",
    "                mini_dict['end2'] = eachtag[1][1]\n",
    "                mini_dict['label2'] = eachtag[1][3]\n",
    "                mini_dict['type'] = tag_type_1 + '-' + tag_type_2\n",
    "\n",
    "                if average_evidence_scores[each_sent_map]:\n",
    "                    mini_dict['evidence_score'] = average_evidence_scores[each_sent_map]\n",
    "                else:\n",
    "                    mini_dict['evidence_score'] = 1\n",
    "\n",
    "                if tag_type_1 == 'GP' and tag_type_2 == 'DS':\n",
    "                    # get associations scores\n",
    "                    mini_dict['association'] = roundoff(relation_model1(each_sent_map).cats)\n",
    "                    # get relations\n",
    "                    rels = get_relations(each_sent_map)\n",
    "                    if rels:\n",
    "                        mini_dict['relation'] = rels\n",
    "                co_occurance_sentences[each_sent_map].append(mini_dict)\n",
    "    return co_occurance_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dictionary for matches and co-occurances, section and other scores\n",
    "def generate_interested_sentences_in_json_format(final_sentences, section_tags, match_gp_ds_cd, co_occurance_gp_ds,co_occurance_gp_cd,co_occurance_ds_cd):\n",
    "    interested_sentences=[]\n",
    "    for each_sentence, tags in final_sentences.items():\n",
    "        minidict = {}\n",
    "\n",
    "        minidict['text'] = each_sentence\n",
    "\n",
    "        if section_tags[each_sentence]:\n",
    "            minidict['section'] = list(section_tags[each_sentence])[0]\n",
    "        else:\n",
    "            minidict['section'] = 'Other'\n",
    "\n",
    "        all_matches = match_gp_ds_cd[each_sentence]\n",
    "\n",
    "        if all_matches:\n",
    "            minidict['matches'] = all_matches\n",
    "\n",
    "        all_co_occurances = co_occurance_gp_ds[each_sentence] + co_occurance_gp_cd[each_sentence]+co_occurance_ds_cd[each_sentence]\n",
    "\n",
    "        if all_co_occurances:\n",
    "            minidict['co-occurrence'] = all_co_occurances\n",
    "        if all_co_occurances or all_matches:\n",
    "            interested_sentences.append(minidict)\n",
    "    \n",
    "    return interested_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ml_tags(all_sentences, missing_list_):\n",
    "    ML_annotations = ml_model.post(all_sentences)\n",
    "    # Biobert is missing COVIS-19, need to retrain the model later. For now I tag it as DS\n",
    "    final_annotations =[]\n",
    "    for each_annotation in ML_annotations['annotations']:\n",
    "        if each_annotation: # Biobert is tagging COVIS-19 as GP need to retrain the model later. For now I tag it as DS\n",
    "            if each_annotation[0][2]=='GP' and each_annotation[0][3].lower() in missing_list_: \n",
    "                each_annotation[0][2]='DS'\n",
    "                final_annotations.append(each_annotation)\n",
    "            elif each_annotation[0][2]=='CD' and each_annotation[0][3].lower()=='and':\n",
    "                final_annotations.append(each_annotation)\n",
    "            else:\n",
    "                final_annotations.append(each_annotation)\n",
    "        else:\n",
    "            final_annotations.append(each_annotation)\n",
    "    \n",
    "    return final_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentence_level_details(soup):\n",
    "    \n",
    "    plain_sentences_ = []\n",
    "    section_tags_ = defaultdict(set)\n",
    "    evidence_scores_ = defaultdict(list)\n",
    "    average_evidence_scores__ = defaultdict(list)\n",
    "    uniprot_set_ = set()\n",
    "    efo_set_ = set()\n",
    "    line_count = 0    \n",
    "  \n",
    "    # get all the sentences\n",
    "    all_sentences = soup.find_all('sent')# all_sentences = soup.find_all('SENT')\n",
    "    # get uniprot tags \n",
    "    try:\n",
    "        uniprot_ztags = soup.find_all('z:uniprot')\n",
    "        for each_tag in uniprot_ztags:\n",
    "            uniprot_set_.add(each_tag.text)\n",
    "    except:\n",
    "        print('no uniprot_ztags found ')\n",
    "    # get efo tags \n",
    "    try:   \n",
    "        efo_ztags = soup.find_all('z:efo')\n",
    "        for each_tag in efo_ztags:\n",
    "            efo_set_.add(each_tag.text)\n",
    "    except:\n",
    "        print('no efo_ztags found ')\n",
    "    \n",
    "\n",
    "    # get abstract details if found\n",
    "    try:\n",
    "        abs_full = soup.find('abstract').text\n",
    "        abs_sentences = soup.find('abstract').find_all('plain')\n",
    "        total_abstract_length = len(abs_sentences)\n",
    "    except:\n",
    "        abs_full =''\n",
    "        abs_sentences =''\n",
    "\n",
    "    # get section tags, evidence_scores_ and plain sentences\n",
    "    for each_sentence in all_sentences:\n",
    "        extracted_sentence = each_sentence.plain\n",
    "\n",
    "        if extracted_sentence:\n",
    "            clean_text = unicodedata.normalize(\"NFKD\",extracted_sentence.text).strip()\n",
    "\n",
    "            try:\n",
    "                title_tag = extracted_sentence.findParent('article-title')\n",
    "            except:\n",
    "                title_tag =''\n",
    "\n",
    "            try:\n",
    "                if title_tag:\n",
    "                    section_tags_[clean_text].add('title')\n",
    "                    evidence_scores_[clean_text].append(10)\n",
    "                else:\n",
    "                    try:\n",
    "                        if extracted_sentence in abs_sentences:\n",
    "                            section_tagged = 'Abstract'\n",
    "                        else:\n",
    "                            section_tagged = extracted_sentence.findParent('sec').title.text.strip()\n",
    "\n",
    "                    except:\n",
    "                        section_tagged =''\n",
    "\n",
    "                    if section_tagged:\n",
    "                        section_tags_[clean_text].add(section_tagged)\n",
    "                        # evidence scores\n",
    "                        if 'abstract' in section_tagged.lower():\n",
    "    #                         print(line_count)\n",
    "                            line_count = line_count+1\n",
    "                            if line_count ==1 or line_count==2:\n",
    "                                evidence_scores_[clean_text].append(2)\n",
    "                            elif line_count==total_abstract_length:\n",
    "                                evidence_scores_[clean_text].append(5)\n",
    "                            else:\n",
    "                                evidence_scores_[clean_text].append(3)\n",
    "                        else:\n",
    "                            evi_scor = assign_scores_to_sections(fulltext_scores,section_tagged)\n",
    "                            evidence_scores_[clean_text].append(evi_scor)\n",
    "                    else:\n",
    "                        evidence_scores_[clean_text].append(1)               \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            plain_sentences_.append(clean_text)\n",
    "#     calculate average evidence scores        \n",
    "    for each_sentence,scores in evidence_scores_.items():\n",
    "        average_score = mean(scores)\n",
    "        average_evidence_scores__[each_sentence] = average_score\n",
    "    \n",
    "    return section_tags_, average_evidence_scores__, plain_sentences_, uniprot_set_, efo_set_,abs_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '/nfs/gns/literature/machine-learning/Santosh/Gitlab/biobertepmc/reproduce_GP_DS_OG_CD/1604049631/'\n",
    "\n",
    "# path to the file that has model parameters\n",
    "params_path = MODEL_PATH + \"params.pickle\"\n",
    "with open(params_path, 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "params['max_ner_token_len'] = -1\n",
    "params['max_bert_token_len'] = -1\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ml_model = MLModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load association and relation models\n",
    "relation_model1 = en_relationv01.load()\n",
    "relation_model2 = en_ner_europepmc_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_publication_date(soup):\n",
    "\n",
    "    try:\n",
    "        try:\n",
    "            date_year = soup.find('pub-date', {'pub-type':\"epub\"})\n",
    "            try:\n",
    "                year = date_year.year.text\n",
    "            except:\n",
    "                year = ''\n",
    "            try:    \n",
    "                month = date_year.month.text\n",
    "            except:\n",
    "                month = ''\n",
    "            try:\n",
    "                day = date_year.day.text\n",
    "            except:\n",
    "                day = ''\n",
    "        \n",
    "        except:\n",
    "            try:\n",
    "                year = soup.find('year').text\n",
    "            except:\n",
    "                year = ''\n",
    "            try:    \n",
    "                month = soup.find('month').text\n",
    "            except:\n",
    "                month = ''\n",
    "            try:\n",
    "                day = soup.find('day').text\n",
    "            except:\n",
    "                day = ''\n",
    "            \n",
    "    except:\n",
    "         pass\n",
    "\n",
    "    \n",
    "    pub_date = year+'-'+month+'-'+day \n",
    "    \n",
    "    return pub_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_json(soup,section_tags,og_set,tagged_sentences, match_gp_ds_cd, co_occurance_gp_ds, co_occurance_gp_cd, co_occurance_ds_cd):\n",
    "    json_generated = {}\n",
    "\n",
    "    try:\n",
    "        json_generated['pmid'] = 'PMC'+soup.find(attrs={\"pub-id-type\" : \"pmcid\"}).text\n",
    "    except:\n",
    "        try:\n",
    "            json_generated['pmid'] = soup.find(attrs={\"pub-id-type\" : \"pmid\"}).text\n",
    "        except:\n",
    "            json_generated['pmid'] = ''\n",
    "\n",
    " \n",
    "    json_generated['pubDate'] = get_publication_date(soup)\n",
    "\n",
    "    json_generated['organisms'] = list(og_set)\n",
    "\n",
    "    interested_sentences = generate_interested_sentences_in_json_format(tagged_sentences, section_tags, match_gp_ds_cd, co_occurance_gp_ds, co_occurance_gp_cd, co_occurance_ds_cd)\n",
    "    json_generated['sentences'] = interested_sentences\n",
    "    \n",
    "    return json_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_ml_tagged_sentences(sentences,ml_annots, missing_list_):\n",
    "    \n",
    "    gp_set = set()\n",
    "    ds_set = set()\n",
    "    cd_set = set()\n",
    "    og_set = set()\n",
    "    ml_tagged_sentences ={}\n",
    "    count=0\n",
    "    \n",
    "    for each_sentence in sentences:\n",
    "        new_entities = get_new_missing_tags(each_sentence, missing_list_, tag_type='DS')\n",
    "        all_tags = new_entities+ml_annots[count] \n",
    "\n",
    "        if all_tags:\n",
    "            ml_tagged_sentences[each_sentence] = all_tags\n",
    "            for each_ml_tag in all_tags:\n",
    "                if each_ml_tag[2] =='GP':\n",
    "                    gp_set.add(each_ml_tag[3])\n",
    "                elif each_ml_tag[2] =='DS':\n",
    "                    ds_set.add(each_ml_tag[3])\n",
    "                if each_ml_tag[2] =='CD':\n",
    "                    cd_set.add(each_ml_tag[3])\n",
    "                if each_ml_tag[2] =='OG':\n",
    "                    og_set.add(each_ml_tag[3])\n",
    "        count = count+1\n",
    "    return ml_tagged_sentences, gp_set, cd_set, og_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_ztag_sentences(sentences,uniprot_fp_removed_set,z_efo_set,cd_set):\n",
    "    \n",
    "    new_cd_set = set()\n",
    "    ztag_sentences = {}\n",
    "\n",
    "    for each_cd_tag in cd_set:\n",
    "        if 'and' != each_cd_tag:\n",
    "            new_cd_set.add(each_cd_tag.replace(')','').replace('(','').strip())\n",
    "        \n",
    "\n",
    "    for each_sentence in sentences:\n",
    "        uniport_entities = get_new_missing_tags(each_sentence, uniprot_fp_removed_set, tag_type='GP')\n",
    "        efo_entities = get_new_missing_tags(each_sentence, z_efo_set, tag_type='DS')\n",
    "        try:\n",
    "            cd_entities = get_new_missing_tags(each_sentence, new_cd_set, tag_type='CD')\n",
    "        except:\n",
    "            cd_entities =[]\n",
    "        \n",
    "        all_tags = uniport_entities+efo_entities+cd_entities\n",
    "        \n",
    "        if all_tags:\n",
    "            ztag_sentences[each_sentence]= all_tags\n",
    "    \n",
    "    return ztag_sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_path = '/nfs/production/literature/Santosh_Tirunagari/20.09_FT_Chunks/'\n",
    "data_file_path = data_folder_path+ 'Annot_PMC2932713_PMC2959506_split_19.xml' #'Annot_PMC1851099_PMC1994013_split_19.xml', Annot_PMC6432232_PMC6447240_split_36.xml'#'Annot_PMC2111990_PMC2131188_split_99.xml'#\n",
    "files_list = getfileblocks(data_file_path)\n",
    "\n",
    "ml_result_path = '/nfs/production/literature/Santosh_Tirunagari/NMP_test/'\n",
    "ztag_result_path ='/nfs/production/literature/Santosh_Tirunagari/NDP_test/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/nfs/production/literature/Santosh_Tirunagari/20.09_FT_Chunks/Annot_PMC2932713_PMC2959506_split_19.xml'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "each_file =files_list[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_soup = BeautifulSoup(each_file, 'lxml')\n",
    "section_tag_sents, average_evidence_scores_sents, plain_sentences, uniprot_set, efo_set, absfull = extract_sentence_level_details(\n",
    "    xml_soup)\n",
    "\n",
    "ml_annotations = get_ml_tags(plain_sentences,missing_list)\n",
    "mltag_sentences, ml_gp_set, ml_cd_set, ml_og_set = get_only_ml_tagged_sentences(plain_sentences, ml_annotations,\n",
    "                                                                                missing_list)\n",
    "\n",
    "uniprot_nofp_set = compare_ml_annotations_with_dictionary_tagged(ml_gp_set, uniprot_set, missing_list)\n",
    "ztag_sentences = get_only_ztag_sentences(plain_sentences, uniprot_nofp_set, efo_set, ml_cd_set)\n",
    "\n",
    "ml_gp_ds, ml_gp_cd, ml_ds_cd = get_sentences_offset_per_cooccurance(mltag_sentences)\n",
    "\n",
    "ztag_gp_ds, ztag_gp_cd, ztag_ds_cd = get_sentences_offset_per_cooccurance(ztag_sentences)\n",
    "\n",
    "ml_co_occurance_gp_ds = get_cooccurance_evidence(average_evidence_scores_sents, ml_gp_ds, tag_type_1='GP', tag_type_2='DS')\n",
    "ml_co_occurance_gp_cd = get_cooccurance_evidence(average_evidence_scores_sents, ml_gp_cd, tag_type_1='GP', tag_type_2='CD')\n",
    "ml_co_occurance_ds_cd = get_cooccurance_evidence(average_evidence_scores_sents, ml_ds_cd, tag_type_1='DS', tag_type_2='CD')\n",
    "\n",
    "ztag_co_occurance_gp_ds = get_cooccurance_evidence(average_evidence_scores_sents, ztag_gp_ds, tag_type_1='GP', tag_type_2='DS')\n",
    "ztag_co_occurance_gp_cd = get_cooccurance_evidence(average_evidence_scores_sents, ztag_gp_cd, tag_type_1='GP', tag_type_2='CD')\n",
    "ztag_co_occurance_ds_cd = get_cooccurance_evidence(average_evidence_scores_sents, ztag_ds_cd, tag_type_1='DS', tag_type_2='CD')\n",
    "\n",
    "ml_match_gp_ds_cd = get_sentences_matches_tags(mltag_sentences, absfull)\n",
    "ztag_match_gp_ds_cd = get_sentences_matches_tags(ztag_sentences, absfull)\n",
    "\n",
    "ml_json = generate_final_json(xml_soup, section_tag_sents, ml_og_set, mltag_sentences, ml_match_gp_ds_cd,\n",
    "                              ml_co_occurance_gp_ds, ml_co_occurance_gp_cd, ml_co_occurance_ds_cd)\n",
    "ztag_json = generate_final_json(xml_soup, section_tag_sents, ml_og_set, ztag_sentences, ztag_match_gp_ds_cd,\n",
    "                                ztag_co_occurance_gp_ds, ztag_co_occurance_gp_cd, ztag_co_occurance_ds_cd)\n",
    "\n",
    "# save ml json\n",
    "with open(ml_result_path + 'NMP_' + data_file_path.split('/')[-1][:-3] + 'jsonl', 'at',\n",
    "          encoding='utf8') as json_file:\n",
    "    json.dump(ml_json, json_file, ensure_ascii=False)\n",
    "    json_file.write('\\n')\n",
    "\n",
    "# save ml json\n",
    "with open(ztag_result_path + 'NDP_' + data_file_path.split('/')[-1][:-3] + 'jsonl', 'at',\n",
    "          encoding='utf8') as json_file:\n",
    "    json.dump(ztag_json, json_file, ensure_ascii=False)\n",
    "    json_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_soup = BeautifulSoup(each_file, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='This script will process patch files to extract GP DS CDs in job folders on OTAR FullTextLoadings')\n",
    "parser.add_argument(\"-f\", \"--file\", nargs=1, required=True, help=\"OTAR New Pipeline GP DS CD extractor to Jsonl format\", metavar=\"PATH\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "process_each_file_in_job(args.file[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
