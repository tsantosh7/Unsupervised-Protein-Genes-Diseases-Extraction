{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "sys.path.append('/nfs/gns/literature/Santosh_Tirunagari/test Gitlab/epmc-ml-misc-library/')\n",
    "import capo_tools_lib\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "json_path = '/nfs/gns/literature/machine-learning/cleaned_dataset/'\n",
    "trainPMCids, devPMCids, testPMCids = capo_tools_lib.create_train_dev_test_splits(json_path, percentage = 0.70)\n",
    "\n",
    "   \n",
    "    \n",
    "def convert_json_to_spacy_format(json_path):\n",
    "\n",
    "    training_data = []\n",
    "    all_files = sorted(glob.glob(json_path + '*.json*'))\n",
    "    for each_manually_annotated_json in all_files:\n",
    "        with open(each_manually_annotated_json) as json_file_ner_rel:\n",
    "            json_data = json.loads(json_file_ner_rel.read())\n",
    "            \n",
    "            for articles in json_data:\n",
    "                pmc_id = articles  # json_data[articles]\n",
    "                if pmc_id in trainPMCids+devPMCids:\n",
    "                    for each_annotation in json_data[articles]['annotations']:\n",
    "                        text = each_annotation['sent'].encode('utf-8').decode('utf-8')\n",
    "                        entities = []\n",
    "                        ner = each_annotation['ner']\n",
    "                        if ner:\n",
    "                            for each_ner in ner:\n",
    "                                point_start = each_ner[0]\n",
    "                                point_end = each_ner[1]\n",
    "                                label = each_ner[3]\n",
    "                                entities.append((point_start, point_end,label))\n",
    "\n",
    "                            training_data.append((text, {\"entities\" : entities}))\n",
    "\n",
    "#                 print(training_data)\n",
    "\n",
    "#     with open(output_file, 'wb') as fp:\n",
    "#         pickle.dump(training_data, fp)\n",
    "    return training_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = convert_json_to_spacy_format(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:53<00:00,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "###### Spacy pretraining language model\n",
    "import glob\n",
    "import jsonlines \n",
    "from tqdm import tqdm\n",
    "\n",
    "textfiles_path = '/nfs/misc/literature/yangx/resources/xmls/articles/fulltext_token/'\n",
    "text_files = sorted(glob.glob(textfiles_path + '*.xml*'))\n",
    "result_dump = '/nfs/gns/literature/Santosh_Tirunagari/pretrained_word_embeddings/Data/'\n",
    "\n",
    "\n",
    "with jsonlines.open(result_dump+'europepmc.jsonl', mode='w') as writer:\n",
    "    for each_file in tqdm(text_files):\n",
    "        with open(each_file) as xml_file:\n",
    "            xml_text = xml_file.read()\n",
    "            for text in xml_text.split('\\n'):\n",
    "                writer.write({'text':text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Spacy pretraining language model\n",
    "import glob\n",
    "import jsonlines \n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "train_set = '/nfs/gns/literature/machine-learning/evaluation/300articles/CSV formats/train.csv'\n",
    "dev_set = '/nfs/gns/literature/machine-learning/evaluation/300articles/CSV formats/dev.csv'\n",
    "result_dump = '/nfs/gns/literature/Santosh_Tirunagari/pretrained_word_embeddings/Data/'\n",
    "\n",
    "train_df = pd.read_csv(train_set, sep = '\\t', names = ['pmcid', 'sentence','ner'])\n",
    "dev_df = pd.read_csv(dev_set, sep = '\\t', names = ['pmcid', 'sentence','ner'])\n",
    "\n",
    "text_train_dev = []\n",
    "text_train_dev.extend(train_df['sentence'].values)\n",
    "text_train_dev.extend(dev_df['sentence'].values)\n",
    "text_train_dev\n",
    "\n",
    "with jsonlines.open(result_dump+'golddata_train_dev.jsonl', mode='w') as writer:\n",
    "    for text in text_train_dev:\n",
    "        if(len(text)>50):\n",
    "            writer.write({'text':text.strip()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy pretrain Data/golddata_train_dev.jsonl /nfs/gns/literature/Santosh_Tirunagari/pretrained_word_embeddings/models/pubmed-pmc models/gold_pretrained --use-vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scispacy\n",
    "import spacy\n",
    "# Setting up the pipeline and entity recognizer.\n",
    "# model = 'en_core_sci_md'\n",
    "model = 'en_core_sci_md'\n",
    "\n",
    "if model is not None:\n",
    "    nlp = spacy.load(model)  # load existing spacy model\n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "else:\n",
    "    nlp = spacy.blank('en')  # create blank Language class\n",
    "    print(\"Created blank 'en' model\")\n",
    "\n",
    "    \n",
    "# nlp = spacy.blank(\"en\")\n",
    "# textcat = nlp.create_pipe(\"textcat\", config={\"architecture\": \"simple_cnn\", \"exclusive_classes\": True}))\n",
    "# textcat.add_label(\"LABEL1\")\n",
    "# textcat.add_label(\"LABEL2\")\n",
    "# # Alternatively, instead of adding all your labels explicitly, you could pass all your examples\n",
    "# # into textcat.begin_training, like this: textcat.begin_training(get_gold_tuples=lambda: my_data)\n",
    "# # It's fine to add the labels and not pass in the data, though. The nlp.begin_training() method will\n",
    "# # work the same as well, if you have other components in your pipeline you want to train.\n",
    "# optimizer = textcat.begin_training()\n",
    "# # Now that we have our model, we can load in the pretrained weights.\n",
    "# with open(path_to_pretrained_weights, \"rb\") as file_:\n",
    "#     textcat.model.tok2vec.from_bytes(file_.read())\n",
    "# # Now we can proceed with training\n",
    "# for epoch in range(nr_epoch):\n",
    "#     random.shuffle(train_data)\n",
    "#     for batch in minibatch(train_data, size=batch_size):\n",
    "#         X, y = zip(*batch)\n",
    "#         textcat.update(X, y, sgd=optimizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is None:\n",
    "    optimizer = nlp.begin_training()\n",
    "else:\n",
    "    optimizer = nlp.entity.create_optimizer()   \n",
    "    \n",
    "optimizer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "    \n",
    "    \n",
    "def train_spacy(data,nlp,iterations,optimizer):\n",
    "    TRAIN_DATA = data\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner') # nlp.add_pipe(ner, last=True)\n",
    "        nlp.add_pipe(ner)\n",
    "        print('new pipeline added')\n",
    "    else:\n",
    "        print('model\\'s ner added')\n",
    "        ner = nlp.get_pipe('ner')\n",
    "    \n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "         for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "    print('labels have been added')\n",
    "    \n",
    "    # Get names of other pipes to disable them during training to train only NER\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    print('Training Started')\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        for itn in range(iterations):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4., 32., 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.35,\n",
    "                           losses=losses)\n",
    "\n",
    "            now = datetime.now()\n",
    "            dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "            print(\"date and time =\", dt_string)\n",
    "            print('Losses', losses)\n",
    "            print('iteration-- '+str(itn))\n",
    "            print('\\n')\n",
    "\n",
    "        \n",
    "    return nlp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.create_pipe(\"ner\", config={\"architecture\": \"simple_cnn\", \"exclusive_classes\": True}))\n",
    "path_to_pretrained_weights = '/nfs/gns/literature/Santosh_Tirunagari/pretrained_word_embeddings/models/gold_pretrained/model14.bin'\n",
    "optimizer = ner.begin_training()\n",
    "# Now that we have our model, we can load in the pretrained weights.\n",
    "with open(path_to_pretrained_weights, \"rb\") as file_:\n",
    "    ner.model.tok2vec.from_bytes(file_.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "europe_pmc_sci_md = train_spacy(train_data,nlp,300,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model \n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = '/nfs/gns/literature/Santosh_Tirunagari/GitHub/spacy_models/iter_300/'\n",
    "new_model_name = 'en_europepmc_sci_md'\n",
    "\n",
    "if output_dir is not None:\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    nlp.meta['name'] = new_model_name  # rename model\n",
    "    nlp.to_disk(output_dir)\n",
    "    print(\"Saved model to\", output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the saved model\n",
    "test_text = 'For the first time, a multi-step selection protocol, consisting in both analytical and biochemical tests, has been proposed in order to identify mold candidates to be tested as autochthonous co-starter (together with already selected yeasts and lactic acid bacteria) for black table olive production. '\n",
    "\n",
    "\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp2 = spacy.load(output_dir)\n",
    "doc2 = nlp2(test_text)\n",
    "for ent in doc2.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = 'Isolates recovered from DRBC agar medium were sequenced firstly in ITS region and only fungal spp. isolates were further sequenced in beta-tubulin gene'\n",
    "doc2 = nlp(test_text)\n",
    "for ent in doc2.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(model)  # load existing spacy model\n",
    "print(\"Loaded model '%s'\" % model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(test_text)\n",
    "for ent in doc2.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize, WordPunctTokenizer\n",
    "from typing import List\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get full sentences from the XML\n",
    "root_path = '/mnt/droplet/nfs/gns/literature/machine-learning/'\n",
    "article_text_path = root_path+'evaluation/time_complexity/articles/'\n",
    "path_to_PMCids = root_path+'benchmarking/benchmark_sent/'\n",
    "\n",
    "############## The below function will extract text from XML files.\n",
    "\n",
    "# evaluation_epmc_lib.extract_sentences_from_sentencised_xml_to_text_files(article_text_path, path_to_PMCids)\n",
    "\n",
    "all_files = sorted(glob.glob(article_text_path + '*.txt*'))\n",
    "result_json_dump_path = root_path+'evaluation/time_complexity/en-pubmed-pmc-lg/'\n",
    "offset =26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scispcacy",
   "language": "python",
   "name": "scispcacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
