{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gensim.models.keyedvectors as word2vec \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve,  roc_auc_score, classification_report\n",
    "\n",
    "\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the current working directory\n",
    "data_path = os.path.abspath(os.path.join(os.path.dirname( '__file__' ), '..', 'Datasets'))+'/'\n",
    "\n",
    "# Although the dataset says csv, it is tab delimited. In addition to this, they have severe codels problems. \n",
    "# So best to parse throught codes first. \n",
    "# UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 2: invalid start byte\n",
    "\n",
    "#open for reading with \"universal\" type set\n",
    "\n",
    "import codecs\n",
    "\n",
    "doc_d_t = codecs.open(data_path+'EUADR_Corpus_IBIgroup/'+'EUADR_drug_target'+'.csv','rU','UTF-8') \n",
    "EUADR_drug_target = pd.read_csv(doc_d_t, sep='\\t', na_filter = False)\n",
    "EUADR_drug_target['CLASS'] = 'drug_gene'\n",
    "\n",
    "doc_t_d = codecs.open(data_path+'EUADR_Corpus_IBIgroup/'+'EUADR_target_disease'+'.csv','rU','UTF-8',errors='ignore') \n",
    "EUADR_target_disease = pd.read_csv(doc_t_d, sep='\\t', na_filter = False)\n",
    "EUADR_target_disease['CLASS'] = 'gene_disease'\n",
    "       \n",
    "doc_d_d = codecs.open(data_path+'EUADR_Corpus_IBIgroup/'+'EUADR_drug_disease'+'.csv','rU','UTF-8')                       \n",
    "EUADR_drug_disease = pd.read_csv(doc_d_d, sep='\\t', na_filter = False)\n",
    "EUADR_drug_disease['CLASS'] = 'drug_disease'\n",
    "\n",
    "\n",
    "EUADR_temp = EUADR_drug_target.append(EUADR_target_disease).append(EUADR_drug_disease)\n",
    "\n",
    "EUADR_temp_1 = EUADR_temp[EUADR_temp['ASSOCIATION_TYPE'] == 'PA']\n",
    "EUADR_temp_2 = EUADR_temp[EUADR_temp['ASSOCIATION_TYPE'] == 'NA']\n",
    "EUADR_temp = EUADR_temp_1.append(EUADR_temp_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GAD datasset\n",
    "\n",
    "doc_t_d = codecs.open(data_path+'GAD_Corpus_IBIgroup/'+'GAD_Y_N'+'.csv','rU','UTF-8',errors='ignore') \n",
    "GAD_target_disease_Y_N = pd.read_csv(doc_t_d, sep='\\t', na_filter = False)\n",
    "GAD_target_disease_Y_N['CLASS'] = 'gene_disease'\n",
    "\n",
    "doc_t_d = codecs.open(data_path+'GAD_Corpus_IBIgroup/'+'GAD_F'+'.csv','rU','UTF-8',errors='ignore') \n",
    "GAD_target_disease_F = pd.read_csv(doc_t_d, sep='\\t', na_filter = False)\n",
    "GAD_target_disease_F['CLASS'] = 'gene_disease'\n",
    "\n",
    "GAD_temp = GAD_target_disease_Y_N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PA    2366\n",
      "NA     994\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get sentences and their associations\n",
    "\n",
    "sentences = EUADR_temp['SENTENCE'].append(GAD_temp['GAD_CONCLUSION'])\n",
    "labels = EUADR_temp['ASSOCIATION_TYPE'].append(GAD_temp['GAD_ASSOC'].apply(lambda x: x.replace('Y', 'PA').replace('N', 'NA').replace('F', 'FA')))\n",
    "\n",
    "labels_count = labels.value_counts()\n",
    "labels_count.plot(kind=\"bar\")\n",
    "print(labels.value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from keras.utils import np_utils\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "le.fit(labels)\n",
    "le.classes_\n",
    "# le.inverse_transform()\n",
    "\n",
    "y = le.transform(labels)\n",
    "dummy_y = np_utils.to_categorical(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['inactivation', 'of', 'egfr', 'kinase', 'by', 'gefitinib', 'was', 'analyzed', 'by', 'western', 'blot', 'analysis', 'and', 'immunofluorescence']\n"
     ]
    }
   ],
   "source": [
    "def format_sentences(a_list_of_sentences):\n",
    "    \n",
    "    tkr = RegexpTokenizer('[a-zA-Z@]+')\n",
    "    sentences_split = []\n",
    "\n",
    "    for i, line in enumerate(a_list_of_sentences):\n",
    "        #print(line)\n",
    "        sent = str(line).lower().split()\n",
    "        sent = tkr.tokenize(str(sent))\n",
    "        sentences_split.append(sent)\n",
    "        \n",
    "    return sentences_split\n",
    "\n",
    "sentences_split = format_sentences(sentences)\n",
    "print(sentences_split[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stirunag/anaconda3/envs/KerasCPU/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "/home/stirunag/anaconda3/envs/KerasCPU/lib/python3.7/site-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  from ipykernel import kernelapp as app\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0723 12:38:33.681522 139955076097792 deprecation_wrapper.py:119] From /home/stirunag/anaconda3/envs/KerasCPU/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3360, 10)\n"
     ]
    }
   ],
   "source": [
    "w2vModel = word2vec.KeyedVectors.load_word2vec_format('/home/stirunag/pre-trained_word_embeddings/PubMed-and-PMC-w2v.bin', binary=True, limit=1000000)\n",
    "\n",
    "#Convert words to integers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences_split)\n",
    "X = tokenizer.texts_to_sequences(sentences_split)\n",
    "\n",
    "#lenght of sentence to consider\n",
    "maxlenth = 10\n",
    "#add padding\n",
    "X = pad_sequences(X, maxlen=maxlenth)\n",
    "print(X.shape)\n",
    "\n",
    "#create a embedding layer using PMC vectors (100000 words)\n",
    "embedding_layer = Embedding(input_dim=w2vModel.wv.vectors.shape[0], output_dim=w2vModel.wv.vectors.shape[1], weights=[w2vModel.wv.vectors], \n",
    "                            input_length=X.shape[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10, 200)           200000000 \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 80)                89920     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 162       \n",
      "=================================================================\n",
      "Total params: 200,090,082\n",
      "Trainable params: 200,090,082\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#create model\n",
    "\n",
    "lstm_out = 80\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(units=lstm_out))\n",
    "# model.add(Dense(1, activation='softmax'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "3024/3024 [==============================] - 6s 2ms/step - loss: 0.0685 - acc: 0.9762\n",
      "Epoch 2/25\n",
      "3024/3024 [==============================] - 6s 2ms/step - loss: 0.0624 - acc: 0.9729\n",
      "Epoch 3/25\n",
      "3024/3024 [==============================] - 6s 2ms/step - loss: 0.0578 - acc: 0.9765\n",
      "Epoch 4/25\n",
      "3024/3024 [==============================] - 6s 2ms/step - loss: 0.0506 - acc: 0.9805\n",
      "Epoch 5/25\n",
      "3024/3024 [==============================] - 6s 2ms/step - loss: 0.0500 - acc: 0.9792\n",
      "Epoch 6/25\n",
      "3024/3024 [==============================] - 6s 2ms/step - loss: 0.0450 - acc: 0.9808\n",
      "Epoch 7/25\n",
      "3024/3024 [==============================] - 6s 2ms/step - loss: 0.0412 - acc: 0.9821\n",
      "Epoch 8/25\n",
      "3024/3024 [==============================] - 6s 2ms/step - loss: 0.0397 - acc: 0.9821\n",
      "Epoch 9/25\n",
      "3024/3024 [==============================] - 6s 2ms/step - loss: 0.0400 - acc: 0.9798\n",
      "Epoch 10/25\n",
      "3024/3024 [==============================] - 6s 2ms/step - loss: 0.0366 - acc: 0.9812\n",
      "Epoch 11/25\n",
      "3024/3024 [==============================] - 6s 2ms/step - loss: 0.0378 - acc: 0.9815\n",
      "Epoch 12/25\n",
      "3024/3024 [==============================] - 6s 2ms/step - loss: 0.0357 - acc: 0.9838\n",
      "Epoch 13/25\n",
      "3024/3024 [==============================] - 6s 2ms/step - loss: 0.0329 - acc: 0.9835\n",
      "Epoch 14/25\n",
      "3024/3024 [==============================] - 6s 2ms/step - loss: 0.0346 - acc: 0.9815\n",
      "Epoch 15/25\n",
      "3024/3024 [==============================] - 6s 2ms/step - loss: 0.0334 - acc: 0.9821\n",
      "Epoch 16/25\n",
      "3024/3024 [==============================] - 6s 2ms/step - loss: 0.0329 - acc: 0.9841\n",
      "Epoch 17/25\n",
      "3024/3024 [==============================] - 6s 2ms/step - loss: 0.0332 - acc: 0.9831\n",
      "Epoch 18/25\n",
      "3024/3024 [==============================] - 6s 2ms/step - loss: 0.0298 - acc: 0.9848\n",
      "Epoch 19/25\n",
      "3024/3024 [==============================] - 6s 2ms/step - loss: 0.0305 - acc: 0.9825\n",
      "Epoch 20/25\n",
      "3024/3024 [==============================] - 6s 2ms/step - loss: 0.0293 - acc: 0.9841\n",
      "Epoch 21/25\n",
      "3024/3024 [==============================] - 6s 2ms/step - loss: 0.0306 - acc: 0.9818\n",
      "Epoch 22/25\n",
      "3024/3024 [==============================] - 6s 2ms/step - loss: 0.0285 - acc: 0.9828\n",
      "Epoch 23/25\n",
      "3024/3024 [==============================] - 6s 2ms/step - loss: 0.0279 - acc: 0.9828\n",
      "Epoch 24/25\n",
      "3024/3024 [==============================] - 6s 2ms/step - loss: 0.0292 - acc: 0.9841\n",
      "Epoch 25/25\n",
      "3024/3024 [==============================] - 6s 2ms/step - loss: 0.0301 - acc: 0.9831\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split dataset\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size= 0.1, random_state = 24, stratify=y)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, dummy_y, test_size= 0.1, random_state = 24, stratify=y)\n",
    "\n",
    "#fit model\n",
    "batch_size = 1024\n",
    "model.fit(X_train, Y_train, epochs=25, verbose=1, batch_size=batch_size)\n",
    "\n",
    "#analyze the results\n",
    "score, acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size=batch_size)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8697869331006051"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "f1_score(Y_test.argmax(axis=1), y_pred.argmax(axis=1), average='weighted')  \n",
    "# f1_score(Y_test, y_pred, average='micro')  \n",
    "\n",
    "# f1_score(Y_test, y_pred.round(), average='macro')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NA': 0.9999264478683472, 'PA': 7.356299465755001e-05}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let us test some  samples\n",
    "# test_samples = [\"The human T1663A GH1 gene polymorphism, which may confer lower levels of GH and IGF-I, appears to be associated with a decreased risk of colorectal cancer.\"]\n",
    "# test_samples = [\"Individuals who bear GSTT1 0/0 genotype or GSTT1 0/0-GSTM1 0/0 combined genotypes are more susceptible to leukemia, especially for male and younger carriers.\"]\n",
    "test_samples = [\"We found no evidence that mutation in GUCA1B,GNGT1,or RGS9 gene is a cause of retinitis pigmentosa.\"]\n",
    "# test_samples = ['These results suggest that HLA class I antigens and TNF-alpha A-308G are not associated with susceptibility or resistance to the development of TDI-induced asthma.']\n",
    "# test_samples = ['The results suggest that the studied dinucleotide repeat polymorphism of the ER alpha gene may contribute to specific components of personality.']\n",
    "\n",
    "test_sample_split = format_sentences(test_samples)\n",
    "test_ = tokenizer.texts_to_sequences(test_sample_split)\n",
    "test_pad = pad_sequences(test_, maxlen=maxlenth)\n",
    "#predict\n",
    "# model.predict(x=test_pad)\n",
    "\n",
    "dic = dict(zip(le.classes_, model.predict(x=test_pad).tolist()[0]))\n",
    "\n",
    "dic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
