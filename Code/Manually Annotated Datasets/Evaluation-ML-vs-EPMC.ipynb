{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to evaluate EPMC annotation to the Manual Annotation\n",
    "# Code to compare results from ML methods and EPMC annotation\n",
    "\n",
    "# (c) EMBL-EBI, September 2019\n",
    "#\n",
    "# Started: 23 Septmember  2019\n",
    "# Updated: 24 Septmember  2019\n",
    "\n",
    "_author_ = 'Santosh Tirunagari'\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "import requests\n",
    "# from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "import time\n",
    "from requests.compat import urljoin\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train, test and dev pmc ids from the random seed set to 2222\n",
    "import math\n",
    "import random\n",
    "file = '/nfs/gns/literature/Santosh_Tirunagari/EBI standard Dataset/NER/list_pmc_ids.csv'\n",
    "percentage=0.70\n",
    "iter = 0\n",
    "\n",
    "trainPMCids = []\n",
    "devPMCids = []\n",
    "testPMCids =[]\n",
    "try:\n",
    "    with open(file, 'r',encoding=\"utf-8\") as fin:\n",
    "        allPMCids = fin.readlines()\n",
    "except:\n",
    "    with open('/mnt/droplet'+file, 'r',encoding=\"utf-8\") as fin:\n",
    "        allPMCids = fin.readlines()    \n",
    "    \n",
    "nLines = sum(1 for line in allPMCids)\n",
    "nTrain = int(nLines*percentage) \n",
    "nValid = math.floor((nLines - nTrain)/2)\n",
    "nTest = nLines - (nTrain+nValid)\n",
    "\n",
    "deck = list(range(0, nLines))\n",
    "random.seed(2222) # Please dont change the seed for the reproducibility \n",
    "random.shuffle(deck)\n",
    "\n",
    "train_ids = deck[0:nTrain]\n",
    "devel_ids = deck[nTrain:nTrain+nValid]\n",
    "test_ids = deck[nTrain+nValid:nTrain+nValid+nTest]\n",
    "\n",
    "for each_pmc_id in allPMCids:\n",
    "    if iter in train_ids:\n",
    "        trainPMCids.append(each_pmc_id.strip())\n",
    "    elif iter in devel_ids:\n",
    "        devPMCids.append(each_pmc_id.strip())\n",
    "    else:\n",
    "        testPMCids.append(each_pmc_id.strip())\n",
    "\n",
    "    iter = iter+1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Json_through_PMCID(pmcid):\n",
    "    \n",
    "    base_url = \"https://www.ebi.ac.uk/europepmc/annotations_api/\"\n",
    "    article_url = urljoin(base_url, \"annotationsByArticleIds?articleIds=PMC%3A\"+pmcid+\"&provider=Europe%20PMC&format=JSON\")\n",
    "    r = requests.get(article_url)\n",
    "    \n",
    "    if r.status_code == 200:\n",
    "        return r\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC3174205\n",
      "PMC4552872\n",
      "PMC5972578\n",
      "PMC5006041\n",
      "PMC4978644\n",
      "PMC5891899\n",
      "PMC5120353\n",
      "PMC3291930\n",
      "PMC4768280\n",
      "PMC4302291\n",
      "PMC3949526\n",
      "PMC5731848\n",
      "PMC4244103\n",
      "PMC3611597\n",
      "PMC4556948\n",
      "PMC5962829\n",
      "PMC3024232\n",
      "PMC3938772\n",
      "PMC2474741\n",
      "PMC3260253\n",
      "PMC4973533\n",
      "PMC6008929\n",
      "PMC2478677\n",
      "PMC4791522\n",
      "PMC4753424\n",
      "PMC4697806\n",
      "PMC3029330\n",
      "PMC5078810\n"
     ]
    }
   ],
   "source": [
    "# result_path = '/mnt/droplet/nfs/gns/literature/Santosh_Tirunagari/EPMC Annotations Dataset/'\n",
    "result_path = '/nfs/gns/literature/Santosh_Tirunagari/EPMC Annotations Dataset/'\n",
    "\n",
    "with open(result_path+'EPMC_annotations_.csv','w',  newline='\\n') as f1:\n",
    "    test_writer=csv.writer(f1, delimiter='\\t',lineterminator='\\n')\n",
    "    \n",
    "    for each_test_pmc_id in testPMCids:\n",
    "        ss = get_Json_through_PMCID(each_test_pmc_id[3:]) # Just the number is needed. SO remove the PMC from the front\n",
    "        if ss:\n",
    "            json_results = ss.json()\n",
    "            pmc_id = json_results[0]['pmcid']\n",
    "            print(pmc_id)\n",
    "            for each_annotation in json_results[0]['annotations']:\n",
    "                exact = each_annotation['prefix']+ each_annotation['exact']+each_annotation['postfix']\n",
    "                token = each_annotation['tags'][0]['name']\n",
    "                ner = each_annotation['type']\n",
    "\n",
    "                row = [pmc_id, exact, token, ner]\n",
    "                test_writer.writerow(row)\n",
    "        else:\n",
    "            continue    \n",
    "            \n",
    "# PMC4556948\n",
    "# PMC3651197\n",
    "# PMC5502978\n",
    "# PMC5259676\n",
    "# PMC3613406\n",
    "# PMC3972685\n",
    "# PMC4618948\n",
    "# PMC1762380\n",
    "# PMC4540425\n",
    "# PMC3960246\n",
    "# PMC5770482\n",
    "# PMC4766309\n",
    "# PMC3031208\n",
    "# PMC3316545\n",
    "# PMC5510223\n",
    "# PMC2727484\n",
    "# PMC3174205\n",
    "# PMC2761781\n",
    "# PMC5666160\n",
    "# PMC6037156\n",
    "# PMC1971115\n",
    "# PMC4352028\n",
    "# PMC3844564\n",
    "# PMC4244103\n",
    "# PMC4753424\n",
    "# PMC4697806\n",
    "# PMC3029330\n",
    "# PMC5078810"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_annot_csv = pd.read_csv('/nfs/gns/literature/Santosh_Tirunagari/EBI standard Dataset/CSV/manual_annot_exacts_180.csv', names=['pmc_id', 'sent_id', 'sentence','ner','relation'], sep ='\\t')\n",
    "manual_annot_csv = manual_annot_csv[manual_annot_csv['ner']!= 'No-Ner']\n",
    "\n",
    "EPMC_annot_csv = pd.read_csv(result_path+'EPMC_annotations_.csv', names=['pmc_id', 'sentence','token','ner'], sep ='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_tags(pmc_id, EPMC_annot_csv,manual_annot_csv):\n",
    "    all_europe_pm_sentences = EPMC_annot_csv[EPMC_annot_csv['pmc_id'] == pmc_id]['sentence'].tolist()\n",
    "    manual_annotated_sentences = manual_annot_csv[manual_annot_csv['pmc_id'] == pmc_id]['sentence'].tolist()\n",
    "    \n",
    "    full_sentences = []\n",
    "    for each_sentence in tqdm(all_europe_pm_sentences):\n",
    "        try:\n",
    "            res = [x for x in manual_annotated_sentences if re.search(re.escape(each_sentence), x)]\n",
    "            full_sentences.append(res[0])\n",
    "        except:\n",
    "            full_sentences.append('None')\n",
    "    \n",
    "    new_epmc = EPMC_annot_csv[EPMC_annot_csv['pmc_id'] == pmc_id]\n",
    "    new_epmc = new_epmc.assign(full_sentence = full_sentences)\n",
    "    new_epmc['combined'] = new_epmc.apply(lambda x: list([x['token'],x['ner']]),axis=1) \n",
    "    sent_tags = new_epmc.groupby('full_sentence')['combined'].apply(list).reset_index(name='tags')\n",
    "    \n",
    "    new_man_annot = manual_annot_csv[manual_annot_csv['pmc_id'] == pmc_id]\n",
    "    new_man_annot = manual_annot_csv[manual_annot_csv['pmc_id'] == pmc_id].reset_index()\n",
    "    new_man_annot.rename(columns={'sentence':'full_sentence'}, inplace=True)\n",
    "    epmc_sentence_tags = pd.merge(new_man_annot, sent_tags, on='full_sentence', how='left') \n",
    "\n",
    "    \n",
    "    return epmc_sentence_tags\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "def find_sub_list(sl,l):\n",
    "    results=[]\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            results.append((ind,ind+sll))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def convert2IOB_dict_manual_annotations(text_data,ner_tags):\n",
    "    \n",
    "    tokens = []\n",
    "    ners = []\n",
    "    \n",
    "    split_text = wordpunct_tokenize(text_data)\n",
    "    # for each word token append 'O'\n",
    "    arr = ['O']*len(split_text) \n",
    "    \n",
    "    if ner_tags != 'No-Ner':\n",
    "        ner_tags = literal_eval(ner_tags)\n",
    "        for each_tag in ner_tags:\n",
    "            token_list = wordpunct_tokenize(each_tag[2])\n",
    "            ner_list = wordpunct_tokenize(each_tag[3])\n",
    "\n",
    "            if(len(token_list) > len(ner_list)):\n",
    "                ner_list = len(token_list) * ner_list\n",
    "\n",
    "            for i in range(0,len(ner_list)):\n",
    "                # The logic here is look for the first B-tag and then append I-tag next\n",
    "                if(i==0):\n",
    "                    ner_list[i] = 'B-'+ner_list[i]\n",
    "                else:\n",
    "                    ner_list[i] = 'I-'+ner_list[i]\n",
    "\n",
    "            tokens.append(token_list)\n",
    "            ners.append(ner_list)\n",
    "\n",
    "        for i in range(0, len(tokens)):\n",
    "            spans = find_sub_list(tokens[i], split_text)\n",
    "            for each_span in spans:\n",
    "                arr[each_span[0]:each_span[1]] = ners[i]\n",
    "    \n",
    "        return zip(split_text, arr)\n",
    "\n",
    "    else:\n",
    "        return zip(split_text, arr)\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "def convert2IOB_dict_EPMC_annotations(text_data,ner_tags):\n",
    "    \n",
    "    tokens = []\n",
    "    ners = []\n",
    "    \n",
    "    split_text = wordpunct_tokenize(text_data)\n",
    "    # for each word token append 'O'\n",
    "    arr = ['O']*len(split_text)\n",
    "        \n",
    "\n",
    "    if ner_tags != 'No-Ner':\n",
    "        for each_tag in ner_tags:\n",
    "            token_list = wordpunct_tokenize(each_tag[0])\n",
    "            ner_list = wordpunct_tokenize(each_tag[1])\n",
    "\n",
    "            if(len(token_list) > len(ner_list)):\n",
    "                ner_list = len(token_list) * ner_list\n",
    "\n",
    "            for i in range(0,len(ner_list)):\n",
    "                # The logic here is look for the first B-tag and then append I-tag next\n",
    "                if(i==0):\n",
    "                    ner_list[i] = 'B-'+ner_list[i]\n",
    "                else:\n",
    "                    ner_list[i] = 'I-'+ner_list[i]\n",
    "\n",
    "            tokens.append(token_list)\n",
    "            ners.append(ner_list)\n",
    "\n",
    "        for i in range(0, len(tokens)):\n",
    "            spans = find_sub_list(tokens[i], split_text)\n",
    "            for each_span in spans:\n",
    "                arr[each_span[0]:each_span[1]] = ners[i]\n",
    "\n",
    "        return zip(split_text, arr)\n",
    "    else:\n",
    "        return zip(split_text, arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 109/390 [00:00<00:00, 1081.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC3174205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 390/390 [00:00<00:00, 1118.37it/s]\n",
      "100%|██████████| 130/130 [00:00<00:00, 2474.89it/s]\n",
      "100%|██████████| 58/58 [00:00<00:00, 1862.34it/s]\n",
      "100%|██████████| 68/68 [00:00<00:00, 2785.09it/s]\n",
      "  0%|          | 0/215 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC4552872\n",
      "PMC5972578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215/215 [00:00<00:00, 2101.08it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 1782.45it/s]\n",
      "100%|██████████| 217/217 [00:00<00:00, 1464.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC5006041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:00<00:00, 2596.30it/s]\n",
      "100%|██████████| 188/188 [00:00<00:00, 2001.41it/s]\n",
      "100%|██████████| 47/47 [00:00<00:00, 1864.95it/s]\n",
      "  0%|          | 0/606 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC4978644\n",
      "PMC5891899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 606/606 [00:00<00:00, 982.91it/s]\n",
      "100%|██████████| 141/141 [00:00<00:00, 2189.65it/s]\n",
      "  8%|▊         | 65/843 [00:00<00:01, 644.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC5120353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 843/843 [00:01<00:00, 660.98it/s]\n",
      "100%|██████████| 241/241 [00:00<00:00, 2331.51it/s]\n",
      "100%|██████████| 52/52 [00:00<00:00, 234772.67it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 83/83 [00:00<00:00, 1096.86it/s]\n",
      "100%|██████████| 81/81 [00:00<00:00, 2517.07it/s]\n",
      "  0%|          | 0/435 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC3291930\n",
      "PMC4768280\n",
      "PMC4302291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 435/435 [00:00<00:00, 1963.19it/s]\n",
      "100%|██████████| 56/56 [00:00<00:00, 2513.44it/s]\n",
      "100%|██████████| 196/196 [00:00<00:00, 2074.33it/s]\n",
      "100%|██████████| 38/38 [00:00<00:00, 1668.90it/s]\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC3949526\n",
      "PMC5731848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 794.44it/s]\n",
      "100%|██████████| 184/184 [00:00<00:00, 1691.93it/s]\n",
      "100%|██████████| 520/520 [00:00<00:00, 4961.25it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 1888.48it/s]\n",
      "  0%|          | 0/441 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC4244103\n",
      "PMC3611597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 441/441 [00:00<00:00, 1011.82it/s]\n",
      "100%|██████████| 136/136 [00:00<00:00, 2051.37it/s]\n",
      " 13%|█▎        | 58/441 [00:00<00:00, 571.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC4556948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 441/441 [00:00<00:00, 632.41it/s]\n",
      "100%|██████████| 266/266 [00:00<00:00, 2906.01it/s]\n",
      "100%|██████████| 152/152 [00:00<00:00, 1284.04it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 2715.88it/s]\n",
      "  0%|          | 0/431 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC5962829\n",
      "PMC3024232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 431/431 [00:00<00:00, 782.85it/s]\n",
      "100%|██████████| 211/211 [00:00<00:00, 2756.51it/s]\n",
      " 31%|███       | 83/270 [00:00<00:00, 829.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC3938772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:00<00:00, 867.06it/s]\n",
      "100%|██████████| 168/168 [00:00<00:00, 2710.83it/s]\n",
      "100%|██████████| 228/228 [00:00<00:00, 1550.19it/s]\n",
      "  0%|          | 0/73 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC2474741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:00<00:00, 1999.94it/s]\n",
      "100%|██████████| 198/198 [00:00<00:00, 1156.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC3260253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:00<00:00, 2351.44it/s]\n",
      "100%|██████████| 294/294 [00:00<00:00, 1753.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC4973533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:00<00:00, 1545.23it/s]\n",
      " 22%|██▏       | 90/414 [00:00<00:00, 891.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC6008929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414/414 [00:00<00:00, 876.12it/s]\n",
      "100%|██████████| 143/143 [00:00<00:00, 1476.45it/s]\n",
      " 29%|██▉       | 87/296 [00:00<00:00, 866.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC2478677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 296/296 [00:00<00:00, 1014.87it/s]\n",
      "100%|██████████| 134/134 [00:00<00:00, 2625.17it/s]\n",
      " 39%|███▉      | 133/342 [00:00<00:00, 1315.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC4791522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 342/342 [00:00<00:00, 1342.09it/s]\n",
      "100%|██████████| 93/93 [00:00<00:00, 2000.37it/s]\n",
      "100%|██████████| 176/176 [00:00<00:00, 1600.97it/s]\n",
      "100%|██████████| 91/91 [00:00<00:00, 2229.93it/s]\n",
      "  0%|          | 0/533 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC4753424\n",
      "PMC4697806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 533/533 [00:00<00:00, 753.80it/s]\n",
      "100%|██████████| 194/194 [00:00<00:00, 1721.58it/s]\n",
      " 13%|█▎        | 46/362 [00:00<00:00, 450.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC3029330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 362/362 [00:00<00:00, 704.40it/s]\n",
      "100%|██████████| 179/179 [00:00<00:00, 2630.78it/s]\n",
      "100%|██████████| 160/160 [00:00<00:00, 481067.13it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC5078810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "result_path = '/nfs/gns/literature/Santosh_Tirunagari/EPMC Annotations Dataset/'\n",
    "\n",
    "with open(result_path+'EPMC_annotated_test.csv','w',  newline='\\n') as f1, open(result_path+'manual_annotated_test.csv','w',  newline='\\n') as f2: \n",
    "    epmc_writer=csv.writer(f1, delimiter='\\t',lineterminator='\\n')\n",
    "    manual_writer=csv.writer(f2, delimiter='\\t',lineterminator='\\n')\n",
    "    \n",
    "    for each_pmc_id in testPMCids:\n",
    "        print(each_pmc_id)\n",
    "        ss_ = sentences_tags(each_pmc_id, EPMC_annot_csv,manual_annot_csv)\n",
    "        ss = ss_.where((pd.notnull(ss_)), 'No-Ner')\n",
    "        for index, row in tqdm(ss.iterrows(), total=ss.shape[0]):\n",
    "            tagged_tokens = convert2IOB_dict_EPMC_annotations(row['full_sentence'],row['tags'])\n",
    "            for each_word in tagged_tokens: # make it to manual annotation format for easy comparison\n",
    "                epmc_tokens = list(each_word)\n",
    "                if epmc_tokens[1] == 'B-Diseases':\n",
    "                    epmc_tokens[1] = 'B-DS'\n",
    "                elif epmc_tokens[1] == 'I-Diseases':\n",
    "                    epmc_tokens[1] = 'I-DS'\n",
    "                elif epmc_tokens[1] == 'B-Organisms':\n",
    "                    epmc_tokens[1] = 'B-OG'\n",
    "                elif epmc_tokens[1] == 'I-Organisms':\n",
    "                    epmc_tokens[1] = 'I-OG'  \n",
    "                elif epmc_tokens[1] == 'B-Gene_Proteins':\n",
    "                    epmc_tokens[1] = 'B-GP'   \n",
    "                elif epmc_tokens[1] == 'I-Gene_Proteins':\n",
    "                    epmc_tokens[1] = 'I-GP'  \n",
    "                else:\n",
    "                    epmc_tokens[1] = 'O'\n",
    "                epmc_writer.writerow(epmc_tokens)\n",
    "            epmc_writer.writerow('') \n",
    "            manual_tagged_tokens = convert2IOB_dict_manual_annotations(row['full_sentence'],row['ner'])\n",
    "            for each_word in manual_tagged_tokens:\n",
    "                manual_writer.writerow(list(each_word))\n",
    "            manual_writer.writerow('') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        B-DS       0.64      0.56      0.60      1154\n",
      "        I-DS       0.70      0.60      0.65       475\n",
      "        B-GP       0.85      0.64      0.73      3246\n",
      "        I-GP       0.91      0.37      0.52      2243\n",
      "        B-OG       0.91      0.76      0.83      2374\n",
      "        I-OG       0.95      0.77      0.85      1185\n",
      "\n",
      "   micro avg       0.85      0.61      0.71     10677\n",
      "   macro avg       0.82      0.62      0.69     10677\n",
      "weighted avg       0.85      0.61      0.70     10677\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "true_lbs = pd.read_csv(result_path+'manual_annotated_test.csv', sep='\\t', names=['tokens','tags'])\n",
    "pred_lbs = pd.read_csv(result_path+'EPMC_annotated_test.csv', sep='\\t', names=['tokens','tags'])\n",
    "\n",
    "y_true = true_lbs['tags'].values\n",
    "y_pred =  pred_lbs['tags'].values\n",
    "\n",
    "\n",
    "class_labels = sorted([tag for tag in set(y_true) if tag != 'O'], key=lambda name: (name[1:], name[0]))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred, labels = class_labels))\n",
    "\n",
    "\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#         B-DS       0.74      0.53      0.62      1323\n",
    "#         I-DS       0.84      0.70      0.76       486\n",
    "#         B-GP       0.76      0.52      0.62      3056\n",
    "#         I-GP       0.89      0.41      0.56      1989\n",
    "#         B-OG       0.93      0.69      0.79      1960\n",
    "#         I-OG       0.98      0.51      0.68       968\n",
    "\n",
    "#    micro avg       0.83      0.54      0.66      9782\n",
    "#    macro avg       0.85      0.56      0.67      9782\n",
    "# weighted avg       0.84      0.54      0.65      9782"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
