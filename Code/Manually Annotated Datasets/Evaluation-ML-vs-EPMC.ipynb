{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to evaluate EPMC annotation to the Manual Annotation\n",
    "# Code to compare results from ML methods and EPMC annotation\n",
    "\n",
    "# (c) EMBL-EBI, September 2019\n",
    "#\n",
    "# Started: 23 Septmember  2019\n",
    "# Updated: 24 Septmember  2019\n",
    "\n",
    "_author_ = 'Santosh Tirunagari'\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "import requests\n",
    "# from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "import time\n",
    "from requests.compat import urljoin\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train, test and dev pmc ids from the random seed set to 2222\n",
    "import math\n",
    "import random\n",
    "file = '/nfs/gns/literature/Santosh_Tirunagari/EBI standard Dataset/NER/list_pmc_ids.csv'\n",
    "percentage=0.70\n",
    "iter = 0\n",
    "\n",
    "trainPMCids = []\n",
    "devPMCids = []\n",
    "testPMCids =[]\n",
    "try:\n",
    "    with open(file, 'r',encoding=\"utf-8\") as fin:\n",
    "        allPMCids = fin.readlines()\n",
    "except:\n",
    "    with open('/mnt/droplet'+file, 'r',encoding=\"utf-8\") as fin:\n",
    "        allPMCids = fin.readlines()    \n",
    "    \n",
    "nLines = sum(1 for line in allPMCids)\n",
    "nTrain = int(nLines*percentage) \n",
    "nValid = math.floor((nLines - nTrain)/2)\n",
    "nTest = nLines - (nTrain+nValid)\n",
    "\n",
    "deck = list(range(0, nLines))\n",
    "random.seed(2222) # Please dont change the seed for the reproducibility \n",
    "random.shuffle(deck)\n",
    "\n",
    "train_ids = deck[0:nTrain]\n",
    "devel_ids = deck[nTrain:nTrain+nValid]\n",
    "test_ids = deck[nTrain+nValid:nTrain+nValid+nTest]\n",
    "\n",
    "for each_pmc_id in allPMCids:\n",
    "    if iter in train_ids:\n",
    "        trainPMCids.append(each_pmc_id.strip())\n",
    "    elif iter in devel_ids:\n",
    "        devPMCids.append(each_pmc_id.strip())\n",
    "    else:\n",
    "        testPMCids.append(each_pmc_id.strip())\n",
    "\n",
    "    iter = iter+1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Json_through_PMCID(pmcid):\n",
    "    \n",
    "    base_url = \"https://www.ebi.ac.uk/europepmc/annotations_api/\"\n",
    "    article_url = urljoin(base_url, \"annotationsByArticleIds?articleIds=PMC%3A\"+pmcid+\"&provider=Europe%20PMC&format=JSON\")\n",
    "    r = requests.get(article_url)\n",
    "    \n",
    "    if r.status_code == 200:\n",
    "        return r\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC4556948\n",
      "PMC3651197\n",
      "PMC5502978\n",
      "PMC5259676\n",
      "PMC3613406\n",
      "PMC3972685\n",
      "PMC4618948\n",
      "PMC1762380\n",
      "PMC4540425\n",
      "PMC3960246\n",
      "PMC5770482\n",
      "PMC4766309\n",
      "PMC3031208\n",
      "PMC3316545\n",
      "PMC5510223\n",
      "PMC2727484\n",
      "PMC3174205\n",
      "PMC2761781\n",
      "PMC5666160\n",
      "PMC6037156\n",
      "PMC1971115\n",
      "PMC4352028\n",
      "PMC3844564\n",
      "PMC4244103\n",
      "PMC4753424\n",
      "PMC4697806\n",
      "PMC3029330\n",
      "PMC5078810\n"
     ]
    }
   ],
   "source": [
    "# result_path = '/mnt/droplet/nfs/gns/literature/Santosh_Tirunagari/EPMC Annotations Dataset/'\n",
    "result_path = '/nfs/gns/literature/Santosh_Tirunagari/EPMC Annotations Dataset/'\n",
    "\n",
    "with open(result_path+'EPMC_annotations_.csv','w',  newline='\\n') as f1:\n",
    "    test_writer=csv.writer(f1, delimiter='\\t',lineterminator='\\n')\n",
    "    \n",
    "    for each_test_pmc_id in testPMCids:\n",
    "        ss = get_Json_through_PMCID(each_test_pmc_id[3:]) # Just the number is needed. SO remove the PMC from the front\n",
    "        if ss:\n",
    "            json_results = ss.json()\n",
    "            pmc_id = json_results[0]['pmcid']\n",
    "            print(pmc_id)\n",
    "            for each_annotation in json_results[0]['annotations']:\n",
    "                exact = each_annotation['prefix']+ each_annotation['exact']+each_annotation['postfix']\n",
    "                token = each_annotation['tags'][0]['name']\n",
    "                ner = each_annotation['type']\n",
    "\n",
    "                row = [pmc_id, exact, token, ner]\n",
    "                test_writer.writerow(row)\n",
    "        else:\n",
    "            continue    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_annot_csv = pd.read_csv('/nfs/gns/literature/Santosh_Tirunagari/EBI standard Dataset/CSV/manual_annot_exacts_180.csv', names=['pmc_id', 'sent_id', 'sentence','ner','relation'], sep ='\\t')\n",
    "manual_annot_csv = manual_annot_csv[manual_annot_csv['ner']!= 'No-Ner']\n",
    "\n",
    "EPMC_annot_csv = pd.read_csv(result_path+'EPMC_annotations_.csv', names=['pmc_id', 'sentence','token','ner'], sep ='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_tags(pmc_id, EPMC_annot_csv,manual_annot_csv):\n",
    "    all_europe_pm_sentences = EPMC_annot_csv[EPMC_annot_csv['pmc_id'] == pmc_id]['sentence'].tolist()\n",
    "    manual_annotated_sentences = manual_annot_csv[manual_annot_csv['pmc_id'] == pmc_id]['sentence'].tolist()\n",
    "    \n",
    "    full_sentences = []\n",
    "    for each_sentence in tqdm(all_europe_pm_sentences):\n",
    "        try:\n",
    "            res = [x for x in manual_annotated_sentences if re.search(re.escape(each_sentence), x)]\n",
    "            full_sentences.append(res[0])\n",
    "        except:\n",
    "            full_sentences.append('None')\n",
    "    \n",
    "    new_epmc = EPMC_annot_csv[EPMC_annot_csv['pmc_id'] == pmc_id]\n",
    "    new_epmc = new_epmc.assign(full_sentence = full_sentences)\n",
    "    new_epmc['combined'] = new_epmc.apply(lambda x: list([x['token'],x['ner']]),axis=1) \n",
    "    sent_tags = new_epmc.groupby('full_sentence')['combined'].apply(list).reset_index(name='tags')\n",
    "    \n",
    "    new_man_annot = manual_annot_csv[manual_annot_csv['pmc_id'] == pmc_id]\n",
    "    new_man_annot = manual_annot_csv[manual_annot_csv['pmc_id'] == pmc_id].reset_index()\n",
    "    new_man_annot.rename(columns={'sentence':'full_sentence'}, inplace=True)\n",
    "    epmc_sentence_tags = pd.merge(new_man_annot, sent_tags, on='full_sentence', how='left') \n",
    "\n",
    "    \n",
    "    return epmc_sentence_tags\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "def find_sub_list(sl,l):\n",
    "    results=[]\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            results.append((ind,ind+sll))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def convert2IOB_dict_manual_annotations(text_data,ner_tags):\n",
    "    \n",
    "    tokens = []\n",
    "    ners = []\n",
    "    \n",
    "    split_text = wordpunct_tokenize(text_data)\n",
    "    # for each word token append 'O'\n",
    "    arr = ['O']*len(split_text) \n",
    "    \n",
    "    if ner_tags != 'No-Ner':\n",
    "        ner_tags = literal_eval(ner_tags)\n",
    "        for each_tag in ner_tags:\n",
    "            token_list = wordpunct_tokenize(each_tag[2])\n",
    "            ner_list = wordpunct_tokenize(each_tag[3])\n",
    "\n",
    "            if(len(token_list) > len(ner_list)):\n",
    "                ner_list = len(token_list) * ner_list\n",
    "\n",
    "            for i in range(0,len(ner_list)):\n",
    "                # The logic here is look for the first B-tag and then append I-tag next\n",
    "                if(i==0):\n",
    "                    ner_list[i] = 'B-'+ner_list[i]\n",
    "                else:\n",
    "                    ner_list[i] = 'I-'+ner_list[i]\n",
    "\n",
    "            tokens.append(token_list)\n",
    "            ners.append(ner_list)\n",
    "\n",
    "        for i in range(0, len(tokens)):\n",
    "            spans = find_sub_list(tokens[i], split_text)\n",
    "            for each_span in spans:\n",
    "                arr[each_span[0]:each_span[1]] = ners[i]\n",
    "    \n",
    "        return zip(split_text, arr)\n",
    "\n",
    "    else:\n",
    "        return zip(split_text, arr)\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "def convert2IOB_dict_EPMC_annotations(text_data,ner_tags):\n",
    "    \n",
    "    tokens = []\n",
    "    ners = []\n",
    "    \n",
    "    split_text = wordpunct_tokenize(text_data)\n",
    "    # for each word token append 'O'\n",
    "    arr = ['O']*len(split_text)\n",
    "        \n",
    "\n",
    "    if ner_tags != 'No-Ner':\n",
    "        for each_tag in ner_tags:\n",
    "            token_list = wordpunct_tokenize(each_tag[0])\n",
    "            ner_list = wordpunct_tokenize(each_tag[1])\n",
    "\n",
    "            if(len(token_list) > len(ner_list)):\n",
    "                ner_list = len(token_list) * ner_list\n",
    "\n",
    "            for i in range(0,len(ner_list)):\n",
    "                # The logic here is look for the first B-tag and then append I-tag next\n",
    "                if(i==0):\n",
    "                    ner_list[i] = 'B-'+ner_list[i]\n",
    "                else:\n",
    "                    ner_list[i] = 'I-'+ner_list[i]\n",
    "\n",
    "            tokens.append(token_list)\n",
    "            ners.append(ner_list)\n",
    "\n",
    "        for i in range(0, len(tokens)):\n",
    "            spans = find_sub_list(tokens[i], split_text)\n",
    "            for each_span in spans:\n",
    "                arr[each_span[0]:each_span[1]] = ners[i]\n",
    "\n",
    "        return zip(split_text, arr)\n",
    "    else:\n",
    "        return zip(split_text, arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 204/441 [00:00<00:00, 922.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC4556948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 441/441 [00:00<00:00, 952.79it/s]\n",
      "100%|██████████| 266/266 [00:00<00:00, 4479.78it/s]\n",
      " 27%|██▋       | 158/578 [00:00<00:00, 1575.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC3651197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 578/578 [00:00<00:00, 1513.48it/s]\n",
      "100%|██████████| 141/141 [00:00<00:00, 3842.58it/s]\n",
      "100%|██████████| 345/345 [00:00<00:00, 3866.67it/s]\n",
      "100%|██████████| 38/38 [00:00<00:00, 3322.78it/s]\n",
      "100%|██████████| 113/113 [00:00<00:00, 2692.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC5502978\n",
      "PMC5259676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 3804.11it/s]\n",
      " 30%|███       | 122/400 [00:00<00:00, 1213.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC3613406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:00<00:00, 1219.31it/s]\n",
      "100%|██████████| 182/182 [00:00<00:00, 1392.80it/s]\n",
      "100%|██████████| 343/343 [00:00<00:00, 2091.52it/s]\n",
      "  0%|          | 0/96 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC3972685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 4024.04it/s]\n",
      "100%|██████████| 153/153 [00:00<00:00, 1677.30it/s]\n",
      "100%|██████████| 121/121 [00:00<00:00, 3474.58it/s]\n",
      "  0%|          | 0/318 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC4618948\n",
      "PMC1762380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 318/318 [00:00<00:00, 1739.34it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 3601.44it/s]\n",
      "100%|██████████| 203/203 [00:00<00:00, 1933.10it/s]\n",
      "100%|██████████| 110/110 [00:00<00:00, 4074.59it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 2662.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC4540425\n",
      "PMC3960246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 63/63 [00:00<00:00, 4105.42it/s]\n",
      "100%|██████████| 68/68 [00:00<00:00, 2879.69it/s]\n",
      "100%|██████████| 52/52 [00:00<00:00, 3059.09it/s]\n",
      "100%|██████████| 119/119 [00:00<00:00, 1366.71it/s]\n",
      "  0%|          | 0/143 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC5770482\n",
      "PMC4766309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [00:00<00:00, 3736.96it/s]\n",
      "100%|██████████| 83/83 [00:00<00:00, 3200.40it/s]\n",
      "100%|██████████| 51/51 [00:00<00:00, 3631.12it/s]\n",
      " 25%|██▌       | 122/483 [00:00<00:00, 1216.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC3031208\n",
      "PMC3316545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 483/483 [00:00<00:00, 1228.98it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 3556.29it/s]\n",
      "100%|██████████| 279/279 [00:00<00:00, 1493.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC5510223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:00<00:00, 3820.99it/s]\n",
      " 42%|████▏     | 161/387 [00:00<00:00, 1595.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC2727484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 387/387 [00:00<00:00, 1505.37it/s]\n",
      "100%|██████████| 156/156 [00:00<00:00, 4596.79it/s]\n",
      " 42%|████▏     | 163/390 [00:00<00:00, 1622.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC3174205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 390/390 [00:00<00:00, 1635.97it/s]\n",
      "100%|██████████| 130/130 [00:00<00:00, 3563.67it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 99189.62it/s]\n",
      "0it [00:00, ?it/s]\n",
      " 35%|███▌      | 155/438 [00:00<00:00, 1549.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC2761781\n",
      "PMC5666160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:00<00:00, 1522.69it/s]\n",
      "100%|██████████| 132/132 [00:00<00:00, 3831.50it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 1587.85it/s]\n",
      "100%|██████████| 127/127 [00:00<00:00, 3873.93it/s]\n",
      "  0%|          | 0/653 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC6037156\n",
      "PMC1971115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 653/653 [00:00<00:00, 843.89it/s]\n",
      "100%|██████████| 257/257 [00:00<00:00, 3615.94it/s]\n",
      "100%|██████████| 426/426 [00:00<00:00, 2168.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC4352028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:00<00:00, 3360.05it/s]\n",
      "100%|██████████| 254/254 [00:00<00:00, 2042.96it/s]\n",
      "100%|██████████| 146/146 [00:00<00:00, 5002.15it/s]\n",
      "  0%|          | 0/520 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC3844564\n",
      "PMC4244103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 520/520 [00:00<00:00, 7687.72it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 2811.82it/s]\n",
      "100%|██████████| 176/176 [00:00<00:00, 2392.66it/s]\n",
      "100%|██████████| 91/91 [00:00<00:00, 3716.00it/s]\n",
      "  0%|          | 0/533 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC4753424\n",
      "PMC4697806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 533/533 [00:00<00:00, 1139.37it/s]\n",
      "100%|██████████| 194/194 [00:00<00:00, 3651.05it/s]\n",
      " 37%|███▋      | 135/362 [00:00<00:00, 1345.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC3029330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 362/362 [00:00<00:00, 1277.92it/s]\n",
      "100%|██████████| 179/179 [00:00<00:00, 4420.31it/s]\n",
      "100%|██████████| 160/160 [00:00<00:00, 707899.41it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC5078810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "result_path = '/nfs/gns/literature/Santosh_Tirunagari/EPMC Annotations Dataset/'\n",
    "\n",
    "with open(result_path+'EPMC_annotated_test.csv','w',  newline='\\n') as f1, open(result_path+'manual_annotated_test.csv','w',  newline='\\n') as f2: \n",
    "    epmc_writer=csv.writer(f1, delimiter='\\t',lineterminator='\\n')\n",
    "    manual_writer=csv.writer(f2, delimiter='\\t',lineterminator='\\n')\n",
    "    \n",
    "    for each_pmc_id in testPMCids:\n",
    "        print(each_pmc_id)\n",
    "        ss_ = sentences_tags(each_pmc_id, EPMC_annot_csv,manual_annot_csv)\n",
    "        ss = ss_.where((pd.notnull(ss_)), 'No-Ner')\n",
    "        for index, row in tqdm(ss.iterrows(), total=ss.shape[0]):\n",
    "            tagged_tokens = convert2IOB_dict_EPMC_annotations(row['full_sentence'],row['tags'])\n",
    "            for each_word in tagged_tokens: # make it to manual annotation format for easy comparison\n",
    "                epmc_tokens = list(each_word)\n",
    "                if epmc_tokens[1] == 'B-Diseases':\n",
    "                    epmc_tokens[1] = 'B-DS'\n",
    "                elif epmc_tokens[1] == 'I-Diseases':\n",
    "                    epmc_tokens[1] = 'I-DS'\n",
    "                elif epmc_tokens[1] == 'B-Organisms':\n",
    "                    epmc_tokens[1] = 'B-OG'\n",
    "                elif epmc_tokens[1] == 'I-Organisms':\n",
    "                    epmc_tokens[1] = 'I-OG'  \n",
    "                elif epmc_tokens[1] == 'B-Gene_Proteins':\n",
    "                    epmc_tokens[1] = 'B-GP'   \n",
    "                elif epmc_tokens[1] == 'I-Gene_Proteins':\n",
    "                    epmc_tokens[1] = 'I-GP'  \n",
    "                else:\n",
    "                    epmc_tokens[1] = 'O'\n",
    "                epmc_writer.writerow(epmc_tokens)\n",
    "            epmc_writer.writerow('') \n",
    "            manual_tagged_tokens = convert2IOB_dict_manual_annotations(row['full_sentence'],row['ner'])\n",
    "            for each_word in manual_tagged_tokens:\n",
    "                manual_writer.writerow(list(each_word))\n",
    "            manual_writer.writerow('') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        B-DS       0.74      0.53      0.62      1323\n",
      "        I-DS       0.84      0.70      0.76       486\n",
      "        B-GP       0.76      0.52      0.62      3056\n",
      "        I-GP       0.89      0.41      0.56      1989\n",
      "        B-OG       0.93      0.69      0.79      1960\n",
      "        I-OG       0.98      0.51      0.68       968\n",
      "\n",
      "   micro avg       0.83      0.54      0.66      9782\n",
      "   macro avg       0.85      0.56      0.67      9782\n",
      "weighted avg       0.84      0.54      0.65      9782\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "true_lbs = pd.read_csv(result_path+'manual_annotated_test.csv', sep='\\t', names=['tokens','tags'])\n",
    "pred_lbs = pd.read_csv(result_path+'EPMC_annotated_test.csv', sep='\\t', names=['tokens','tags'])\n",
    "\n",
    "y_true = true_lbs['tags'].values\n",
    "y_pred =  pred_lbs['tags'].values\n",
    "\n",
    "\n",
    "class_labels = sorted([tag for tag in set(y_true) if tag != 'O'], key=lambda name: (name[1:], name[0]))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred, labels = class_labels))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
