{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to evaluate EPMC annotation to the Manual Annotation\n",
    "# Code to compare results from ML methods and EPMC annotation\n",
    "\n",
    "# (c) EMBL-EBI, September 2019\n",
    "#\n",
    "# Started: 23 Septmember  2019\n",
    "# Updated: 24 Septmember  2019\n",
    "\n",
    "_author_ = 'Santosh Tirunagari'\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "import requests\n",
    "# from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "import time\n",
    "from requests.compat import urljoin\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EBI_data_folder = '/nfs/gns/literature/Santosh_Tirunagari/EBI standard Dataset/NER/'\n",
    "test_df = pd.read_csv(EBI_data_folder+'test_text_format.csv', sep='\\t', names=['Sentences', 'true_ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-08 17:28:14,775 loading file /nfs/gns/literature/Santosh_Tirunagari/GitHub/flair_models/ner/multi_bio_ner_model_gene/v01/best-model.pt\n",
      "2019-10-08 17:28:20,508 loading file /nfs/gns/literature/Santosh_Tirunagari/GitHub/flair_models/ner/multi_bio_ner_model_disease/best-model.pt\n",
      "2019-10-08 17:28:22,906 loading file /nfs/gns/literature/Santosh_Tirunagari/GitHub/flair_models/ner/multi_bio_ner_model_organisms/v01/best-model.pt\n"
     ]
    }
   ],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence, Token\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "flair_models = '/nfs/gns/literature/Santosh_Tirunagari/GitHub/flair_models/ner/'\n",
    "# load the model you trained\n",
    "gene_model = SequenceTagger.load(flair_models+'multi_bio_ner_model_gene/v01/'+'best-model.pt')\n",
    "disease_model = SequenceTagger.load(flair_models+'multi_bio_ner_model_disease/'+'best-model.pt')\n",
    "organisms_model = SequenceTagger.load(flair_models+'multi_bio_ner_model_organisms/v01/'+'best-model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sentence(sentence):\n",
    "    gene_sentence = Sentence(sentence)\n",
    "    gene_model.predict(gene_sentence)\n",
    "\n",
    "    disease_sentence = Sentence(sentence)\n",
    "    disease_model.predict(disease_sentence)\n",
    "\n",
    "    organisms_sentence = Sentence(sentence)\n",
    "    organisms_model.predict(organisms_sentence)\n",
    "\n",
    "    ml_target = gene_sentence.get_spans('ner')\n",
    "    ml_disease = disease_sentence.get_spans('ner')\n",
    "    ml_organism = organisms_sentence.get_spans('ner')\n",
    "    \n",
    "    return ml_target, ml_disease, ml_organism\n",
    "\n",
    "def get_ents(ml_ent_type, ent_type):\n",
    "    all_ents = []\n",
    "    ents = re.findall(r'\"(.*?)\"', str(ml_ent_type))\n",
    "    for each_ents in ents:\n",
    "        all_ents.append([each_ents, ent_type])\n",
    "    return all_ents\n",
    "\n",
    "\n",
    "def find_sub_list(sl,l):\n",
    "    results=[]\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            results.append((ind,ind+sll))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def convert2IOB_dict(text_data,ner_tags):\n",
    "    \n",
    "    tokens = []\n",
    "    ners = []\n",
    "    \n",
    "    split_text = wordpunct_tokenize(text_data)\n",
    "    # for each word token append 'O'\n",
    "    arr = ['O']*len(split_text)\n",
    "\n",
    "    for each_tag in ner_tags:\n",
    "        token_list = wordpunct_tokenize(each_tag[0])\n",
    "        ner_list = wordpunct_tokenize(each_tag[1])\n",
    "\n",
    "        if(len(token_list) > len(ner_list)):\n",
    "            ner_list = len(token_list) * ner_list\n",
    "\n",
    "        for i in range(0,len(ner_list)):\n",
    "            # The logic here is look for the first B-tag and then append I-tag next\n",
    "            if(i==0):\n",
    "                ner_list[i] = 'B-'+ner_list[i]\n",
    "            else:\n",
    "                ner_list[i] = 'I-'+ner_list[i]\n",
    "\n",
    "        tokens.append(token_list)\n",
    "        ners.append(ner_list)\n",
    "        \n",
    "    for i in range(0, len(tokens)):\n",
    "        spans = find_sub_list(tokens[i], split_text)\n",
    "        for each_span in spans:\n",
    "            arr[each_span[0]:each_span[1]] = ners[i]\n",
    "    \n",
    "    return zip(split_text, arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3197/3197 [4:24:08<00:00,  4.96s/it]   \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "result_path = '/nfs/gns/literature/Santosh_Tirunagari/EPMC Annotations Dataset/'\n",
    "\n",
    "with open(result_path+'Public_annotated_test.csv','a',  newline='\\n') as f1:\n",
    "    public_writer=csv.writer(f1, delimiter='\\t',lineterminator='\\n')\n",
    "    for index, row in tqdm(test_df.iterrows(), total=test_df.shape[0]):\n",
    "#         get the text\n",
    "        text = ' '.join(wordpunct_tokenize(row['Sentences']))\n",
    "        ml_target, ml_disease, ml_organism = tag_sentence(text)\n",
    "        \n",
    "#         get the ner tags\n",
    "        all_entities = []\n",
    "\n",
    "        if ml_target:\n",
    "            for each_entity in get_ents(ml_target,'GP'):\n",
    "                all_entities.append(each_entity)\n",
    "        if ml_disease:\n",
    "            for each_entity in get_ents(ml_disease,'DS'):\n",
    "                all_entities.append(each_entity)\n",
    "        if ml_organism:\n",
    "            for each_entity in get_ents(ml_organism,'OG'):\n",
    "                all_entities.append(each_entity) \n",
    "     \n",
    "        \n",
    "        tagged_tokens = convert2IOB_dict(text,all_entities)\n",
    "#         print(list(tagged_tokens))\n",
    "\n",
    "        for each_word in tagged_tokens:\n",
    "            public_writer.writerow(list(each_word))\n",
    "        public_writer.writerow('') \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[[0, 11, 'Keratoconus', 'DS'], [67, 76, 'cytokines', 'GP']]\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "ner = row['true_ner']\n",
    "ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Keratoconus', 'DS'], ['cytokines', 'GP']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[2:4] for x in eval(ner)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3197/3197 [00:01<00:00, 3036.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from ast import literal_eval\n",
    "\n",
    "result_path = '/nfs/gns/literature/Santosh_Tirunagari/EPMC Annotations Dataset/'\n",
    "\n",
    "with open(result_path+'test_manual_annotated_on_public.csv','a',  newline='\\n') as f1:\n",
    "    public_writer=csv.writer(f1, delimiter='\\t',lineterminator='\\n')\n",
    "    for index, row in tqdm(test_df.iterrows(), total=test_df.shape[0]):\n",
    "#         get the text\n",
    "        text = ' '.join(wordpunct_tokenize(row['Sentences']))\n",
    "#         ml_target, ml_disease, ml_organism = tag_sentence(text)\n",
    "        true_ner = [x[2:4] for x in eval(row['true_ner'])] \n",
    "        \n",
    "        tagged_tokens = convert2IOB_dict(text,true_ner)\n",
    "#         print(list(tagged_tokens))\n",
    "\n",
    "        for each_word in tagged_tokens:\n",
    "            public_writer.writerow(list(each_word))\n",
    "        public_writer.writerow('') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        B-DS       0.68      0.74      0.70      1310\n",
      "        I-DS       0.61      0.71      0.66       536\n",
      "        B-GP       0.81      0.82      0.81      3367\n",
      "        I-GP       0.57      0.87      0.69      2245\n",
      "        B-OG       0.77      0.57      0.65      2545\n",
      "        I-OG       0.85      0.74      0.79      1251\n",
      "\n",
      "   micro avg       0.71      0.75      0.73     11254\n",
      "   macro avg       0.72      0.74      0.72     11254\n",
      "weighted avg       0.73      0.75      0.73     11254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "true_lbs = pd.read_csv(result_path+'test_manual_annotated_on_public.csv', sep='\\t', names=['tokens','tags'])\n",
    "pred_lbs = pd.read_csv(result_path+'Public_annotated_test.csv', sep='\\t', names=['tokens','tags'])\n",
    "\n",
    "y_true = true_lbs['tags'].values\n",
    "y_pred =  pred_lbs['tags'].values\n",
    "\n",
    "\n",
    "class_labels = sorted([tag for tag in set(y_true) if tag != 'O'], key=lambda name: (name[1:], name[0]))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred, labels = class_labels, output_dict=True))\n",
    "\n",
    "\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#         B-DS       0.74      0.53      0.62      1323\n",
    "#         I-DS       0.84      0.70      0.76       486\n",
    "#         B-GP       0.76      0.52      0.62      3056\n",
    "#         I-GP       0.89      0.41      0.56      1989\n",
    "#         B-OG       0.93      0.69      0.79      1960\n",
    "#         I-OG       0.98      0.51      0.68       968\n",
    "\n",
    "#    micro avg       0.83      0.54      0.66      9782\n",
    "#    macro avg       0.85      0.56      0.67      9782\n",
    "# weighted avg       0.84      0.54      0.65      9782"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = classification_report(y_true, y_pred, labels = class_labels, output_dict=True)\n",
    "\n",
    "result_df = pd.DataFrame(result_dict).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>B-DS</td>\n",
       "      <td>0.675316</td>\n",
       "      <td>0.735115</td>\n",
       "      <td>0.703947</td>\n",
       "      <td>1310.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>I-DS</td>\n",
       "      <td>0.614767</td>\n",
       "      <td>0.714552</td>\n",
       "      <td>0.660915</td>\n",
       "      <td>536.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>B-GP</td>\n",
       "      <td>0.809944</td>\n",
       "      <td>0.817642</td>\n",
       "      <td>0.813775</td>\n",
       "      <td>3367.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>I-GP</td>\n",
       "      <td>0.574726</td>\n",
       "      <td>0.865033</td>\n",
       "      <td>0.690612</td>\n",
       "      <td>2245.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>B-OG</td>\n",
       "      <td>0.771444</td>\n",
       "      <td>0.568959</td>\n",
       "      <td>0.654907</td>\n",
       "      <td>2545.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>I-OG</td>\n",
       "      <td>0.845943</td>\n",
       "      <td>0.741807</td>\n",
       "      <td>0.790460</td>\n",
       "      <td>1251.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>micro avg</td>\n",
       "      <td>0.713245</td>\n",
       "      <td>0.747912</td>\n",
       "      <td>0.730167</td>\n",
       "      <td>11254.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>macro avg</td>\n",
       "      <td>0.715357</td>\n",
       "      <td>0.740518</td>\n",
       "      <td>0.719103</td>\n",
       "      <td>11254.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.733350</td>\n",
       "      <td>0.747912</td>\n",
       "      <td>0.730623</td>\n",
       "      <td>11254.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score  support\n",
       "B-DS           0.675316  0.735115  0.703947   1310.0\n",
       "I-DS           0.614767  0.714552  0.660915    536.0\n",
       "B-GP           0.809944  0.817642  0.813775   3367.0\n",
       "I-GP           0.574726  0.865033  0.690612   2245.0\n",
       "B-OG           0.771444  0.568959  0.654907   2545.0\n",
       "I-OG           0.845943  0.741807  0.790460   1251.0\n",
       "micro avg      0.713245  0.747912  0.730167  11254.0\n",
       "macro avg      0.715357  0.740518  0.719103  11254.0\n",
       "weighted avg   0.733350  0.747912  0.730623  11254.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score\n",
      "DS          0.645041  0.724833  0.682431\n",
      "GP          0.692335  0.841338  0.752193\n",
      "OG          0.808694  0.655383  0.722684\n",
      "micro avg   0.713245  0.747912  0.730167\n",
      "macro avg   0.715357  0.740518  0.719103\n"
     ]
    }
   ],
   "source": [
    "report_list = []\n",
    "entity_index = ['DS', 'GP', 'OG', 'micro avg', 'macro avg']\n",
    "report_list.append(result_df.loc[['B-DS','I-DS']].mean(axis=0))\n",
    "report_list.append(result_df.loc[['B-GP','I-GP']].mean(axis=0))\n",
    "report_list.append(result_df.loc[['B-OG','I-OG']].mean(axis=0))\n",
    "report_list.append(result_df.loc[['micro avg']].mean(axis=0))\n",
    "report_list.append(result_df.loc[['macro avg']].mean(axis=0))\n",
    "# report_list.append(result_df.loc[['weighted avg']].mean(axis=0))\n",
    "\n",
    "report_df = pd.concat(report_list, axis=1).T\n",
    "# report_df.reindex(entity_index)\n",
    "report_df.index = entity_index\n",
    "print(report_df[['precision','recall','f1-score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_lbs = pd.read_csv(result_path+'test_manual_annotated_on_public.csv', sep='\\t', names=['tokens','tags'])\n",
    "pred_lbs = pd.read_csv(result_path+'Public_annotated_test.csv', sep='\\t', names=['tokens','tags'])\n",
    "\n",
    "true_lbs['tags'].replace('B-|I-','',regex=True, inplace=True)\n",
    "pred_lbs['tags'].replace('B-|I-','',regex=True, inplace=True)\n",
    "\n",
    "y_true = true_lbs['tags'].values\n",
    "y_pred =  pred_lbs['tags'].values\n",
    "\n",
    "\n",
    "class_labels = sorted([tag for tag in set(y_true) if tag != 'O'], key=lambda name: (name[1:], name[0]))\n",
    "print(classification_report(y_true, y_pred, labels = class_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          OG       0.82      0.64      0.72      3796\n",
      "          GP       0.73      0.89      0.80      5612\n",
      "          DS       0.70      0.77      0.73      1846\n",
      "\n",
      "   micro avg       0.75      0.79      0.77     11254\n",
      "   macro avg       0.75      0.77      0.75     11254\n",
      "weighted avg       0.76      0.79      0.76     11254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = true_lbs['tags'].values\n",
    "y_pred =  pred_lbs['tags'].values\n",
    "\n",
    "\n",
    "class_labels = sorted([tag for tag in set(y_true) if tag != 'O'], key=lambda name: (name[1:], name[0]))\n",
    "print(classification_report(y_true, y_pred, labels = class_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
