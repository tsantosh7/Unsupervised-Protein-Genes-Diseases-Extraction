{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to split manual annotation into train, devel and valid\n",
    "# Code to convert the splits into IOB format\n",
    "\n",
    "# (c) EMBL-EBI, September 2019\n",
    "#\n",
    "# Started: 19 Septmember  2019\n",
    "# Updated: 20 Septmember  2019\n",
    "\n",
    "_author_ = 'Santosh Tirunagari'\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "colNames = ('text', 'ner')\n",
    "\n",
    "file_path = '/nfs/gns/literature/Santosh_Tirunagari/EBI standard Dataset/'\n",
    "all_files = glob.glob(file_path+'*fulltext_batch*')\n",
    "result_folder = '/nfs/gns/literature/Santosh_Tirunagari/EBI standard Dataset/NER/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/nfs/gns/literature/Santosh_Tirunagari/EBI standard Dataset/NER/list_pmc_ids.csv','w') as f1:\n",
    "    writer=csv.writer(f1, delimiter='\\t',lineterminator='\\n',)\n",
    "    \n",
    "    for files in all_files:\n",
    "        with open(files) as json_file_ner_rel:\n",
    "            json_data = json.loads(json_file_ner_rel.read())\n",
    "            for articles in json_data:\n",
    "                pmc_id = articles #json_data[articles]\n",
    "                writer.writerow([pmc_id])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train, test and dev pmc ids\n",
    "import math\n",
    "import random\n",
    "file = '/nfs/gns/literature/Santosh_Tirunagari/EBI standard Dataset/NER/list_pmc_ids.csv'\n",
    "percentage=0.70\n",
    "iter = 0\n",
    "\n",
    "trainPMCids = []\n",
    "devPMCids = []\n",
    "testPMCids =[]\n",
    "\n",
    "with open(file, 'r',encoding=\"utf-8\") as fin:\n",
    "    allPMCids = fin.readlines()\n",
    "    \n",
    "nLines = sum(1 for line in allPMCids)\n",
    "nTrain = int(nLines*percentage) \n",
    "nValid = math.floor((nLines - nTrain)/2)\n",
    "nTest = nLines - (nTrain+nValid)\n",
    "\n",
    "deck = list(range(0, nLines))\n",
    "random.seed(2222) # Please dont change the seed for the reproducibility \n",
    "random.shuffle(deck)\n",
    "\n",
    "train_ids = deck[0:nTrain]\n",
    "devel_ids = deck[nTrain:nTrain+nValid]\n",
    "test_ids = deck[nTrain+nValid:nTrain+nValid+nTest]\n",
    "\n",
    "for each_pmc_id in allPMCids:\n",
    "    if iter in train_ids:\n",
    "        trainPMCids.append(each_pmc_id.strip())\n",
    "    elif iter in devel_ids:\n",
    "        devPMCids.append(each_pmc_id.strip())\n",
    "    else:\n",
    "        testPMCids.append(each_pmc_id.strip())\n",
    "\n",
    "    iter = iter+1       \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sub_list(sl,l):\n",
    "    results=[]\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            results.append((ind,ind+sll))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def convert2IOB_dict(text_data,ner_tags):\n",
    "    \n",
    "    tokens = []\n",
    "    ners = []\n",
    "    \n",
    "    split_text = wordpunct_tokenize(text_data)\n",
    "    # for each word token append 'O'\n",
    "    arr = ['O']*len(split_text)\n",
    "\n",
    "    for each_tag in ner_tags:\n",
    "        token_list = wordpunct_tokenize(each_tag[2])\n",
    "        ner_list = wordpunct_tokenize(each_tag[3])\n",
    "\n",
    "        if(len(token_list) > len(ner_list)):\n",
    "            ner_list = len(token_list) * ner_list\n",
    "\n",
    "        for i in range(0,len(ner_list)):\n",
    "            # The logic here is look for the first B-tag and then append I-tag next\n",
    "            if(i==0):\n",
    "                ner_list[i] = 'B-'+ner_list[i]\n",
    "            else:\n",
    "                ner_list[i] = 'I-'+ner_list[i]\n",
    "\n",
    "        tokens.append(token_list)\n",
    "        ners.append(ner_list)\n",
    "        \n",
    "    for i in range(0, len(tokens)):\n",
    "        spans = find_sub_list(tokens[i], split_text)\n",
    "        for each_span in spans:\n",
    "            arr[each_span[0]:each_span[1]] = ners[i]\n",
    "    \n",
    "    return zip(split_text, arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = '/nfs/gns/literature/Santosh_Tirunagari/EBI standard Dataset/NER/'\n",
    "\n",
    "with open(result_path+'train.csv','w',  newline='\\n') as f1, open(result_path+'dev.csv','w', newline='\\n') as f2, open(result_path+'test.csv','w', newline='\\n') as f3:  \n",
    "    train_writer=csv.writer(f1, delimiter='\\t',lineterminator='\\n')\n",
    "    dev_writer=csv.writer(f2, delimiter='\\t',lineterminator='\\n')\n",
    "    test_writer=csv.writer(f3, delimiter='\\t',lineterminator='\\n')\n",
    "    \n",
    "    for each_manually_annotated_json in all_files:\n",
    "        with open(each_manually_annotated_json) as json_file_ner_rel:\n",
    "            json_data = json.loads(json_file_ner_rel.read())\n",
    "\n",
    "            for articles in json_data:\n",
    "                pmc_id = articles #json_data[articles]\n",
    "                for each_annotation in json_data[articles]['annotations']:\n",
    "                    if each_annotation['ner'] != None:\n",
    "                        text = each_annotation['sent'].encode('utf-8').decode('utf-8')\n",
    "                        ner = each_annotation['ner']\n",
    "                        tagged_tokens = convert2IOB_dict(text,ner)\n",
    "                        \n",
    "                        if pmc_id in trainPMCids:\n",
    "                            for each_word in tagged_tokens:\n",
    "                                train_writer.writerow(list(each_word))\n",
    "                            train_writer.writerow('') \n",
    "\n",
    "                        elif pmc_id in devPMCids:\n",
    "                            for each_word in tagged_tokens:\n",
    "                                dev_writer.writerow(list(each_word))\n",
    "                            dev_writer.writerow('') \n",
    "\n",
    "                        elif pmc_id in testPMCids:\n",
    "                            for each_word in tagged_tokens:\n",
    "                                test_writer.writerow(list(each_word))\n",
    "                            test_writer.writerow('') \n",
    "                    \n",
    "                \n",
    "                \n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
