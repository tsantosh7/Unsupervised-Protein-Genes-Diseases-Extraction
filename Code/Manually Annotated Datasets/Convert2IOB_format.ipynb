{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to parallel process OTAR json file to extract sentences and entities\n",
    "# (c) EMBL-EBI, June 2019\n",
    "#\n",
    "# Started: 9 Septmember  2019\n",
    "# Updated: 9 Septmember  2019\n",
    "\n",
    "_author_ = 'Santosh Tirunagari'\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import sys\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "colNames = ('text', 'ner')\n",
    "\n",
    "file_path = '/nfs/gns/literature/Santosh_Tirunagari/EBI standard Dataset/'\n",
    "all_files = glob.glob(file_path+'*fulltext_batch*')\n",
    "result_folder = '/nfs/gns/literature/Santosh_Tirunagari/EBI standard Dataset/NER/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('/nfs/gns/literature/Santosh_Tirunagari/EBI standard Dataset/NER/list_pmc_ids.csv','w') as f1:\n",
    "    writer=csv.writer(f1, delimiter='\\t',lineterminator='\\n',)\n",
    "    \n",
    "    for files in all_files:\n",
    "        with open(files) as json_file_ner_rel:\n",
    "            json_data = json.loads(json_file_ner_rel.read())\n",
    "            for articles in json_data:\n",
    "                pmc_id = articles #json_data[articles]\n",
    "                writer.writerow([pmc_id])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    " \n",
    "def split_file(file,train,devel,test,percentage=0.70,isShuffle=True,seed=123):\n",
    "\"\"\"Splits a file in 3 given the `percentage` to go in the large file.\"\"\"\n",
    "random.seed(seed)\n",
    "with open(file, 'r',encoding=\"utf-8\") as fin:\n",
    " \n",
    "    nLines = sum(1 for line in fin)\n",
    "    fin.seek(0)\n",
    " \n",
    "    nTrain = int(nLines*percentage) \n",
    "    \n",
    "    nValid = (nLines - nTrain)/2\n",
    "    \n",
    "    nTest = nLines - nValid\n",
    " \n",
    "    i = 0\n",
    "    for line in fin:\n",
    "        r = random.random() if isShuffle else 0 # so that always evaluated to true when not isShuffle\n",
    "        if (i < nTrain and r < percentage) or (nLines - i > nValid):\n",
    "            foutBig.write(line)\n",
    "            i += 1\n",
    "        else:\n",
    "            foutSmall.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125, 27, 28)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "file = '/nfs/gns/literature/Santosh_Tirunagari/EBI standard Dataset/NER/list_pmc_ids.csv'\n",
    "percentage=0.70\n",
    "with open(file, 'r',encoding=\"utf-8\") as fin:\n",
    " \n",
    "    nLines = sum(1 for line in fin)\n",
    "    fin.seek(0)\n",
    " \n",
    "    nTrain = int(nLines*percentage) \n",
    "    \n",
    "    nValid = math.floor((nLines - nTrain)/2)\n",
    "    \n",
    "    nTest = nLines - (nTrain+nValid)\n",
    "\n",
    "nTrain, nValid, nTest    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5172811423030917"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = random.random() \n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = random.random() \n",
    "if isShuffle else 0 # so that always evaluated to true when not isShuffle\n",
    "    if (i < nTrain and r < percentage) or (nLines - i > nValid):\n",
    "#         foutBig.write(line)\n",
    "            \n",
    "        i += 1\n",
    "    else:\n",
    "        foutSmall.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(all_files[0]) as json_file_ner_rel:\n",
    "    json_data = json.loads(json_file_ner_rel.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('/nfs/gns/literature/Santosh_Tirunagari/EBI standard Dataset/NER/annot.csv','w') as f1:\n",
    "    writer=csv.writer(f1, delimiter='\\t',lineterminator='\\n',)\n",
    "\n",
    "    for articles in json_data:\n",
    "        pmc_id = articles #json_data[articles]\n",
    "    #     print(pmc_id)\n",
    "        for each_annotation in json_data[articles]['annotations']:\n",
    "#             break\n",
    "    #         print(each_annotation)\n",
    "              \n",
    "\n",
    "            if each_annotation['ner'] != None:\n",
    "                text = each_annotation['sent'].encode('utf-8').decode('utf-8')\n",
    "                ner = each_annotation['ner']\n",
    "\n",
    "\n",
    "                split_text = wordpunct_tokenize(text)\n",
    "                arr = ['O']*len(split_text)\n",
    "                ner_dict = dict(zip(split_text,arr))\n",
    "\n",
    "                ner_dict_ = {}\n",
    "\n",
    "                for each_tag in ner:\n",
    "                    token_list = wordpunct_tokenize(each_tag[2])\n",
    "                    ner_list = wordpunct_tokenize(each_tag[3])\n",
    "\n",
    "                    if(len(token_list) > len(ner_list)):\n",
    "                        ner_list = len(token_list) * ner_list\n",
    "\n",
    "                    for i in range(0,len(ner_list)):\n",
    "                        if(i==0):\n",
    "                            ner_list[i] = 'B-'+ner_list[i]\n",
    "                        else:\n",
    "                            ner_list[i] = 'I-'+ner_list[i]\n",
    "\n",
    "\n",
    "                    ner_dict_.update(dict(zip(token_list,ner_list)))\n",
    "                    \n",
    "\n",
    "                ner_dict.update(ner_dict_)\n",
    "                    \n",
    "                for each_word in split_text:\n",
    "                    row = [each_word,ner_dict[each_word]]\n",
    "                    writer.writerow(row)\n",
    "                writer.writerow('\\n')    \n",
    "\n",
    "\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
