{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is time taking unfortunately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.gold import minibatch\n",
    "from spacy.language import Language\n",
    "from spacy import util\n",
    "\n",
    "import jsonlines \n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import exrex\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Organims Dictionary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1683023/1683023 [02:01<00:00, 13859.43it/s]\n"
     ]
    }
   ],
   "source": [
    "Diseases = '/nfs/gns/literature/lit-textmining-pipelines/automata/DiseaseDictionary.mwt' #(# of terms - 59088)\n",
    "Genes_Proteins = '/nfs/gns/literature/lit-textmining-pipelines/automata/swissprot_Sept2014.2.3.mwt' #(# of terms - 347929)\n",
    "Organism = '/nfs/gns/literature/lit-textmining-pipelines/automata/Organisms150507.2.mwt' #(# of terms - 1683021)\n",
    "\n",
    "gene_patterns = []\n",
    "disease_patterns = []\n",
    "organisms_patterns = []\n",
    "\n",
    "\n",
    "# print('Loading Genes Dictionary')\n",
    "# with open(Genes_Proteins, 'r') as f:\n",
    "#     contents = f.read()\n",
    "#     soup = BeautifulSoup(contents, 'lxml')\n",
    "#     pattern_types = soup.find_all('t')\n",
    "#     for pattern in tqdm(pattern_types):\n",
    "#         gene_patterns.append(pattern.text)\n",
    "#     pattern_types = soup.find_all('r')\n",
    "#     for pattern in tqdm(pattern_types):\n",
    "#         gene_patterns.append(pattern.text)    \n",
    "        \n",
    "        \n",
    "\n",
    "# print('loading Disease Dictionary')\n",
    "# with open(Diseases, 'r') as f:\n",
    "#     contents = f.read()\n",
    "#     soup = BeautifulSoup(contents, 'lxml')\n",
    "#     pattern_types = soup.find_all('t')\n",
    "#     for pattern in tqdm(pattern_types):\n",
    "#         disease_patterns.append(pattern.text)\n",
    "\n",
    "\n",
    "        \n",
    "# Organism = '/nfs/gns/literature/machine-learning/Dictionaries/MWT/organisms_toy.mwt' #(# of terms - 1683021)        \n",
    "# organisms_patterns = []\n",
    "print('Loading Organims Dictionary')\n",
    "with open(Organism, 'r') as f:\n",
    "    contents = f.read()\n",
    "    soup = BeautifulSoup(contents, 'lxml')\n",
    "    pattern_types = soup.find_all('r')\n",
    "for pattern in tqdm(pattern_types):\n",
    "    dictionary_term = exrex.getone(pattern.text.replace('[ \\\\-_]*', ' ').replace('[^A-Za-z0-9]', ''))\n",
    "    clean_term = ' '.join(dictionary_term.split())\n",
    "    organisms_patterns.append(clean_term)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = '/nfs/gns/literature/Santosh_Tirunagari/GitHub/spacy_models/pubmed-pmc-lg/best'\n",
    "nlp = spacy.load(best_model_path)\n",
    "print(\"Loaded from\", best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy_lookup import Entity\n",
    "\n",
    "# dict_GP = Entity(keywords_list=gene_patterns, label='GP', case_sensitive=False)\n",
    "# dict_DS = Entity(keywords_list=disease_patterns, label='DS')\n",
    "dict_OG = Entity(keywords_list=organisms_patterns, label='OG')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_GP.name ='GP_dict'\n",
    "# dict_DS.name ='DS_dict'\n",
    "dict_OG.name = 'OG_dict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp.add_pipe(dict_GP, last=True)\n",
    "nlp.add_pipe(dict_OG, last=True)\n",
    "# nlp.add_pipe(dict_DS, last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('OG_dict', <spacy_lookup.Entity at 0x2aef004a53c8>)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cancers 103 110 OG\n",
      "cancers 455 462 OG\n",
      "human 1279 1284 OG\n",
      "this 1455 1459 OG\n",
      "this 1692 1696 OG\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"Hereditary breast cancer is brca1 known for its strong tendency of inheritance. Most hereditary breast cancers are related to BRCA1/BRCA2 pathogenic variants. The lifelong risk of breast cancer in pathogenic BRCA1 and BRCA2 variant carriers is approximately 65% and 45%, respectively, whereas that of ovarian cancer is estimated to be 39% and 11%, respectively. Therefore, understanding these variants and clinical knowledge on their occurrence in breast cancers and carriers are important. BRCA1 pathogenic variant breast cancer shows more aggressive clinicopathological features than the BRCA2 pathogenic variant breast cancer. Compared with sporadic breast cancer, their prognosis is still debated. Treatments of BRCA1/BRCA2 pathogenic variant breast cancer are similar to those for BRCA-negative breast cancer, mainly including surgery, radiotherapy, and chemotherapy. Recently, various clinical trials have investigated poly (adenosine diphosphate [ADP]-ribose) polymerase (PARP) inhibitor treatment for advanced-stage BRCA1/BRCA2 pathogenic variant breast cancer. Among the various PARP inhibitors, olaparib and talazoparib, which reached phase III clinical trials, showed improvement of median progression-free survival around three months. Preventive and surveillance in human and mouse strategies for BRCA pathogenic variant breast cancer to reduce cancer recurrence and improve treatment outcomes have recently received increasing attention. In this review, we provide an information on the clinical features of BRCA1/BRCA2 pathogenic variant breast cancer and clinical recommendations for BRCA pathogenic variant carriers, with a focus on treatment and prevention strategies. With this knowledge, clinicians could manage the BRCA1/BRCA2 pathogenic variant breast cancer patients more effectively.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from ast import literal_eval\n",
    "import csv\n",
    "\n",
    "\n",
    "gene_patterns_ = set(gene_patterns)\n",
    "disease_patterns_ = set(disease_patterns)\n",
    "organisms_patterns_ = set(organisms_patterns)\n",
    "\n",
    "train_set = '/nfs/gns/literature/machine-learning/evaluation/300articles/CSV formats/train.csv'\n",
    "dev_set = '/nfs/gns/literature/machine-learning/evaluation/300articles/CSV formats/dev.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_set, sep = '\\t', names = ['pmcid', 'sentence','ner'])\n",
    "dev_df = pd.read_csv(dev_set, sep = '\\t', names = ['pmcid', 'sentence','ner'])\n",
    "\n",
    "for index, row in tqdm(train_df.iterrows(), total=train_df.shape[0]):\n",
    "    ner_tags = row['ner']\n",
    "    if ner_tags:\n",
    "        try:\n",
    "            ner_tags = literal_eval(ner_tags)\n",
    "#             print(ner_tags)\n",
    "        except:\n",
    "            pass\n",
    "    try:        \n",
    "        for each_tag in ner_tags:\n",
    "            token_ = each_tag[2].lower()\n",
    "            ner_ = each_tag[3]\n",
    "\n",
    "            if ner_ == 'GP':\n",
    "                gene_patterns_.add(token_)\n",
    "            elif ner_ == 'OG':\n",
    "                organisms_patterns_.add(token_)\n",
    "            elif ner_ == 'DS':\n",
    "                disease_patterns_.add(token_)  \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "            \n",
    "for index, row in tqdm(dev_df.iterrows(), total=dev_df.shape[0]):\n",
    "    ner_tags = row['ner']\n",
    "    if ner_tags:\n",
    "        try:\n",
    "            ner_tags = literal_eval(ner_tags)\n",
    "#             print(ner_tags)\n",
    "        except:\n",
    "            pass\n",
    "    try:        \n",
    "        for each_tag in ner_tags:\n",
    "            token_ = each_tag[2].lower()\n",
    "            ner_ = each_tag[3]\n",
    "\n",
    "            if ner_ == 'GP':\n",
    "                gene_patterns_.add(token_)\n",
    "            elif ner_ == 'OG':\n",
    "                organisms_patterns_.add(token_)\n",
    "            elif ner_ == 'DS':\n",
    "                disease_patterns_.add(token_)  \n",
    "    except:\n",
    "        pass               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the best model\n",
    "\n",
    "best_model_path = '/nfs/gns/literature/Santosh_Tirunagari/GitHub/spacy_models/pubmed-pmc-lg/best'\n",
    "\n",
    "nlp = spacy.load(best_model_path)\n",
    "print(\"Loaded from\", best_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# nlp.add_pipe(entity_matcher_OG, after='GP_dict')\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interspecific tumor PT overgrowth phenocopies the s.Crocodylurus tarahumaras frogs female gametophytic mutants feronia/sirène (fer/srn), lorelei (lre), nortia (nta), TuRan (tun), evAn (evn), and Zea mays embryo sac 4 (ZmES4) RNAi-lines1314151617181920, which are defective in the reception of intraspecific PTs. \n",
      "frogs 77 82 OG\n",
      "nortia 152 158 OG\n",
      "Zea mays 195 203 OG\n"
     ]
    }
   ],
   "source": [
    "text = 'Interspecific tumor PT overgrowth phenocopies the s.Crocodylurus tarahumaras frogs female gametophytic mutants feronia/sirène (fer/srn), lorelei (lre), nortia (nta), TuRan (tun), evAn (evn), and Zea mays embryo sac 4 (ZmES4) RNAi-lines1314151617181920, which are defective in the reception of intraspecific PTs. '\n",
    "# text = 'These results indicate the dentate gyrus is mostly comprised of mature neurons (NeuN), along with a smaller population of precursor cells (nestin) and newly differentiated neurons (DCX), which corresponds to prior findings examining the relative number of each cell population in the dentate gyrus, indicating the proportion of cells labeled by the sensor approximately reflects physiological proportions [21, 22]. '\n",
    "sentence =nlp(text)\n",
    "\n",
    "print(sentence)\n",
    "for ent in sentence.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"/nfs/gns/literature/Santosh_Tirunagari/GitHub/spacy_models/dictionary_en-pubmed-pmc-lg/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer, wordpunct_tokenize\n",
    "\n",
    "\n",
    "def convert2IOB(text_data, ner_tags):\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "\n",
    "    tokens = []\n",
    "    ners = []\n",
    "    spans = []\n",
    "\n",
    "    split_text = tokenizer.tokenize(text_data)\n",
    "    span_text = list(tokenizer.span_tokenize(text_data))\n",
    "    # for each word token append 'O'\n",
    "    arr = ['O'] * len(split_text)\n",
    "\n",
    "    if ner_tags:\n",
    "        try:\n",
    "            ner_tags = literal_eval(ner_tags)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    elif isinstance(ner_tags, float) or ner_tags is None:\n",
    "        return zip(split_text, arr)\n",
    "    else:\n",
    "        return zip(split_text, arr)\n",
    "\n",
    "    for each_tag in ner_tags:\n",
    "        span_list = (each_tag[0], each_tag[1])\n",
    "        token_list = wordpunct_tokenize(each_tag[2])\n",
    "        ner_list = wordpunct_tokenize(each_tag[3])\n",
    "\n",
    "        if (len(token_list) > len(ner_list)):\n",
    "            ner_list = len(token_list) * ner_list\n",
    "        for i in range(0, len(ner_list)):\n",
    "            # The logic here is look for the first B-tag and then append I-tag next\n",
    "            if (i == 0):\n",
    "                ner_list[i] = 'B-' + ner_list[i]\n",
    "            else:\n",
    "                ner_list[i] = 'I-' + ner_list[i]\n",
    "\n",
    "        tokens.append(token_list)\n",
    "        ners.append(ner_list)\n",
    "        spans.append(span_list)\n",
    "\n",
    "    split_token_span_list = list(zip(split_text, span_text))\n",
    "    span_ner_list = list(zip(spans, ners))\n",
    "\n",
    "    sub_spans = []  # get sub spans from the full spans of the ner\n",
    "\n",
    "    for each_span_ner_list in span_ner_list:\n",
    "        # in full range ner e.g., [144, 150, 'GM-CSF', 'GP']\n",
    "        count = 0\n",
    "        # count is to keep track of the B, I, sub tags in the ner list\n",
    "        for each_token in split_token_span_list:\n",
    "            sub_spans_ = find_sub_span(each_token[1], each_span_ner_list[0])\n",
    "            if sub_spans_:\n",
    "                sub_spans.append([sub_spans_, each_span_ner_list[1][count]])\n",
    "                count = count + 1\n",
    "\n",
    "    for i, each_span_token in enumerate(split_token_span_list):\n",
    "        for each_ner_span in sub_spans:\n",
    "            if each_span_token[1] == each_ner_span[0]:\n",
    "                arr[i] = ''.join(each_ner_span[1])\n",
    "\n",
    "    return zip(split_text, arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17957/17957 [00:57<00:00, 313.66it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from ast import literal_eval\n",
    "import csv\n",
    "\n",
    "\n",
    "def find_sub_span(sub_span_range, full_spans_range):\n",
    "    # if a sub span is present in full span return it\n",
    "    if sub_span_range[0] in range(full_spans_range[0], full_spans_range[1]):\n",
    "        return sub_span_range\n",
    "    \n",
    "    \n",
    "\n",
    "test_set = '/nfs/gns/literature/machine-learning/evaluation/300articles/CSV formats/test.csv'\n",
    "\n",
    "result_path = '/nfs/gns/literature/machine-learning/evaluation/300articles/ML-NER/Dictionary_fused_en_pubmed_pmc_lg/'\n",
    "\n",
    "\n",
    "df_45 = pd.read_csv(test_set, sep = '\\t', names = ['pmcid', 'sentence','ner'])\n",
    "\n",
    "with open(result_path + 'Dictionary_OG_iob.csv', 'w', newline='\\n') as f1:\n",
    "    ml_writer = csv.writer(f1, delimiter='\\t', lineterminator='\\n')\n",
    "    \n",
    "    for index, row in tqdm(df_45.iterrows(), total=df_45.shape[0]):\n",
    "#         print(row['ner'])\n",
    "        text = row['sentence'] # .encode('utf-8').decode('utf-8')\n",
    "#         print(text)\n",
    "        sentence = nlp(text)\n",
    "        ml_ner =[]\n",
    "        for ent in sentence.ents:\n",
    "            ml_ner.append([ent.start_char, ent.end_char, ent.text, ent.label_])\n",
    "\n",
    "        tagged_tokens = convert2IOB(text, ml_ner)\n",
    "\n",
    "        for each_word in tagged_tokens:\n",
    "            ml_writer.writerow(list(each_word))\n",
    "        ml_writer.writerow('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test best model performance on 2000 set\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "epmc_annotations_2000 = '/nfs/gns/literature/machine-learning/evaluation/2000articles/europePMC-NER/annotations_API/full_sentences/tagged_sentences/Europe_PMC_annotation.csv'\n",
    "\n",
    "result_path = '/nfs/gns/literature/machine-learning/evaluation/2000articles/ML-NER/Dictionary_fused_en_pubmed_pmc_lg/'\n",
    "\n",
    "\n",
    "df_2000 = pd.read_csv(epmc_annotations_2000, sep = '\\t', names = ['pmcid', 'section', 'sentence','ner'])\n",
    "\n",
    "\n",
    "with open(result_path + 'Dictionary_and_trainingset_en-pubmed-pmc-lg_2000_iob.csv', 'a', newline='\\n') as f1:\n",
    "    ml_writer = csv.writer(f1, delimiter='\\t', lineterminator='\\n')\n",
    "    \n",
    "    for index, row in tqdm(df_2000.iterrows(), total=df_2000.shape[0]):\n",
    "#         print(row['ner'])\n",
    "        text = row['sentence'].encode('utf-8').decode('utf-8')\n",
    "#         print(text)\n",
    "        sentence = nlp(text)\n",
    "        ml_ner =[]\n",
    "        for ent in sentence.ents:\n",
    "            ml_ner.append([ent.start_char, ent.end_char, ent.text, ent.label_])\n",
    "\n",
    "        tagged_tokens = convert2IOB(text, ml_ner)\n",
    "\n",
    "        for each_word in tagged_tokens:\n",
    "            ml_writer.writerow(list(each_word))\n",
    "        ml_writer.writerow('')\n",
    "        \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scispcacy",
   "language": "python",
   "name": "scispcacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
