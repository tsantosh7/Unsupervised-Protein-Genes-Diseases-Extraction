{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.gold import minibatch\n",
    "from spacy.language import Language\n",
    "from spacy import util\n",
    "\n",
    "import jsonlines \n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import exrex\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "class EntityMatcher(object):\n",
    "#     name = 'entity_matcher'\n",
    "\n",
    "    def __init__(self, nlp, terms, label):\n",
    "#         patterns = [nlp(term) for term in tqdm(terms)]\n",
    "        patterns = [nlp.make_doc(term) for term in tqdm(terms)]\n",
    "        \n",
    "        if label =='OG':\n",
    "            self.matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\") # attribute lower is removed. e.g., IF A gene ==> if a ..\n",
    "        else:\n",
    "            self.matcher = PhraseMatcher(nlp.vocab) \n",
    "        \n",
    "        self.matcher.add(label, None, *patterns)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        matches = self.matcher(doc)\n",
    "        seen_tokens = set()\n",
    "        new_entities = []\n",
    "        entities = doc.ents\n",
    "        for match_id, start, end in matches:\n",
    "            # check for end - 1 here because boundaries are inclusive\n",
    "            if start not in seen_tokens and end - 1 not in seen_tokens:\n",
    "                new_entities.append(Span(doc, start, end, label=match_id))\n",
    "                entities = [\n",
    "                    e for e in entities if not (e.start < end and e.end > start)\n",
    "                ]\n",
    "                seen_tokens.update(range(start, end))\n",
    "\n",
    "        doc.ents = tuple(entities) + tuple(new_entities)\n",
    "        return doc\n",
    "    \n",
    "#         matches = set(\n",
    "#                     [(m_id, start, end) for m_id, start, end in matches if start != end]\n",
    "#                 )\n",
    "#         get_sort_key = lambda m: (m[2] - m[1], m[1])\n",
    "#         matches = sorted(matches, key=get_sort_key, reverse=True)\n",
    "#         entities = list(doc.ents)\n",
    "#         new_entities = []\n",
    "#         seen_tokens = set()\n",
    "#         for match_id, start, end in matches:\n",
    "#             if any(t.ent_type for t in doc[start:end]):\n",
    "#                 continue\n",
    "#             # check for end - 1 here because boundaries are inclusive\n",
    "#             if start not in seen_tokens and end - 1 not in seen_tokens:\n",
    "#                 new_entities.append(Span(doc, start, end, label=match_id))\n",
    "#                 entities = [\n",
    "#                     e for e in entities if not (e.start < end and e.end > start)\n",
    "#                 ]\n",
    "#                 seen_tokens.update(range(start, end))\n",
    "#         doc.ents = entities + new_entities\n",
    "#         return doc     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Genes Dictionary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 347929/347929 [00:01<00:00, 254233.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Disease Dictionary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59088/59088 [00:00<00:00, 262426.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Organims Dictionary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1683023/1683023 [03:03<00:00, 9168.87it/s]\n"
     ]
    }
   ],
   "source": [
    "Diseases = '/nfs/gns/literature/lit-textmining-pipelines/automata/DiseaseDictionary.mwt' #(# of terms - 59088)\n",
    "Genes_Proteins = '/nfs/gns/literature/lit-textmining-pipelines/automata/swissprot_Sept2014.2.3.mwt' #(# of terms - 347929)\n",
    "Organism = '/nfs/gns/literature/lit-textmining-pipelines/automata/Organisms150507.2.mwt' #(# of terms - 1683021)\n",
    "\n",
    "gene_patterns = []\n",
    "disease_patterns = []\n",
    "organisms_patterns = []\n",
    "\n",
    "\n",
    "print('Loading Genes Dictionary')\n",
    "with open(Genes_Proteins, 'r') as f:\n",
    "    contents = f.read()\n",
    "    soup = BeautifulSoup(contents, 'lxml')\n",
    "    pattern_types = soup.find_all('t')\n",
    "    for pattern in tqdm(pattern_types):\n",
    "        gene_patterns.append(pattern.text)\n",
    "#     pattern_types = soup.find_all('r')\n",
    "#     for pattern in tqdm(pattern_types):\n",
    "#         gene_patterns.append(pattern.text)    \n",
    "        \n",
    "        \n",
    "\n",
    "print('loading Disease Dictionary')\n",
    "with open(Diseases, 'r') as f:\n",
    "    contents = f.read()\n",
    "    soup = BeautifulSoup(contents, 'lxml')\n",
    "    pattern_types = soup.find_all('t')\n",
    "    for pattern in tqdm(pattern_types):\n",
    "        disease_patterns.append(pattern.text)\n",
    "\n",
    "\n",
    "        \n",
    "# Organism = '/nfs/gns/literature/machine-learning/Dictionaries/MWT/organisms_toy.mwt' #(# of terms - 1683021)        \n",
    "# organisms_patterns = []\n",
    "print('Loading Organims Dictionary')\n",
    "with open(Organism, 'r') as f:\n",
    "    contents = f.read()\n",
    "    soup = BeautifulSoup(contents, 'lxml')\n",
    "    pattern_types = soup.find_all('r')\n",
    "for pattern in tqdm(pattern_types):\n",
    "    dictionary_term = exrex.getone(pattern.text.replace('[ \\\\-_]*', ' ').replace('[^A-Za-z0-9]', ''))\n",
    "    clean_term = ' '.join(dictionary_term.split())\n",
    "    organisms_patterns.append(clean_term)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the blank model\n",
    "\n",
    "# nlp = spacy.blank('en')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from /nfs/gns/literature/Santosh_Tirunagari/GitHub/spacy_models/pubmed-pmc-lg/best\n"
     ]
    }
   ],
   "source": [
    "best_model_path = '/nfs/gns/literature/Santosh_Tirunagari/GitHub/spacy_models/pubmed-pmc-lg/best'\n",
    "nlp = spacy.load(best_model_path)\n",
    "print(\"Loaded from\", best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 347929/347929 [00:22<00:00, 15778.10it/s]\n",
      "100%|██████████| 59088/59088 [00:01<00:00, 30045.73it/s]\n",
      "100%|██████████| 1683023/1683023 [03:04<00:00, 9131.11it/s]\n"
     ]
    }
   ],
   "source": [
    "entity_matcher_GP = EntityMatcher(nlp, gene_patterns, 'GP')\n",
    "entity_matcher_DS = EntityMatcher(nlp, disease_patterns, 'DS')\n",
    "entity_matcher_OG = EntityMatcher(nlp, organisms_patterns, 'OG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'entity_matcher_GP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-57854b2bae35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mentity_matcher_GP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'GP_dict'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mentity_matcher_DS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'DS_dict'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mentity_matcher_OG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'OG_dict'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# nlp.add_pipe(entity_matcher_GP)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'entity_matcher_GP' is not defined"
     ]
    }
   ],
   "source": [
    "entity_matcher_GP.name = 'GP_dict'\n",
    "entity_matcher_DS.name = 'DS_dict'\n",
    "entity_matcher_OG.name = 'OG_dict'\n",
    "\n",
    "# nlp.add_pipe(entity_matcher_GP)\n",
    "# nlp.add_pipe(entity_matcher_OG, after='GP_dict')\n",
    "# nlp.add_pipe(entity_matcher_DS, after='OG_dict')\n",
    "\n",
    "\n",
    "nlp.add_pipe(entity_matcher_GP, after='ner')\n",
    "nlp.add_pipe(entity_matcher_DS, after='ner')\n",
    "nlp.add_pipe(entity_matcher_OG, after='GP_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp.add_pipe(nlp2.pipeline[-1][-1], 'ner')\n",
    "\n",
    "# nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ner', 'DS_dict', 'GP_dict', 'OG_dict']\n"
     ]
    }
   ],
   "source": [
    "# nlp.remove_pipe('DS_dict')\n",
    "# nlp.remove_pipe('GP_dict')\n",
    "# nlp.remove_pipe('OG_dict')\n",
    "# nlp.add_pipe(entity_matcher_GP, after='ner')\n",
    "# nlp.add_pipe(entity_matcher_DS, after='ner')\n",
    "# nlp.remove_pipe('GP_dict')\n",
    "# print(nlp.pipe_names)\n",
    "# nlp.add_pipe(entity_matcher_GP, after='ner')\n",
    "# nlp.add_pipe(entity_matcher_DS, before='ner')\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interspecific tumor ZBTB12 PT Sergef Ribonuclease 5 EN116 overgrowth TGN48 Uncharacterized protein Rv2292c phenocopies the s.Crocodylurus Transmembrane protein 2 tarahumara frogs Putative HLA class I histocompatibility antigen, alpha chain H female gametophytic mutants feronia/sirène (fer/srn), lorelei (lre), nortia (nta), Turan (tun), evAn (evn), and Zea mays embryo sac 4 (ZmES4) RNAi-lines1314151617181920, PtdInsTP β,  hmg2l1, MXL8.8 which are defective in the reception of intraspecific PTs. GTP\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~ **** ~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tumor 14 19 DS\n",
      "ZBTB12 20 26 GP\n",
      "Sergef 30 36 GP\n",
      "Ribonuclease 37 49 GP\n",
      "EN116 52 57 GP\n",
      "TGN48 69 74 GP\n",
      "Uncharacterized protein Rv2292c 75 106 GP\n",
      "Transmembrane protein 2 138 161 GP\n",
      "frogs 173 178 OG\n",
      "Putative HLA class I histocompatibility antigen, alpha chain H 179 241 GP\n",
      "feronia 270 277 GP\n",
      "sirène 278 284 GP\n",
      "srn 290 293 GP\n",
      "Zea mays 354 362 OG\n",
      "PtdInsTP β 412 422 GP\n",
      "hmg2l1 425 431 GP\n",
      "MXL8.8 433 439 GP\n"
     ]
    }
   ],
   "source": [
    "text = 'Interspecific tumor ZBTB12 PT Sergef Ribonuclease 5 EN116 overgrowth TGN48 Uncharacterized protein Rv2292c phenocopies the s.Crocodylurus Transmembrane protein 2 tarahumara frogs Putative HLA class I histocompatibility antigen, alpha chain H female gametophytic mutants feronia/sirène (fer/srn), lorelei (lre), nortia (nta), Turan (tun), evAn (evn), and Zea mays embryo sac 4 (ZmES4) RNAi-lines1314151617181920, PtdInsTP β,  hmg2l1, MXL8.8 which are defective in the reception of intraspecific PTs. GTP'\n",
    "# text = 'These ractory cytopenia with unilineage dysplasia iris neoplasms results indicate the dentate gyrus is breast tumor mostly comprised of mature neurons (NeuN), along with a smaller population of precursor cells (nestin) and newly differentiated neurons (DCX), which corresponds to prior findings examining the relative number of each cell population in the dentate gyrus, indicating the proportion of cells labeled by the sensor approximately reflects physiological proportions [21, 22]. '\n",
    "# text = ' However, if a pollen grain originating from a different species (interspecific pollination) is placed on a plant\\'s stigma, all the communication processes desc     27 ribed above have the potential to act as pre-zygotic post-pollination barriers.'\n",
    "# text = 'Bradyrhizobium sp. B1P5L4 STM2336, M. zalophi, Tarahumara Frogs, Residual erythrocytes were removed by hypotonic lysis'\n",
    "sentence =nlp(text)\n",
    "\n",
    "print(sentence)\n",
    "print('~~~~~~~~~~~~~~~~~~~~~~~~~~~ **** ~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "for ent in sentence.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer, wordpunct_tokenize\n",
    "\n",
    "\n",
    "def convert2IOB(text_data, ner_tags):\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "\n",
    "    tokens = []\n",
    "    ners = []\n",
    "    spans = []\n",
    "\n",
    "    split_text = tokenizer.tokenize(text_data)\n",
    "    span_text = list(tokenizer.span_tokenize(text_data))\n",
    "    # for each word token append 'O'\n",
    "    arr = ['O'] * len(split_text)\n",
    "\n",
    "    if ner_tags:\n",
    "        try:\n",
    "            ner_tags = literal_eval(ner_tags)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    elif isinstance(ner_tags, float) or ner_tags is None:\n",
    "        return zip(split_text, arr)\n",
    "    else:\n",
    "        return zip(split_text, arr)\n",
    "\n",
    "    for each_tag in ner_tags:\n",
    "        span_list = (each_tag[0], each_tag[1])\n",
    "        token_list = wordpunct_tokenize(each_tag[2])\n",
    "        ner_list = wordpunct_tokenize(each_tag[3])\n",
    "\n",
    "        if (len(token_list) > len(ner_list)):\n",
    "            ner_list = len(token_list) * ner_list\n",
    "        for i in range(0, len(ner_list)):\n",
    "            # The logic here is look for the first B-tag and then append I-tag next\n",
    "            if (i == 0):\n",
    "                ner_list[i] = 'B-' + ner_list[i]\n",
    "            else:\n",
    "                ner_list[i] = 'I-' + ner_list[i]\n",
    "\n",
    "        tokens.append(token_list)\n",
    "        ners.append(ner_list)\n",
    "        spans.append(span_list)\n",
    "\n",
    "    split_token_span_list = list(zip(split_text, span_text))\n",
    "    span_ner_list = list(zip(spans, ners))\n",
    "\n",
    "    sub_spans = []  # get sub spans from the full spans of the ner\n",
    "\n",
    "    for each_span_ner_list in span_ner_list:\n",
    "        # in full range ner e.g., [144, 150, 'GM-CSF', 'GP']\n",
    "        count = 0\n",
    "        # count is to keep track of the B, I, sub tags in the ner list\n",
    "        for each_token in split_token_span_list:\n",
    "            sub_spans_ = find_sub_span(each_token[1], each_span_ner_list[0])\n",
    "            if sub_spans_:\n",
    "                sub_spans.append([sub_spans_, each_span_ner_list[1][count]])\n",
    "                count = count + 1\n",
    "\n",
    "    for i, each_span_token in enumerate(split_token_span_list):\n",
    "        for each_ner_span in sub_spans:\n",
    "            if each_span_token[1] == each_ner_span[0]:\n",
    "                arr[i] = ''.join(each_ner_span[1])\n",
    "\n",
    "    return zip(split_text, arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17957/17957 [01:19<00:00, 225.89it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from ast import literal_eval\n",
    "import csv\n",
    "\n",
    "\n",
    "def find_sub_span(sub_span_range, full_spans_range):\n",
    "    # if a sub span is present in full span return it\n",
    "    if sub_span_range[0] in range(full_spans_range[0], full_spans_range[1]):\n",
    "        return sub_span_range\n",
    "    \n",
    "    \n",
    "\n",
    "test_set = '/nfs/gns/literature/machine-learning/evaluation/300articles/CSV formats/test.csv'\n",
    "\n",
    "result_path = '/nfs/gns/literature/machine-learning/evaluation/300articles/ML-NER/Dictionary_fused_en_pubmed_pmc_lg/'\n",
    "\n",
    "\n",
    "df_45 = pd.read_csv(test_set, sep = '\\t', names = ['pmcid', 'sentence','ner'])\n",
    "\n",
    "with open(result_path + 'Dictionary_fused_en_pubmed_pmc_lg_iob.csv', 'w', newline='\\n') as f1:\n",
    "    ml_writer = csv.writer(f1, delimiter='\\t', lineterminator='\\n')\n",
    "    \n",
    "    for index, row in tqdm(df_45.iterrows(), total=df_45.shape[0]):\n",
    "#         print(row['ner'])\n",
    "        text = row['sentence'] # .encode('utf-8').decode('utf-8')\n",
    "#         print(text)\n",
    "        sentence = nlp(text)\n",
    "        ml_ner =[]\n",
    "        for ent in sentence.ents:\n",
    "            ml_ner.append([ent.start_char, ent.end_char, ent.text, ent.label_])\n",
    "\n",
    "        tagged_tokens = convert2IOB(text, ml_ner)\n",
    "\n",
    "        for each_word in tagged_tokens:\n",
    "            ml_writer.writerow(list(each_word))\n",
    "        ml_writer.writerow('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test best model performance on 2000 set\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "epmc_annotations_2000 = '/nfs/gns/literature/machine-learning/evaluation/2000articles/europePMC-NER/annotations_API/full_sentences/tagged_sentences/Europe_PMC_annotation.csv'\n",
    "\n",
    "result_path = '/nfs/gns/literature/machine-learning/evaluation/2000articles/ML-NER/Dictionary_fused_en_pubmed_pmc_lg/'\n",
    "\n",
    "\n",
    "df_2000 = pd.read_csv(epmc_annotations_2000, sep = '\\t', names = ['pmcid', 'section', 'sentence','ner'])\n",
    "\n",
    "\n",
    "with open(result_path + 'Dictionary_en-pubmed-pmc-lg_2000_iob.csv', 'a', newline='\\n') as f1:\n",
    "    ml_writer = csv.writer(f1, delimiter='\\t', lineterminator='\\n')\n",
    "    \n",
    "    for index, row in tqdm(df_2000.iterrows(), total=df_2000.shape[0]):\n",
    "#         print(row['ner'])\n",
    "        text = row['sentence'].encode('utf-8').decode('utf-8')\n",
    "#         print(text)\n",
    "        sentence = nlp(text)\n",
    "        ml_ner =[]\n",
    "        for ent in sentence.ents:\n",
    "            ml_ner.append([ent.start_char, ent.end_char, ent.text, ent.label_])\n",
    "\n",
    "        tagged_tokens = convert2IOB(text, ml_ner)\n",
    "\n",
    "        for each_word in tagged_tokens:\n",
    "            ml_writer.writerow(list(each_word))\n",
    "        ml_writer.writerow('')\n",
    "        \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scispcacy",
   "language": "python",
   "name": "scispcacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
